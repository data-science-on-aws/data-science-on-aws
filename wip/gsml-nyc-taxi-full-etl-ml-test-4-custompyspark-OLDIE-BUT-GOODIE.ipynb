{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to S3 with a SageMaker Processing Job\n",
    "\n",
    "<div class=\"alert alert-info\"> üí° <strong> Quick Start </strong>\n",
    "To save your processed data to S3, select the Run menu above and click <strong>Run all cells</strong>. \n",
    "<strong><a style=\"color: #0397a7 \" href=\"#Job-Status-&-S3-Output-Location\">\n",
    "    <u>View the status of the export job and the output S3 location</u></a>.\n",
    "</strong>\n",
    "</div>\n",
    "\n",
    "\n",
    "This notebook executes your Data Wrangler Flow `gsml-nyc-taxi-full-etl-test-3-custompyspark.flow` on the entire dataset using a SageMaker \n",
    "Processing Job and will save the processed data to S3.\n",
    "\n",
    "This notebook saves data from the step `Custom Code`. To save from a different step, go to Data Wrangler \n",
    "to select a new step to export. \n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Inputs and Outputs](#Inputs-and-Outputs)\n",
    "1. [Run Processing Job](#Run-Processing-Job)\n",
    "   1. [Job Configurations](#Job-Configurations)\n",
    "   1. [Create Processing Job](#Create-Processing-Job)\n",
    "   1. [Job Status & S3 Output Location](#Job-Status-&-S3-Output-Location)\n",
    "1. [Optional Next Steps](#(Optional)Next-Steps)\n",
    "    1. [Load Processed Data into Pandas](#(Optional)-Load-Processed-Data-into-Pandas)\n",
    "    1. [Train a model with SageMaker](#(Optional)Train-a-model-with-SageMaker)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs\n",
    "\n",
    "The below settings configure the inputs and outputs for the flow export.\n",
    "\n",
    "<div class=\"alert alert-info\"> üí° <strong> Configurable Settings </strong>\n",
    "\n",
    "In <b>Input - Source</b> you can configure the data sources that will be used as input by Data Wrangler\n",
    "\n",
    "1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n",
    "2. For all other sources, configure attributes like query_string, database in the source's \n",
    "<b>DatasetDefinition</b> object.\n",
    "\n",
    "If you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \n",
    "You should also re-execute the cells in this section if you have modified the settings in any data sources.\n",
    "\n",
    "Parametrized data sources will be ignored when creating ProcessingInputs, and will directly read from the source.\n",
    "Network isolation is not supported for parametrized data sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "data_sources = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: ride-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://dsoaws/nyc-taxi-orig-cleaned-split-parquet-per-year/ride-info/\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/ride-info\",\n",
    "    input_name=\"ride-info\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: ride-fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://dsoaws/nyc-taxi-orig-cleaned-split-parquet-per-year/ride-fare/\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/ride-fare\",\n",
    "    input_name=\"ride-fare\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: S3 settings\n",
    "\n",
    "<div class=\"alert alert-info\"> üí° <strong> Configurable Settings </strong>\n",
    "\n",
    "1. <b>bucket</b>: you can configure the S3 bucket where Data Wrangler will save the output. The default bucket from \n",
    "the SageMaker notebook session is used. \n",
    "2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \n",
    "conflict with other flow exports \n",
    "3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler export storage bucket: sagemaker-us-east-1-079002598131\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# bucket = \"my-bucket\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Data Wrangler export storage bucket: {bucket}\")\n",
    "\n",
    "# unique flow export ID\n",
    "# flow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_id = f\"{time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "# # THIS IS THE DEFAULT GENERATE BY DATAWRANGLER\n",
    "# # KHOI: this is only for one of the Transform node, Creating training set\n",
    "# output_name = \"842a9df0-a299-4625-8f8a-ffca58929650.default\"\n",
    "\n",
    "# s3_folder = 'gsml-nyc-taxi-full-etl-ml-test-3-custompyspark-export-s3-via-notebook'\n",
    "# s3_output_prefix = f\"{s3_folder}/export-{flow_export_name}/output\"\n",
    "# s3_output_base_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "# print(f\"Processing output base path: {s3_output_base_path}\\nThe final output location will contain additional subdirectories.\")\n",
    "\n",
    "# processing_job_output = ProcessingOutput(\n",
    "#     output_name=output_name,\n",
    "#     source=\"/opt/ml/processing/output\",\n",
    "#     destination=s3_output_base_path,\n",
    "#     s3_upload_mode=\"EndOfJob\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Processing output base path: s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/training\n",
      "The final output location will contain additional subdirectories.\n",
      "Validation Processing output base path: s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/validation\n",
      "The final output location will contain additional subdirectories.\n"
     ]
    }
   ],
   "source": [
    "#! KHOI'S MODIFICATION:\n",
    "\n",
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "s3_folder = 'gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook'\n",
    "s3_output_prefix = f\"{s3_folder}/export-{flow_export_name}/output\"\n",
    "s3_output_base_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "# !!!Make sure to change this according to the `node_id` of the exact transformation using the Flow Editor (right click on flow file and Open With Editor)\n",
    "splitting_data_mapping = {\n",
    "    'training': '842a9df0-a299-4625-8f8a-ffca58929650',\n",
    "    'validation': '052787ea-75d7-4079-be2a-3d436a45b0c8', \n",
    "}\n",
    "\n",
    "processing_job_outputs = []\n",
    "for set_name, transform_id in splitting_data_mapping.items():\n",
    "    output_name = f'{transform_id}.default'\n",
    "    s3_output_prefix_set = f\"{s3_output_prefix}/{set_name}\"\n",
    "    s3_output_base_path_set = f\"s3://{bucket}/{s3_output_prefix_set}\"\n",
    "    print(f\"{set_name.title()} Processing output base path: {s3_output_base_path_set}\\nThe final output location will contain additional subdirectories.\")\n",
    "\n",
    "    processing_job_outputs.append(ProcessingOutput(output_name=output_name,\n",
    "                                                   source=f\"/opt/ml/processing/output/{set_name}\",\n",
    "                                                   destination=s3_output_base_path_set,\n",
    "                                                   s3_upload_mode=\"EndOfJob\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Flow to S3\n",
    "\n",
    "To use the Data Wrangler as an input to the processing job,  first upload your flow file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flow file from current notebook working directory: /root/data-science-on-aws.xgboost\n",
      "Data Wrangler flow gsml-nyc-taxi-full-etl-test-3-custompyspark.flow uploaded to s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/data_wrangler_flows/flow-2023-03-02-03-32-10-53926e35.flow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"gsml-nyc-taxi-full-etl-test-3-custompyspark.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "flow_file_prefix_and_name = f\"{s3_folder}/data_wrangler_flows/{flow_export_name}.flow\"\n",
    "s3_client.upload_file(flow_file_name, bucket, flow_file_prefix_and_name, ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/{flow_file_prefix_and_name}\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Wrangler Flow is also provided to the Processing Job as an input source which we configure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: gsml-nyc-taxi-full-etl-test-3-custompyspark.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Processing Job \n",
    "## Job Configurations\n",
    "\n",
    "<div class=\"alert alert-info\"> üí° <strong> Configurable Settings </strong>\n",
    "\n",
    "You can configure the following settings for Processing Jobs. If you change any configurations you will \n",
    "need to re-execute this and all cells below it by selecting the Run menu above and click \n",
    "<b>Run Selected Cells and All Below</b>\n",
    "\n",
    "1. IAM role for executing the processing job. \n",
    "2. A unique name of the processing job. Give a unique name every time you re-execute processing jobs\n",
    "3. Data Wrangler Container URL.\n",
    "4. Instance count, instance type and storage volume size in GB.\n",
    "5. Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "6. Network Isolation settings\n",
    "7. KMS key to encrypt output data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs.\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:2.x\"\n",
    "# Pinned Data Wrangler Container URL.\n",
    "container_uri_pinned = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:2.1.2\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 6\n",
    "instance_type = \"ml.m5.24xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing.\n",
    "# !KHOI: must change this to at least 200GB for full dataset\n",
    "volume_size_in_gb = 200\n",
    "\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"Parquet\"\n",
    "\n",
    "# Delimiter to use for the output if the output content type is CSV. Uncomment to set.\n",
    "# delimiter = \",\"\n",
    "\n",
    "# Compression to use for the output. Uncomment to set.\n",
    "# compression = \"gzip\"\n",
    "\n",
    "# Configuration for partitioning the output. Uncomment to set.\n",
    "# \"num_partition\" sets the number of partitions/files written in the output.\n",
    "# \"partition_by\" sets the column names to partition the output by.\n",
    "# partition_config = {\n",
    "#     \"num_partitions\": 1,\n",
    "#     \"partition_by\": [\"column_name_1\", \"column_name_2\"],\n",
    "# }\n",
    "\n",
    "# Network Isolation mode; default is off.\n",
    "enable_network_isolation = False\n",
    "\n",
    "# List of tags to be passed to the processing job.\n",
    "user_tags = []\n",
    "\n",
    "# Output configuration used as processing job container arguments. Only applies when writing to S3.\n",
    "# Uncomment to set additional configurations.\n",
    "# output_config = {\n",
    "#     output_name: {\n",
    "#         \"content_type\": output_content_type,\n",
    "#         # \"delimiter\": delimiter,\n",
    "#         # \"compression\": compression,\n",
    "#         # \"partition_config\": partition_config,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# KHOI: has to create separate output_config for each output:\n",
    "output_configs = []\n",
    "for set_name, transform_id in splitting_data_mapping.items():\n",
    "    output_name = f'{transform_id}.default'\n",
    "    output_configs.append({\n",
    "        output_name: {\n",
    "            \"content_type\": output_content_type,\n",
    "            # \"delimiter\": delimiter,\n",
    "            # \"compression\": compression,\n",
    "            # \"partition_config\": partition_config,\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Refit configuration determines whether Data Wrangler refits the trainable parameters on the entire dataset. \n",
    "# When True, the processing job relearns the parameters and outputs a new flow file.\n",
    "# You can specify the name of the output flow file under 'output_flow'.\n",
    "# Note: There are length constraints on the container arguments (max 256 characters).\n",
    "refit_trained_params = {\n",
    "    \"refit\": False,\n",
    "    \"output_flow\": f\"data-wrangler-flow-processing-{flow_export_id}.flow\"\n",
    "}\n",
    "\n",
    "# KMS key for per object encryption; default is None.\n",
    "kms_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Configure Spark Cluster Driver Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Config file uploaded to s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/spark_configuration/data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35/configuration.json\n"
     ]
    }
   ],
   "source": [
    "# The Spark memory configuration. Change to specify the driver and executor memory in MB for the Spark cluster during processing.\n",
    "driver_memory_in_mb = 2048\n",
    "executor_memory_in_mb = 55742\n",
    "\n",
    "config = json.dumps({\n",
    "    \"Classification\": \"spark-defaults\",\n",
    "    \"Properties\": {\n",
    "        \"spark.driver.memory\": f\"{driver_memory_in_mb}m\",\n",
    "        \"spark.executor.memory\": f\"{executor_memory_in_mb}m\"\n",
    "    }\n",
    "})\n",
    "\n",
    "config_file = f\"config-{flow_export_id}.json\"\n",
    "with open(config_file, \"w\") as f:\n",
    "    f.write(config)\n",
    "\n",
    "config_s3_path = f\"{s3_folder}/spark_configuration/{processing_job_name}/configuration.json\"\n",
    "config_s3_uri = f\"s3://{bucket}/{config_s3_path}\"\n",
    "s3_client.upload_file(config_file, bucket, config_s3_path, ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "print(f\"Spark Config file uploaded to {config_s3_uri}\")\n",
    "os.remove(config_file)\n",
    "\n",
    "# Provides the spark config file to processing job and set the cluster driver memory. Uncomment to set.\n",
    "# data_sources.append(ProcessingInput(\n",
    "#     source=config_s3_uri,\n",
    "#     destination=\"/opt/ml/processing/input/conf\",\n",
    "#     input_name=\"spark-config\",\n",
    "#     s3_data_type=\"S3Prefix\",\n",
    "#     s3_input_mode=\"File\",\n",
    "#     s3_data_distribution_type=\"FullyReplicated\"\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing Job\n",
    "\n",
    "To launch a Processing Job, you will use the SageMaker Python SDK to create a Processor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup processing job network configuration\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "network_config = NetworkConfig(\n",
    "        enable_network_isolation=enable_network_isolation,\n",
    "        security_group_ids=None,\n",
    "        subnets=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=network_config,\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key,\n",
    "    tags=user_tags\n",
    ")\n",
    "\n",
    "# # Start Job\n",
    "# processor.run(\n",
    "#     inputs=[flow_input] + data_sources, \n",
    "#     outputs=[processing_job_output],\n",
    "#     arguments=[f\"--output-config '{json.dumps(output_config)}'\"] + [f\"--refit-trained-params '{json.dumps(refit_trained_params)}'\"],\n",
    "#     wait=False,\n",
    "#     logs=False,\n",
    "#     job_name=processing_job_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Status & S3 Output Location\n",
    "\n",
    "Below you wait for processing job to finish. If it finishes successfully, the raw parameters used by the \n",
    "Processing Job will be printed.\n",
    "\n",
    "To prevent data of different processing jobs and different output nodes from being overwritten or combined, \n",
    "Data Wrangler uses the name of the processing job and the name of the output to write the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.10 (default, Jun  4 2021, 14:48:32) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 base path: s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output\n",
      "Processing job name: data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35\n"
     ]
    }
   ],
   "source": [
    "print(f'S3 base path: {s3_output_base_path}')\n",
    "print(f'Processing job name: {processing_job_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job results (training set) are saved to S3 path: s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/training/data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35/842a9df0-a299-4625-8f8a-ffca58929650/default\n",
      "Job results (validation set) are saved to S3 path: s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/validation/data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35/052787ea-75d7-4079-be2a-3d436a45b0c8/default\n",
      "............................................................................................................................................................................!CPU times: user 749 ms, sys: 92.7 ms, total: 842 ms\n",
      "Wall time: 14min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'flow',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/data_wrangler_flows/flow-2023-03-02-03-32-10-53926e35.flow',\n",
       "    'LocalPath': '/opt/ml/processing/flow',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'ride-info',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://dsoaws/nyc-taxi-orig-cleaned-split-parquet-per-year/ride-info/',\n",
       "    'LocalPath': '/opt/ml/processing/ride-info',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'ride-fare',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://dsoaws/nyc-taxi-orig-cleaned-split-parquet-per-year/ride-fare/',\n",
       "    'LocalPath': '/opt/ml/processing/ride-fare',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': '842a9df0-a299-4625-8f8a-ffca58929650.default',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/training',\n",
       "     'LocalPath': '/opt/ml/processing/output/training',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False},\n",
       "   {'OutputName': '052787ea-75d7-4079-be2a-3d436a45b0c8.default',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/validation',\n",
       "     'LocalPath': '/opt/ml/processing/output/validation',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 6,\n",
       "   'InstanceType': 'ml.m5.24xlarge',\n",
       "   'VolumeSizeInGB': 200}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:2.x',\n",
       "  'ContainerArguments': ['--output-config \\'{\"842a9df0-a299-4625-8f8a-ffca58929650.default\": {\"content_type\": \"Parquet\"}}\\'',\n",
       "   '--output-config \\'{\"052787ea-75d7-4079-be2a-3d436a45b0c8.default\": {\"content_type\": \"Parquet\"}}\\'',\n",
       "   '--refit-trained-params \\'{\"refit\": false, \"output_flow\": \"data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35.flow\"}\\'']},\n",
       " 'NetworkConfig': {'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableNetworkIsolation': False},\n",
       " 'RoleArn': 'arn:aws:iam::079002598131:role/service-role/AmazonSageMaker-ExecutionRole-20220804T150518',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:079002598131:processing-job/data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2023, 3, 2, 3, 46, 33, 142000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2023, 3, 2, 3, 36, 21, 832000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 3, 2, 3, 46, 33, 588000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2023, 3, 2, 3, 32, 11, 695000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:079002598131:user-profile/d-iik9aga3atel/demo',\n",
       "  'UserProfileName': 'demo',\n",
       "  'DomainId': 'd-iik9aga3atel'},\n",
       " 'ResponseMetadata': {'RequestId': '38e3f2d7-d07e-4c9e-9b61-467be24cb4fb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '38e3f2d7-d07e-4c9e-9b61-467be24cb4fb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3116',\n",
       "   'date': 'Thu, 02 Mar 2023 03:46:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# KHOI:\n",
    "output_args = [f\"--output-config '{json.dumps(output_config)}'\" for output_config in output_configs]\n",
    "\n",
    "# Start Job\n",
    "processor.run(\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=processing_job_outputs,\n",
    "    arguments=output_args + [f\"--refit-trained-params '{json.dumps(refit_trained_params)}'\"],\n",
    "    wait=False,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name\n",
    ")\n",
    "\n",
    "# Status\n",
    "training_set_specific_path = f'{list(splitting_data_mapping.values())[0]}.default'\n",
    "s3_job_results_training_path = f\"{s3_output_base_path}/{list(splitting_data_mapping.keys())[0]}/{processing_job_name}/{training_set_specific_path.replace('.', '/')}\"\n",
    "print(f\"Job results (training set) are saved to S3 path: {s3_job_results_training_path}\")\n",
    "validation_set_specific_path = f'{list(splitting_data_mapping.values())[1]}.default'\n",
    "s3_job_results_validation_path = f\"{s3_output_base_path}/{list(splitting_data_mapping.keys())[1]}/{processing_job_name}/{validation_set_specific_path.replace('.', '/')}\"\n",
    "print(f\"Job results (validation set) are saved to S3 path: {s3_job_results_validation_path}\")\n",
    "\n",
    "job_result = sess.wait_for_processing_job(processing_job_name)\n",
    "job_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)Next Steps\n",
    "\n",
    "Now that data is available on S3 you can use other SageMaker components that take S3 URIs as input such as \n",
    "SageMaker Training, Built-in Algorithms, etc. Similarly you can load the dataset into a Pandas dataframe \n",
    "in this notebook for further inspection and work. The examples below show how to do both of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default optional steps do not run automatically, set `run_optional_steps` to True if you want to \n",
    "execute optional steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optional_steps = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will stop the below cells from executing if \"Run All Cells\" was used on the notebook.\n",
    "if not run_optional_steps:\n",
    "    raise SystemExit(\"Stop here. Do not automatically execute optional steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Load Processed Data into Pandas\n",
    "\n",
    "We use the [AWS SDK for pandas library](https://github.com/awslabs/aws-sdk-pandas) to load the exported \n",
    "dataset into a Pandas data frame for a preview of first 10000 rows.\n",
    "\n",
    "To turn on automated visualizations and data insights for your pandas data frame, import the sagemaker_datawrangler library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q awswrangler pandas\n",
    "import awswrangler as wr\n",
    "\n",
    "# Import sagemaker_datawrangler to show visualizations and automated data\n",
    "# quality insights, and export code to prepare data in a pandas data frame.\n",
    "try:\n",
    "    import sagemaker_datawrangler\n",
    "except ImportError:\n",
    "    print(\"sagemaker_datawrangler is not imported. Change your kernel to the Data Science 3.0 Kernel Image and try again.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{\"event_type\": \"ganymede.initialization\", \"event_status\": \"start\", \"app_context\": {\"ganymede_version\": \"0.3.8\", \"app_metadata\": {\"AppType\": \"KernelGateway\", \"DomainId\": \"d-iik9aga3atel\", \"UserProfileName\": \"demo\", \"ResourceArn\": \"arn:aws:sagemaker:us-east-1:079002598131:app/d-iik9aga3atel/demo/KernelGateway/datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395\", \"ResourceName\": \"datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395\", \"AppImageVersion\": \"\", \"Region\": \"us-east-1\", \"AccountId\": \"079002598131\"}}}\n",
      "INFO:root:DataFrame size: row_count = 10000, column_count = 11.\n",
      "INFO:root:Computing on the top 10000 rows of the DataFrame.\n",
      "INFO:root:Toggled to the sagemaker_datawrangler view.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d01976ecfb4abaa1103fd70ec19073"
      },
      "text/plain": [
       "      total_amount      ride_id_0  passenger_count  trip_distance  \\\n",
       "0        26.250000  2345053812090                1           7.98   \n",
       "1         9.740000  3616363168001                1           0.97   \n",
       "2        20.700001  2345053812210                1           8.03   \n",
       "3        13.200000  1692217947308                2           2.20   \n",
       "4        12.500000  1692217947853                1           2.70   \n",
       "...            ...            ...              ...            ...   \n",
       "9995      6.860000  3624953193593                2           0.91   \n",
       "9996     14.100000  2731599453867                3           3.46   \n",
       "9997     12.740000  3624953193717                4           2.76   \n",
       "9998     16.459999  1520420564183                1           2.35   \n",
       "9999      8.200000  3624953194196                1           1.90   \n",
       "\n",
       "      rate_code_id  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0                1             1    20.100000    0.5      0.5        5.15   \n",
       "1                1             1     7.700000    0.0      0.5        1.54   \n",
       "2                1             2    19.700001    0.5      0.5        0.00   \n",
       "3                1             1    10.000000    0.5      0.5        2.20   \n",
       "4                1             2    11.500000    0.5      0.5        0.00   \n",
       "...            ...           ...          ...    ...      ...         ...   \n",
       "9995             1             1     5.300000    0.0      0.5        1.06   \n",
       "9996             1             1    10.100000    0.5      0.5        3.00   \n",
       "9997             1             1     9.700000    0.5      0.5        2.04   \n",
       "9998             1             1    13.300000    0.0      0.5        2.66   \n",
       "9999             1             2     7.700000    0.0      0.5        0.00   \n",
       "\n",
       "      tolls_amount  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  \n",
       "...            ...  \n",
       "9995           0.0  \n",
       "9996           0.0  \n",
       "9997           0.0  \n",
       "9998           0.0  \n",
       "9999           0.0  \n",
       "\n",
       "[10000 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{\"event_type\": \"ganymede.initialization\", \"event_status\": \"success\", \"app_context\": {\"ganymede_version\": \"0.3.8\", \"app_metadata\": {\"AppType\": \"KernelGateway\", \"DomainId\": \"d-iik9aga3atel\", \"UserProfileName\": \"demo\", \"ResourceArn\": \"arn:aws:sagemaker:us-east-1:079002598131:app/d-iik9aga3atel/demo/KernelGateway/datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395\", \"ResourceName\": \"datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395\", \"AppImageVersion\": \"\", \"Region\": \"us-east-1\", \"AccountId\": \"079002598131\"}}, \"metadata\": {\"latency\": 0.14584922790527344}}\n"
     ]
    }
   ],
   "source": [
    "chunksize = 10000\n",
    "\n",
    "# KHOI: we are loading the training set here only\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_job_results_training_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_job_results_training_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "df = next(dfs)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)Train a model with SageMaker\n",
    "Now that the data has been processed, you may want to train a model using the data. The following shows an \n",
    "example of doing so using a popular algorithm - XGBoost. For more information on algorithms available in \n",
    "SageMaker, see [Getting Started with SageMaker Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). \n",
    "It is important to note that the following XGBoost objective ['binary', 'regression', 'multiclass'] \n",
    "hyperparameters, or content_type may not be suitable for the output data, and will require changes to \n",
    "train a proper model. Furthermore, for CSV training, the algorithm assumes that the target \n",
    "variable is in the first column. For more information on SageMaker XGBoost, \n",
    "see https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html.\n",
    "\n",
    "\n",
    "### Set Training Data and Validation Data paths\n",
    "We set the training input data path from the output of the Data Wrangler processing job.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input data path: s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/training/data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35/842a9df0-a299-4625-8f8a-ffca58929650/default\n",
      "Validation input data path: s3://sagemaker-us-east-1-079002598131/gsml-nyc-taxi-full-etl-ml-test-4-custompyspark-export-s3-via-notebook/export-flow-2023-03-02-03-32-10-53926e35/output/validation/data-wrangler-flow-processing-2023-03-02-03-32-10-53926e35/052787ea-75d7-4079-be2a-3d436a45b0c8/default\n"
     ]
    }
   ],
   "source": [
    "s3_training_input_path = s3_job_results_training_path\n",
    "print(f\"Training input data path: {s3_training_input_path}\")\n",
    "\n",
    "s3_validation_input_path = s3_job_results_validation_path\n",
    "print(f\"Validation input data path: {s3_validation_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the algorithm and training job\n",
    "\n",
    "The Training Job hyperparameters are set. For more information on XGBoost Hyperparameters, \n",
    "see https://xgboost.readthedocs.io/en/latest/parameter.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "# KHOI: switched to get newer XGBoost image: default was 1.2-1\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "\n",
    "# KHOI: set an output path where the trained model will be saved\n",
    "s3_training_job_output_prefix = f\"{s3_folder}/built-in-xgboost\"\n",
    "training_job_output_path = 's3://{}/{}/{}/output'.format(bucket, s3_training_job_output_prefix, 'nyc-taxi-full-built-in-xgboost')\n",
    "\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"10\",\n",
    "}\n",
    "train_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n",
    "    else \"text/csv\"\n",
    ")\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_training_input_path,\n",
    "    content_type=train_content_type,\n",
    "    distribution='ShardedByS3Key',  # testing\n",
    ")\n",
    "\n",
    "# KHOI: add validation input\n",
    "validation_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n",
    "    else \"text/csv\"\n",
    ")\n",
    "validation_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_validation_input_path,\n",
    "    content_type=validation_content_type,\n",
    "    distribution='ShardedByS3Key',   # testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Training Job\n",
    "\n",
    "The TrainingJob configurations are set using the SageMaker Python SDK Estimator, and which is fit using \n",
    "the training data from the Processing Job that was run earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # KHOI: training job without ShardedByS3Key\n",
    "# estimator = sagemaker.estimator.Estimator(\n",
    "#     container,\n",
    "#     iam_role,\n",
    "#     hyperparameters=hyperparameters,\n",
    "#     instance_count=6,\n",
    "#     instance_type=\"ml.m5.24xlarge\",\n",
    "#     volume_size=200,    # KHOI: must change this or will get diskspace error, default is 30GB\n",
    "# )\n",
    "# # estimator.fit({\"train\": train_input})\n",
    "# # KHOI: add validation inout\n",
    "# estimator.fit({\"train\": train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.135.1.tar.gz (673 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m674.0/674.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.26.69)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Collecting importlib-metadata<5.0,>=1.4.0\n",
      "  Using cached importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.69 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.69)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.13.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.69->boto3<2.0,>=1.26.28->sagemaker) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.135.1-py2.py3-none-any.whl size=911593 sha256=4c35e553a3e7382d596ca919780225357acddd1c42235d7c0d936a4f1c3a2ae9\n",
      "  Stored in directory: /root/.cache/pip/wheels/fe/8d/13/4676f5847fd7b702de26744c89720edb2c1e94ff829dab21ed\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: importlib-metadata, sagemaker\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed importlib-metadata-4.13.0 sagemaker-2.135.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsml-nyc-taxi-full-built-in-xgboost-2023-03-02-03-46-46\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "\n",
    "experiment_name = f\"gsml-nyc-taxi-full-built-in-xgboost-{time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())}\"\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment-run-2023-03-02-03-46-46\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"experiment-run-{time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())}\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KHOI: rerun training job with \"SharededByS3Key\"\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.24xlarge\",\n",
    "    # volume_size=200,    # KHOI: must change this or will get diskspace error, default is 30GB,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.experiments.run:run_name is explicitly supplied in load_run, which will be prioritized to load the Run object. In other words, the run name in the experiment config, fetched from the job environment or the current run context, will be ignored.\n",
      "INFO:sagemaker.experiments.run:The run (experiment-run-2023-03-02-03-46-46) under experiment (gsml-nyc-taxi-full-built-in-xgboost-2023-03-02-03-46-46) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-03-02-03-58-02-063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-02 03:58:02 Starting - Starting the training job...\n",
      "2023-03-02 03:58:18 Starting - Preparing the instances for training......\n",
      "2023-03-02 03:59:21 Downloading - Downloading input data.........\n",
      "2023-03-02 04:01:03 Training - Training image download completed. Training in progress..\u001b[32m[2023-03-02 04:01:04.378 ip-10-0-237-228.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:04:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:04:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[32mReturning the value itself\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:04:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:04:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:04:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2023-03-02 04:01:04.394 ip-10-0-200-21.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2023-03-02 04:01:04.406 ip-10-0-224-184.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:04:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35m[2023-03-02 04:01:04.363 ip-10-0-252-51.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:04:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:04:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:04:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:04:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:04:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[33m[2023-03-02 04:01:04.411 ip-10-0-218-106.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:04:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:04:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[33mReturning the value itself\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:04:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:04:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:04:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[36m[2023-03-02 04:01:04.440 ip-10-0-207-73.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:04:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:04:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[36mReturning the value itself\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:04:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:04:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:04:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:04:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:27:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:27:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:27:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:27:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:27:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:27:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:49:INFO] Distributed node training with 6 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6']\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:49:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:49:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:48:INFO] Distributed node training with 6 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6']\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:48:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:48:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:49:INFO] Distributed node training with 6 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6']\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:49:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] Distributed node training with 6 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6']\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:52:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:51:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[04:01:52] task NULL got new rank 3\u001b[0m\n",
      "\u001b[33m[04:01:52] task NULL got new rank 2\u001b[0m\n",
      "\u001b[36m[04:01:52] task NULL got new rank 1\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:52:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:52:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[32m[04:01:52] task NULL got new rank 4\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:52:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:52:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:52:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:52:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] Distributed node training with 6 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6']\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] start listen on algo-1:9099\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9099}\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] No data received from connection ('10.0.200.21', 39596). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] No data received from connection ('10.0.218.106', 34460). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] No data received from connection ('10.0.252.51', 33054). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:49:INFO] No data received from connection ('10.0.224.184', 47480). Closing.\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:49:INFO] Distributed node training with 6 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6']\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:49:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:51:INFO] No data received from connection ('10.0.237.228', 60220). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] No data received from connection ('10.0.207.73', 55062). Closing.\u001b[0m\n",
      "\u001b[34m[04:01:52] task NULL got new rank 0\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Recieve start signal from 10.0.200.21; assign rank 0\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Recieve start signal from 10.0.207.73; assign rank 1\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Recieve start signal from 10.0.218.106; assign rank 2\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Recieve start signal from 10.0.224.184; assign rank 3\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Recieve start signal from 10.0.237.228; assign rank 4\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Recieve start signal from 10.0.252.51; assign rank 5\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] @tracker All of 6 nodes getting started\u001b[0m\n",
      "\u001b[35m[04:01:52] task NULL got new rank 5\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:52:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:52:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] @tracker 0.13427019119262695 secs between node start and job finish\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] start listen on algo-1:9100\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9100}\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:52:INFO] No data received from connection ('10.0.200.21', 37740). Closing.\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:55:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[36m[04:01:56] task NULL got new rank 1\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:55:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[32m[04:01:56] task NULL got new rank 4\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:56:INFO] Train matrix has 175998374 rows and 10 columns\u001b[0m\n",
      "\u001b[32m[2023-03-02:04:01:56:INFO] Validation matrix has 175998374 rows\u001b[0m\n",
      "\u001b[32m[2023-03-02 04:01:56.475 ip-10-0-237-228.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:55:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[04:01:56] task NULL got new rank 3\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Train matrix has 174981242 rows and 10 columns\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Validation matrix has 174981242 rows\u001b[0m\n",
      "\u001b[34m[2023-03-02 04:01:56.386 ip-10-0-224-184.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:55:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[35m[04:01:56] task NULL got new rank 5\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:56:INFO] Train matrix has 176042533 rows and 10 columns\u001b[0m\n",
      "\u001b[35m[2023-03-02:04:01:56:INFO] Validation matrix has 176042533 rows\u001b[0m\n",
      "\u001b[35m[2023-03-02 04:01:56.475 ip-10-0-252-51.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:55:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[33m[04:01:56] task NULL got new rank 2\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:56:INFO] Train matrix has 174961345 rows and 10 columns\u001b[0m\n",
      "\u001b[33m[2023-03-02:04:01:56:INFO] Validation matrix has 174961345 rows\u001b[0m\n",
      "\u001b[33m[2023-03-02 04:01:56.298 ip-10-0-218-106.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:55:INFO] No data received from connection ('10.0.237.228', 52852). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:55:INFO] No data received from connection ('10.0.252.51', 34576). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:55:INFO] No data received from connection ('10.0.207.73', 52104). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:55:INFO] No data received from connection ('10.0.218.106', 33654). Closing.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:55:INFO] No data received from connection ('10.0.224.184', 47204). Closing.\u001b[0m\n",
      "\u001b[34m[04:01:55] task NULL got new rank 0\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Recieve start signal from 10.0.200.21; assign rank 0\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Recieve start signal from 10.0.207.73; assign rank 1\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Recieve start signal from 10.0.218.106; assign rank 2\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Recieve start signal from 10.0.224.184; assign rank 3\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Recieve start signal from 10.0.237.228; assign rank 4\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Train matrix has 176030083 rows and 10 columns\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Validation matrix has 176030083 rows\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] Recieve start signal from 10.0.252.51; assign rank 5\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:01:56:INFO] @tracker All of 6 nodes getting started\u001b[0m\n",
      "\u001b[34m[2023-03-02 04:01:56.474 ip-10-0-200-21.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:56:INFO] Train matrix has 176025188 rows and 10 columns\u001b[0m\n",
      "\u001b[36m[2023-03-02:04:01:56:INFO] Validation matrix has 176025188 rows\u001b[0m\n",
      "\u001b[36m[2023-03-02 04:01:56.297 ip-10-0-207-73.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-02:04:03:36:INFO] [0]#011train-rmse:151.46197#011validation-rmse:151.45602\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# KHOI: add validation input\n",
    "with sagemaker.experiments.load_run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=sagemaker.session.Session()) as run:\n",
    "# with sagemaker.experiments.Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=sagemaker.session.Session()) as run:\n",
    "# with sagemaker.experiments.Run() as run:\n",
    "    # run.experiment_config = experiment_config\n",
    "    # run.log_parameter(\n",
    "        # {\"num_train_samples\": len(train_input), \"num_validation_samples\": len(validation_input)},\n",
    "        \n",
    "    # )\n",
    "    # run.log_parameters()\n",
    "\n",
    "    # run.log_metric(name='Khoi', value='0.1')\n",
    "\n",
    "    # run.log_metric(name=metric_type+\":loss\", value=loss, step=epoch)\n",
    "    # run.log_metric(name=metric_type+\":accuracy\", value=accuracy, step=epoch)\n",
    "    # run.log_confusion_matrix(target, pred, \"Confusion-Matrix-Test-Data\")\n",
    "    # Log a metric over the course of a run at each epoch\n",
    "    # run.log_metric(name=\"test:loss\", value=loss, step=epoch)\n",
    "\n",
    "    # Define values for the parameters to log\n",
    "    # run.log_parameter(\"batch_size\", batch_size)\n",
    "    # run.log_parameter(\"epochs\", epochs)\n",
    "    # run.log_parameter(\"dropout\", 0.5)\n",
    "    \n",
    "    estimator.fit({\"train\": train_input, 'validation': validation_input},\n",
    "                  # experiment_config=experiment_config\n",
    "                  # experiment_config=run.experiment_config\n",
    "                  \n",
    "                 )\n",
    "\n",
    "\n",
    "#     score = estimator.evaluate(x_test, y_test, verbose=0)\n",
    "#     print(\"Test loss:\", score[0])\n",
    "#     print(\"Test accuracy:\", score[1])\n",
    "    \n",
    "#     # Define metrics to log\n",
    "#     run.log_metric(name = \"Final Test Loss\", value = score[0])\n",
    "#     run.log_metric(name = \"Final Test Loss\", value = score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model there are a number of different things you can do. \n",
    "For more details on training with SageMaker, please see \n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = client.describe_hyper_parameter_tuning_job(\n",
    "        HyperParameterTuningJobName=tuning_job_name\n",
    "    )[\"BestTrainingJob\"][\"TrainingJobName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Ref: https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_parquet_input_training.html\n",
    "#  Get the `training_job_name` from the output of `estimator.fit()` above\n",
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "\n",
    "metric_name = \"validation:rmse\"\n",
    "\n",
    "metrics_dataframe = TrainingJobAnalytics(\n",
    "    training_job_name='sagemaker-xgboost-2023-02-20-20-33-16-657', metric_names=[metric_name]\n",
    ").dataframe()\n",
    "plt = metrics_dataframe.plot(\n",
    "    kind=\"line\", figsize=(12, 5), x=\"timestamp\", y=\"value\", style=\"b.\", legend=False\n",
    ")\n",
    "plt.set_ylabel(metric_name);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_nyctaxi_full_experiment = sagemaker.experiment.Experiment.create(experiment_name=experiment_name, \n",
    "                                              description=\"GSML test\", \n",
    "                                              sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "trial = sagemaker.trial.Trial.create(trial_name=run_name, \n",
    "                     experiment_name=xgboost_nyctaxi_full_experiment.experiment_name,\n",
    "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "# container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker XGBoost container: {} ({})'.format(container, region_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING 2023-02-28\n",
    "Using SageMaker Experiment SDK, which is not recommended anymore: https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-additional-sdk.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# role = sagemaker.get_execution_role()\n",
    "# sm_sess = sagemaker.session.Session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "demo_experiment = Experiment.create(experiment_name = \"DEMO-{}\".format(create_date),\n",
    "                                    description = \"Demo experiment\",\n",
    "                                    tags = [{'Key': 'demo-experiments', 'Value': 'demo1'}])\n",
    "\n",
    "demo_trial = Trial.create(trial_name = \"DEMO-{}\".format(create_date),\n",
    "                          experiment_name = demo_experiment.experiment_name,\n",
    "                          tags = [{'Key': 'demo-trials', 'Value': 'demo1'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KHOI: rerun training job with \"SharededByS3Key\"\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.24xlarge\",\n",
    "    volume_size=200,    # KHOI: must change this or will get diskspace error, default is 30GB,\n",
    ")\n",
    "\n",
    "estimator.fit({\"train\": train_input, 'validation': validation_input},\n",
    "              # experiment_config=experiment_config\n",
    "              experiment_config={\n",
    "                  # \"ExperimentName\"\n",
    "                  \"TrialName\" : demo_trial.trial_name,\n",
    "                  \"TrialComponentDisplayName\" : \"TrainingJob\",\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
