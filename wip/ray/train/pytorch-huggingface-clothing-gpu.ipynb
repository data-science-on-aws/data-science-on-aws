{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf5dc9c-5f7c-45dd-a702-89fb0ecf58e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 07:57:30,115\tINFO trainer.py:243 -- Trainer logs will be logged in: /home/ray/ray_results/train_2022-06-24_07-57-30\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m 2022-06-24 07:57:35,279\tINFO torch.py:347 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "2022-06-24 07:57:35,350\tINFO trainer.py:249 -- Run results will be logged in: /home/ray/ray_results/train_2022-06-24_07-57-30/run_001\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m 2022-06-24 07:57:35,305\tINFO torch.py:347 -- Setting up process group for: env:// [rank=1, world_size=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m Downloading and preparing dataset csv/default to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m Downloading and preparing dataset csv/default to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m Dataset csv downloaded and prepared to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m Dataset csv downloaded and prepared to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 5486.34it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 896.51it/s]\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 4739.33it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1169.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 730.14it/s].7)\u001b[0m \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 757.85it/s].32)\u001b[0m \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [00:00<00:00, 440kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [00:00<00:00, 460kB/s]\n",
      "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\n",
      "Downloading:   4%|â–Ž         | 32.0k/878k [00:00<00:03, 265kB/s]\n",
      "Downloading:   5%|â–         | 40.0k/878k [00:00<00:02, 311kB/s]\n",
      "Downloading:  11%|â–ˆâ–        | 100k/878k [00:00<00:01, 439kB/s] \n",
      "Downloading:  12%|â–ˆâ–        | 108k/878k [00:00<00:01, 437kB/s] \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878k/878k [00:00<00:00, 2.02MB/s]\n",
      "Downloading:  25%|â–ˆâ–ˆâ–Œ       | 220k/878k [00:00<00:01, 631kB/s]\n",
      "Downloading:  34%|â–ˆâ–ˆâ–ˆâ–      | 300k/878k [00:00<00:00, 625kB/s]\n",
      "Downloading:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 556k/878k [00:00<00:00, 1.11MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878k/878k [00:00<00:00, 1.22MB/s]\n",
      "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\n",
      "Downloading:   6%|â–‹         | 28.0k/446k [00:00<00:01, 222kB/s]\n",
      "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\n",
      "Downloading:  30%|â–ˆâ–ˆâ–‰       | 133k/446k [00:00<00:00, 572kB/s] \n",
      "Downloading:   8%|â–Š         | 36.0k/446k [00:00<00:01, 281kB/s]\n",
      "Downloading:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 197k/446k [00:00<00:00, 545kB/s]\n",
      "Downloading:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 309k/446k [00:00<00:00, 673kB/s]\n",
      "Downloading:  19%|â–ˆâ–‰        | 85.0k/446k [00:00<00:01, 340kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446k/446k [00:00<00:00, 694kB/s]\n",
      "Downloading:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 197k/446k [00:00<00:00, 582kB/s] \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446k/446k [00:00<00:00, 863kB/s] \n",
      "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\n",
      "Downloading:   3%|â–Ž         | 37.0k/1.29M [00:00<00:04, 293kB/s]\n",
      "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\n",
      "Downloading:   7%|â–‹         | 91.0k/1.29M [00:00<00:03, 371kB/s]\n",
      "Downloading:   5%|â–         | 64.0k/1.29M [00:00<00:02, 513kB/s]\n",
      "Downloading:  13%|â–ˆâ–Ž        | 171k/1.29M [00:00<00:02, 489kB/s] \n",
      "Downloading:  13%|â–ˆâ–Ž        | 176k/1.29M [00:00<00:01, 729kB/s] \n",
      "Downloading:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 475k/1.29M [00:00<00:00, 1.24MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29M/1.29M [00:00<00:00, 2.23MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.29M/1.29M [00:00<00:00, 2.91MB/s]\n",
      "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 1.31M/478M [00:00<00:36, 13.8MB/s]\n",
      "Downloading:   0%|          | 772k/478M [00:00<01:04, 7.76MB/s]\n",
      "Downloading:   1%|          | 5.26M/478M [00:00<00:16, 30.0MB/s]\n",
      "Downloading:   0%|          | 1.49M/478M [00:00<01:04, 7.74MB/s]\n",
      "Downloading:   2%|â–         | 9.49M/478M [00:00<00:13, 36.6MB/s]\n",
      "Downloading:   0%|          | 2.25M/478M [00:00<01:04, 7.76MB/s]\n",
      "Downloading:   3%|â–Ž         | 13.7M/478M [00:00<00:12, 39.7MB/s]\n",
      "Downloading:   1%|          | 3.00M/478M [00:00<01:04, 7.75MB/s]\n",
      "Downloading:   4%|â–         | 18.0M/478M [00:00<00:11, 41.5MB/s]\n",
      "Downloading:   1%|          | 3.74M/478M [00:00<01:04, 7.73MB/s]\n",
      "Downloading:   5%|â–         | 22.3M/478M [00:00<00:11, 42.6MB/s]\n",
      "Downloading:   1%|          | 4.97M/478M [00:00<00:52, 9.47MB/s]\n",
      "Downloading:   6%|â–Œ         | 26.3M/478M [00:00<00:11, 40.9MB/s]\n",
      "Downloading:   3%|â–Ž         | 13.1M/478M [00:00<00:17, 27.6MB/s]\n",
      "Downloading:   6%|â–‹         | 30.2M/478M [00:00<00:11, 40.0MB/s]\n",
      "Downloading:   4%|â–Ž         | 17.4M/478M [00:00<00:14, 33.1MB/s]\n",
      "Downloading:   7%|â–‹         | 34.5M/478M [00:00<00:11, 41.5MB/s]\n",
      "Downloading:   5%|â–         | 21.8M/478M [00:01<00:12, 37.0MB/s]\n",
      "Downloading:   8%|â–Š         | 38.8M/478M [00:01<00:10, 42.7MB/s]\n",
      "Downloading:   5%|â–Œ         | 26.1M/478M [00:01<00:11, 39.6MB/s]\n",
      "Downloading:   9%|â–‰         | 43.2M/478M [00:01<00:10, 43.6MB/s]\n",
      "Downloading:   6%|â–‹         | 30.5M/478M [00:01<00:11, 41.6MB/s]\n",
      "Downloading:  10%|â–‰         | 47.6M/478M [00:01<00:10, 44.2MB/s]\n",
      "Downloading:   7%|â–‹         | 34.9M/478M [00:01<00:10, 42.9MB/s]\n",
      "Downloading:  11%|â–ˆ         | 51.9M/478M [00:01<00:10, 44.6MB/s]\n",
      "Downloading:   8%|â–Š         | 39.3M/478M [00:01<00:10, 43.9MB/s]\n",
      "Downloading:  12%|â–ˆâ–        | 56.2M/478M [00:01<00:09, 44.7MB/s]\n",
      "Downloading:   9%|â–‰         | 43.8M/478M [00:01<00:10, 44.7MB/s]\n",
      "Downloading:  13%|â–ˆâ–Ž        | 60.5M/478M [00:01<00:09, 44.9MB/s]\n",
      "Downloading:  10%|â–ˆ         | 48.2M/478M [00:01<00:09, 45.3MB/s]\n",
      "Downloading:  14%|â–ˆâ–Ž        | 64.9M/478M [00:01<00:09, 45.1MB/s]\n",
      "Downloading:  11%|â–ˆ         | 52.6M/478M [00:01<00:09, 45.5MB/s]\n",
      "Downloading:  14%|â–ˆâ–        | 69.2M/478M [00:01<00:09, 45.3MB/s]\n",
      "Downloading:  12%|â–ˆâ–        | 57.0M/478M [00:01<00:09, 45.7MB/s]\n",
      "Downloading:  15%|â–ˆâ–Œ        | 73.6M/478M [00:01<00:09, 45.5MB/s]\n",
      "Downloading:  13%|â–ˆâ–Ž        | 61.4M/478M [00:01<00:09, 45.9MB/s]\n",
      "Downloading:  16%|â–ˆâ–‹        | 78.0M/478M [00:01<00:09, 45.4MB/s]\n",
      "Downloading:  14%|â–ˆâ–        | 65.8M/478M [00:02<00:09, 45.9MB/s]\n",
      "Downloading:  17%|â–ˆâ–‹        | 82.3M/478M [00:02<00:09, 45.4MB/s]\n",
      "Downloading:  15%|â–ˆâ–        | 70.2M/478M [00:02<00:09, 45.9MB/s]\n",
      "Downloading:  18%|â–ˆâ–Š        | 86.6M/478M [00:02<00:09, 45.4MB/s]\n",
      "Downloading:  16%|â–ˆâ–Œ        | 74.6M/478M [00:02<00:09, 45.9MB/s]\n",
      "Downloading:  19%|â–ˆâ–‰        | 91.0M/478M [00:02<00:08, 45.4MB/s]\n",
      "Downloading:  17%|â–ˆâ–‹        | 78.9M/478M [00:02<00:09, 45.8MB/s]\n",
      "Downloading:  20%|â–ˆâ–‰        | 95.3M/478M [00:02<00:08, 45.3MB/s]\n",
      "Downloading:  17%|â–ˆâ–‹        | 83.3M/478M [00:02<00:09, 45.8MB/s]\n",
      "Downloading:  21%|â–ˆâ–ˆ        | 99.6M/478M [00:02<00:08, 45.3MB/s]\n",
      "Downloading:  18%|â–ˆâ–Š        | 87.7M/478M [00:02<00:08, 45.8MB/s]\n",
      "Downloading:  22%|â–ˆâ–ˆâ–       | 104M/478M [00:02<00:08, 45.4MB/s] \n",
      "Downloading:  19%|â–ˆâ–‰        | 92.1M/478M [00:02<00:08, 45.9MB/s]\n",
      "Downloading:  23%|â–ˆâ–ˆâ–Ž       | 108M/478M [00:02<00:08, 45.3MB/s]\n",
      "Downloading:  20%|â–ˆâ–ˆ        | 96.5M/478M [00:02<00:08, 45.7MB/s]\n",
      "Downloading:  24%|â–ˆâ–ˆâ–Ž       | 113M/478M [00:02<00:08, 45.5MB/s]\n",
      "Downloading:  21%|â–ˆâ–ˆ        | 101M/478M [00:02<00:08, 45.7MB/s] \n",
      "Downloading:  24%|â–ˆâ–ˆâ–       | 117M/478M [00:02<00:08, 45.5MB/s]\n",
      "Downloading:  22%|â–ˆâ–ˆâ–       | 105M/478M [00:02<00:08, 45.7MB/s]\n",
      "Downloading:  25%|â–ˆâ–ˆâ–Œ       | 121M/478M [00:02<00:08, 45.5MB/s]\n",
      "Downloading:  23%|â–ˆâ–ˆâ–Ž       | 110M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  26%|â–ˆâ–ˆâ–‹       | 126M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  24%|â–ˆâ–ˆâ–       | 114M/478M [00:03<00:08, 45.4MB/s]\n",
      "Downloading:  27%|â–ˆâ–ˆâ–‹       | 130M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  25%|â–ˆâ–ˆâ–       | 118M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  28%|â–ˆâ–ˆâ–Š       | 134M/478M [00:03<00:07, 45.6MB/s]\n",
      "Downloading:  26%|â–ˆâ–ˆâ–Œ       | 123M/478M [00:03<00:08, 45.3MB/s]\n",
      "Downloading:  29%|â–ˆâ–ˆâ–‰       | 139M/478M [00:03<00:07, 45.6MB/s]\n",
      "Downloading:  27%|â–ˆâ–ˆâ–‹       | 127M/478M [00:03<00:08, 45.3MB/s]\n",
      "Downloading:  30%|â–ˆâ–ˆâ–‰       | 143M/478M [00:03<00:07, 45.5MB/s]\n",
      "Downloading:  27%|â–ˆâ–ˆâ–‹       | 131M/478M [00:03<00:08, 45.3MB/s]\n",
      "Downloading:  31%|â–ˆâ–ˆâ–ˆ       | 147M/478M [00:03<00:07, 45.6MB/s]\n",
      "Downloading:  28%|â–ˆâ–ˆâ–Š       | 136M/478M [00:03<00:07, 45.3MB/s]\n",
      "Downloading:  32%|â–ˆâ–ˆâ–ˆâ–      | 152M/478M [00:03<00:07, 44.7MB/s]\n",
      "Downloading:  29%|â–ˆâ–ˆâ–‰       | 140M/478M [00:03<00:07, 45.2MB/s]\n",
      "Downloading:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 156M/478M [00:03<00:07, 44.8MB/s]\n",
      "Downloading:  30%|â–ˆâ–ˆâ–ˆ       | 144M/478M [00:03<00:07, 45.2MB/s]\n",
      "Downloading:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 160M/478M [00:03<00:07, 44.9MB/s]\n",
      "Downloading:  31%|â–ˆâ–ˆâ–ˆ       | 149M/478M [00:03<00:07, 45.4MB/s]\n",
      "Downloading:  34%|â–ˆâ–ˆâ–ˆâ–      | 165M/478M [00:03<00:07, 45.0MB/s]\n",
      "Downloading:  32%|â–ˆâ–ˆâ–ˆâ–      | 153M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 169M/478M [00:04<00:07, 45.0MB/s]\n",
      "Downloading:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 157M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 173M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  34%|â–ˆâ–ˆâ–ˆâ–      | 162M/478M [00:04<00:07, 45.4MB/s]\n",
      "Downloading:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 182M/478M [00:04<00:06, 45.7MB/s]\n",
      "Downloading:  35%|â–ˆâ–ˆâ–ˆâ–      | 166M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 187M/478M [00:04<00:06, 45.8MB/s]\n",
      "Downloading:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 170M/478M [00:04<00:07, 45.1MB/s]\n",
      "Downloading:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 191M/478M [00:04<00:06, 46.0MB/s]\n",
      "Downloading:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 175M/478M [00:04<00:07, 44.4MB/s]\n",
      "Downloading:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 195M/478M [00:04<00:06, 46.1MB/s]\n",
      "Downloading:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 179M/478M [00:04<00:07, 43.7MB/s]\n",
      "Downloading:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200M/478M [00:04<00:06, 46.3MB/s]\n",
      "Downloading:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 183M/478M [00:04<00:07, 44.2MB/s]\n",
      "Downloading:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 204M/478M [00:04<00:06, 46.1MB/s]\n",
      "Downloading:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 187M/478M [00:04<00:06, 44.6MB/s]\n",
      "Downloading:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 209M/478M [00:04<00:06, 46.0MB/s]\n",
      "Downloading:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 192M/478M [00:04<00:06, 45.1MB/s]\n",
      "Downloading:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 213M/478M [00:05<00:06, 46.0MB/s]\n",
      "Downloading:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 196M/478M [00:05<00:06, 45.2MB/s]\n",
      "Downloading:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 218M/478M [00:05<00:05, 46.0MB/s]\n",
      "Downloading:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200M/478M [00:05<00:06, 43.9MB/s]\n",
      "Downloading:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 222M/478M [00:05<00:05, 46.0MB/s]\n",
      "Downloading:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 205M/478M [00:05<00:06, 44.5MB/s]\n",
      "Downloading:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 226M/478M [00:05<00:05, 45.8MB/s]\n",
      "Downloading:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 209M/478M [00:05<00:06, 44.9MB/s]\n",
      "Downloading:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 231M/478M [00:05<00:05, 45.7MB/s]\n",
      "Downloading:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 214M/478M [00:05<00:06, 44.6MB/s]\n",
      "Downloading:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 235M/478M [00:05<00:05, 45.7MB/s]\n",
      "Downloading:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 218M/478M [00:05<00:06, 44.9MB/s]\n",
      "Downloading:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 239M/478M [00:05<00:05, 45.8MB/s]\n",
      "Downloading:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 222M/478M [00:05<00:05, 45.0MB/s]\n",
      "Downloading:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 244M/478M [00:05<00:05, 45.7MB/s]\n",
      "Downloading:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 226M/478M [00:05<00:05, 45.1MB/s]\n",
      "Downloading:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248M/478M [00:05<00:05, 45.6MB/s]\n",
      "Downloading:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 231M/478M [00:05<00:05, 45.1MB/s]\n",
      "Downloading:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 252M/478M [00:05<00:05, 45.6MB/s]\n",
      "Downloading:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 235M/478M [00:05<00:05, 45.0MB/s]\n",
      "Downloading:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 257M/478M [00:06<00:05, 45.7MB/s]\n",
      "Downloading:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 239M/478M [00:06<00:05, 44.1MB/s]\n",
      "Downloading:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 261M/478M [00:06<00:04, 45.7MB/s]\n",
      "Downloading:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 244M/478M [00:06<00:05, 43.8MB/s]\n",
      "Downloading:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 266M/478M [00:06<00:04, 45.8MB/s]\n",
      "Downloading:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248M/478M [00:06<00:05, 44.2MB/s]\n",
      "Downloading:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 270M/478M [00:06<00:04, 45.8MB/s]\n",
      "Downloading:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 252M/478M [00:06<00:05, 44.7MB/s]\n",
      "Downloading:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 274M/478M [00:06<00:04, 45.7MB/s]\n",
      "Downloading:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 257M/478M [00:06<00:05, 44.9MB/s]\n",
      "Downloading:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 279M/478M [00:06<00:04, 45.8MB/s]\n",
      "Downloading:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 261M/478M [00:06<00:05, 45.2MB/s]\n",
      "Downloading:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 265M/478M [00:06<00:04, 45.5MB/s]\n",
      "Downloading:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 283M/478M [00:06<00:04, 43.3MB/s]\n",
      "Downloading:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 270M/478M [00:06<00:04, 45.7MB/s]\n",
      "Downloading:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 287M/478M [00:06<00:04, 44.0MB/s]\n",
      "Downloading:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 274M/478M [00:06<00:04, 45.9MB/s]\n",
      "Downloading:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 292M/478M [00:06<00:04, 44.6MB/s]\n",
      "Downloading:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 279M/478M [00:06<00:04, 46.0MB/s]\n",
      "Downloading:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296M/478M [00:06<00:04, 45.1MB/s]\n",
      "Downloading:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 283M/478M [00:07<00:04, 46.0MB/s]\n",
      "Downloading:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 301M/478M [00:07<00:04, 45.4MB/s]\n",
      "Downloading:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 287M/478M [00:07<00:04, 45.3MB/s]\n",
      "Downloading:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 305M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 292M/478M [00:07<00:04, 44.5MB/s]\n",
      "Downloading:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 309M/478M [00:07<00:03, 45.4MB/s]\n",
      "Downloading:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296M/478M [00:07<00:04, 45.3MB/s]\n",
      "Downloading:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 314M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 301M/478M [00:07<00:04, 45.7MB/s]\n",
      "Downloading:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 318M/478M [00:07<00:03, 45.6MB/s]\n",
      "Downloading:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 305M/478M [00:07<00:03, 46.2MB/s]\n",
      "Downloading:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 323M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 310M/478M [00:07<00:03, 46.6MB/s]\n",
      "Downloading:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 327M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 314M/478M [00:07<00:03, 46.8MB/s]\n",
      "Downloading:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 331M/478M [00:07<00:03, 45.8MB/s]\n",
      "Downloading:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 319M/478M [00:07<00:03, 46.9MB/s]\n",
      "Downloading:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 336M/478M [00:07<00:03, 45.8MB/s]\n",
      "Downloading:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 323M/478M [00:07<00:03, 46.8MB/s]\n",
      "Downloading:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 340M/478M [00:07<00:03, 45.9MB/s]\n",
      "Downloading:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 328M/478M [00:08<00:03, 46.8MB/s]\n",
      "Downloading:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 345M/478M [00:08<00:03, 45.8MB/s]\n",
      "Downloading:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 332M/478M [00:08<00:03, 46.9MB/s]\n",
      "Downloading:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 349M/478M [00:08<00:02, 45.7MB/s]\n",
      "Downloading:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 337M/478M [00:08<00:03, 46.9MB/s]\n",
      "Downloading:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 353M/478M [00:08<00:02, 45.4MB/s]\n",
      "Downloading:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 341M/478M [00:08<00:03, 46.8MB/s]\n",
      "Downloading:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 358M/478M [00:08<00:02, 45.3MB/s]\n",
      "Downloading:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 346M/478M [00:08<00:02, 46.7MB/s]\n",
      "Downloading:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 362M/478M [00:08<00:02, 45.3MB/s]\n",
      "Downloading:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 350M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 366M/478M [00:08<00:02, 45.2MB/s]\n",
      "Downloading:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 355M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 371M/478M [00:08<00:02, 44.4MB/s]\n",
      "Downloading:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 359M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 375M/478M [00:08<00:02, 43.4MB/s]\n",
      "Downloading:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 363M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 379M/478M [00:08<00:02, 43.9MB/s]\n",
      "Downloading:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 368M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 383M/478M [00:08<00:02, 44.2MB/s]\n",
      "Downloading:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 372M/478M [00:09<00:02, 46.5MB/s]\n",
      "Downloading:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 388M/478M [00:09<00:02, 44.6MB/s]\n",
      "Downloading:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 377M/478M [00:09<00:02, 46.4MB/s]\n",
      "Downloading:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 392M/478M [00:09<00:02, 44.8MB/s]\n",
      "Downloading:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 381M/478M [00:09<00:02, 46.3MB/s]\n",
      "Downloading:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 396M/478M [00:09<00:01, 45.0MB/s]\n",
      "Downloading:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 386M/478M [00:09<00:02, 46.2MB/s]\n",
      "Downloading:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 401M/478M [00:09<00:01, 44.9MB/s]\n",
      "Downloading:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 390M/478M [00:09<00:01, 46.4MB/s]\n",
      "Downloading:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 405M/478M [00:09<00:01, 44.9MB/s]\n",
      "Downloading:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 394M/478M [00:09<00:01, 46.2MB/s]\n",
      "Downloading:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 409M/478M [00:09<00:01, 45.0MB/s]\n",
      "Downloading:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 399M/478M [00:09<00:01, 46.4MB/s]\n",
      "Downloading:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 414M/478M [00:09<00:01, 45.2MB/s]\n",
      "Downloading:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 403M/478M [00:09<00:01, 45.4MB/s]\n",
      "Downloading:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 418M/478M [00:09<00:01, 45.2MB/s]\n",
      "Downloading:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 408M/478M [00:09<00:01, 44.5MB/s]\n",
      "Downloading:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 422M/478M [00:09<00:01, 45.2MB/s]\n",
      "Downloading:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 412M/478M [00:09<00:01, 44.9MB/s]\n",
      "Downloading:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 427M/478M [00:09<00:01, 45.1MB/s]\n",
      "Downloading:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 416M/478M [00:10<00:01, 45.3MB/s]\n",
      "Downloading:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 431M/478M [00:10<00:01, 45.2MB/s]\n",
      "Downloading:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 421M/478M [00:10<00:01, 45.5MB/s]\n",
      "Downloading:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 425M/478M [00:10<00:01, 45.7MB/s]\n",
      "Downloading:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 435M/478M [00:10<00:01, 28.5MB/s]\n",
      "Downloading:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 439M/478M [00:10<00:01, 32.0MB/s]\n",
      "Downloading:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 430M/478M [00:10<00:01, 33.2MB/s]\n",
      "Downloading:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 444M/478M [00:10<00:01, 35.0MB/s]\n",
      "Downloading:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 448M/478M [00:10<00:00, 37.7MB/s]\n",
      "Downloading:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 433M/478M [00:10<00:01, 30.9MB/s]\n",
      "Downloading:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 452M/478M [00:10<00:00, 39.7MB/s]\n",
      "Downloading:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 438M/478M [00:10<00:01, 34.3MB/s]\n",
      "Downloading:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 457M/478M [00:10<00:00, 41.3MB/s]\n",
      "Downloading:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 442M/478M [00:10<00:01, 37.1MB/s]\n",
      "Downloading:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 461M/478M [00:10<00:00, 41.6MB/s]\n",
      "Downloading:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 446M/478M [00:10<00:00, 39.3MB/s]\n",
      "Downloading:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 465M/478M [00:11<00:00, 41.8MB/s]\n",
      "Downloading:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 451M/478M [00:11<00:00, 41.0MB/s]\n",
      "Downloading:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 469M/478M [00:11<00:00, 42.8MB/s]\n",
      "Downloading:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 459M/478M [00:11<00:00, 43.5MB/s]\n",
      "Downloading:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 474M/478M [00:11<00:00, 43.1MB/s]\n",
      "Downloading:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 464M/478M [00:11<00:00, 44.2MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 478M/478M [00:11<00:00, 44.0MB/s]\n",
      "Downloading:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 468M/478M [00:11<00:00, 44.8MB/s]\n",
      "Downloading:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 473M/478M [00:11<00:00, 45.1MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 477M/478M [00:11<00:00, 45.8MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 478M/478M [00:11<00:00, 42.9MB/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:  17%|â–ˆâ–‹        | 1/6 [00:00<00:01,  4.95ba/s]\n",
      "Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:00<00:00,  5.36ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:00<00:00,  5.20ba/s]\n",
      "Running tokenizer on dataset:  17%|â–ˆâ–‹        | 1/6 [00:00<00:01,  4.87ba/s]\n",
      "Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:00<00:00,  5.26ba/s]\n",
      "Running tokenizer on dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:00<00:00,  5.41ba/s]\n",
      "Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:00<00:00,  5.42ba/s]\n",
      "Running tokenizer on dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00,  5.51ba/s]\n",
      "Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.85ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:00<00:00,  5.53ba/s]\n",
      "Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.64ba/s]\n",
      "Running tokenizer on dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00,  5.60ba/s]\n",
      "Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.94ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.06ba/s]\n",
      "Downloading builder script: 3.19kB [00:00, 1.54MB/s]                   \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]72.16.5.7)\u001b[0m \n",
      "Downloading builder script: 3.19kB [00:00, 1.92MB/s]                   \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]72.16.0.32)\u001b[0m \n",
      "  1%|          | 1/100 [00:00<00:28,  3.43it/s]7)\u001b[0m \n",
      "  2%|â–         | 2/100 [00:00<00:26,  3.68it/s]7)\u001b[0m \n",
      "  1%|          | 1/100 [00:00<00:27,  3.61it/s]32)\u001b[0m \n",
      "  3%|â–Ž         | 3/100 [00:00<00:24,  3.89it/s]7)\u001b[0m \n",
      "  2%|â–         | 2/100 [00:00<00:25,  3.83it/s]32)\u001b[0m \n",
      "  4%|â–         | 4/100 [00:01<00:23,  4.07it/s]7)\u001b[0m \n",
      "  3%|â–Ž         | 3/100 [00:00<00:24,  4.04it/s]32)\u001b[0m \n",
      "  5%|â–Œ         | 5/100 [00:01<00:22,  4.17it/s]7)\u001b[0m \n",
      "  4%|â–         | 4/100 [00:00<00:22,  4.22it/s]32)\u001b[0m \n",
      "  6%|â–Œ         | 6/100 [00:01<00:22,  4.25it/s]7)\u001b[0m \n",
      "  5%|â–Œ         | 5/100 [00:01<00:22,  4.31it/s]32)\u001b[0m \n",
      "  6%|â–Œ         | 6/100 [00:01<00:21,  4.37it/s]32)\u001b[0m \n",
      "  7%|â–‹         | 7/100 [00:01<00:21,  4.28it/s]7)\u001b[0m \n",
      "  8%|â–Š         | 8/100 [00:01<00:21,  4.31it/s]7)\u001b[0m \n",
      "  7%|â–‹         | 7/100 [00:01<00:21,  4.41it/s]32)\u001b[0m \n",
      "  9%|â–‰         | 9/100 [00:02<00:20,  4.35it/s]7)\u001b[0m \n",
      "  8%|â–Š         | 8/100 [00:01<00:20,  4.43it/s]32)\u001b[0m \n",
      " 10%|â–ˆ         | 10/100 [00:02<00:20,  4.37it/s])\u001b[0m \n",
      "  9%|â–‰         | 9/100 [00:02<00:20,  4.46it/s]32)\u001b[0m \n",
      " 10%|â–ˆ         | 10/100 [00:02<00:19,  4.50it/s]2)\u001b[0m \n",
      " 11%|â–ˆ         | 11/100 [00:02<00:20,  4.42it/s])\u001b[0m \n",
      " 11%|â–ˆ         | 11/100 [00:02<00:19,  4.54it/s]2)\u001b[0m \n",
      " 12%|â–ˆâ–        | 12/100 [00:02<00:19,  4.45it/s])\u001b[0m \n",
      " 12%|â–ˆâ–        | 12/100 [00:02<00:19,  4.56it/s]2)\u001b[0m \n",
      " 13%|â–ˆâ–Ž        | 13/100 [00:03<00:19,  4.46it/s])\u001b[0m \n",
      " 13%|â–ˆâ–Ž        | 13/100 [00:02<00:19,  4.57it/s]2)\u001b[0m \n",
      " 14%|â–ˆâ–        | 14/100 [00:03<00:19,  4.48it/s])\u001b[0m \n",
      " 15%|â–ˆâ–Œ        | 15/100 [00:03<00:18,  4.48it/s])\u001b[0m \n",
      " 14%|â–ˆâ–        | 14/100 [00:03<00:18,  4.57it/s]2)\u001b[0m \n",
      " 15%|â–ˆâ–Œ        | 15/100 [00:03<00:18,  4.59it/s]2)\u001b[0m \n",
      " 16%|â–ˆâ–Œ        | 16/100 [00:03<00:18,  4.48it/s])\u001b[0m \n",
      " 16%|â–ˆâ–Œ        | 16/100 [00:03<00:18,  4.59it/s]2)\u001b[0m \n",
      " 17%|â–ˆâ–‹        | 17/100 [00:03<00:18,  4.49it/s])\u001b[0m \n",
      " 17%|â–ˆâ–‹        | 17/100 [00:03<00:18,  4.60it/s]2)\u001b[0m \n",
      " 18%|â–ˆâ–Š        | 18/100 [00:04<00:18,  4.51it/s])\u001b[0m \n",
      " 18%|â–ˆâ–Š        | 18/100 [00:04<00:17,  4.60it/s]2)\u001b[0m \n",
      " 19%|â–ˆâ–‰        | 19/100 [00:04<00:17,  4.51it/s])\u001b[0m \n",
      " 19%|â–ˆâ–‰        | 19/100 [00:04<00:17,  4.60it/s]2)\u001b[0m \n",
      " 20%|â–ˆâ–ˆ        | 20/100 [00:04<00:17,  4.51it/s])\u001b[0m \n",
      " 20%|â–ˆâ–ˆ        | 20/100 [00:04<00:17,  4.61it/s]2)\u001b[0m \n",
      " 21%|â–ˆâ–ˆ        | 21/100 [00:04<00:17,  4.53it/s])\u001b[0m \n",
      " 21%|â–ˆâ–ˆ        | 21/100 [00:04<00:17,  4.61it/s]2)\u001b[0m \n",
      " 22%|â–ˆâ–ˆâ–       | 22/100 [00:05<00:17,  4.54it/s])\u001b[0m \n",
      " 22%|â–ˆâ–ˆâ–       | 22/100 [00:04<00:16,  4.61it/s]2)\u001b[0m \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:05<00:16,  4.55it/s])\u001b[0m \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:05<00:16,  4.61it/s]2)\u001b[0m \n",
      " 24%|â–ˆâ–ˆâ–       | 24/100 [00:05<00:16,  4.54it/s])\u001b[0m \n",
      " 24%|â–ˆâ–ˆâ–       | 24/100 [00:05<00:16,  4.61it/s]2)\u001b[0m \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:16,  4.50it/s])\u001b[0m \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:16,  4.61it/s]2)\u001b[0m \n",
      " 26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:05<00:16,  4.51it/s])\u001b[0m \n",
      " 26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:05<00:16,  4.60it/s]2)\u001b[0m \n",
      " 27%|â–ˆâ–ˆâ–‹       | 27/100 [00:06<00:16,  4.53it/s])\u001b[0m \n",
      " 27%|â–ˆâ–ˆâ–‹       | 27/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 28%|â–ˆâ–ˆâ–Š       | 28/100 [00:06<00:15,  4.54it/s])\u001b[0m \n",
      " 28%|â–ˆâ–ˆâ–Š       | 28/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:15,  4.52it/s])\u001b[0m \n",
      " 29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:06<00:15,  4.53it/s])\u001b[0m \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:07<00:15,  4.53it/s])\u001b[0m \n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:06<00:14,  4.61it/s]2)\u001b[0m \n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:07<00:15,  4.52it/s])\u001b[0m \n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:07<00:14,  4.60it/s]2)\u001b[0m \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:07<00:14,  4.53it/s])\u001b[0m \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:07<00:14,  4.60it/s]2)\u001b[0m \n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:14,  4.51it/s])\u001b[0m \n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:14,  4.60it/s]2)\u001b[0m \n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:14,  4.49it/s])\u001b[0m \n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:14,  4.61it/s]2)\u001b[0m \n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:08<00:14,  4.50it/s])\u001b[0m \n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:13,  4.60it/s]2)\u001b[0m \n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:08<00:13,  4.53it/s])\u001b[0m \n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:08<00:13,  4.61it/s]2)\u001b[0m \n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:08<00:13,  4.54it/s])\u001b[0m \n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:08<00:13,  4.60it/s]2)\u001b[0m \n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:08<00:13,  4.53it/s])\u001b[0m \n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:08<00:13,  4.59it/s]2)\u001b[0m \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:09<00:13,  4.54it/s])\u001b[0m \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:13,  4.60it/s]2)\u001b[0m \n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:09<00:13,  4.54it/s])\u001b[0m \n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:09<00:12,  4.60it/s]2)\u001b[0m \n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:09<00:12,  4.53it/s])\u001b[0m \n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:09<00:12,  4.60it/s]2)\u001b[0m \n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:09<00:12,  4.53it/s])\u001b[0m \n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:09<00:12,  4.60it/s]2)\u001b[0m \n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:09<00:12,  4.52it/s])\u001b[0m \n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:09<00:12,  4.59it/s]2)\u001b[0m \n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:10<00:12,  4.51it/s])\u001b[0m \n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:09<00:12,  4.58it/s]2)\u001b[0m \n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:10<00:11,  4.54it/s])\u001b[0m \n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:10<00:11,  4.59it/s]2)\u001b[0m \n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:10<00:11,  4.54it/s])\u001b[0m \n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:10<00:11,  4.59it/s]2)\u001b[0m \n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:10<00:11,  4.53it/s])\u001b[0m \n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:10<00:11,  4.60it/s]2)\u001b[0m \n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:11<00:11,  4.53it/s])\u001b[0m \n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:11,  4.60it/s]2)\u001b[0m \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:11<00:11,  4.51it/s])\u001b[0m \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:11<00:10,  4.49it/s])\u001b[0m \n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:11<00:10,  4.50it/s])\u001b[0m \n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:11<00:10,  4.52it/s])\u001b[0m \n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:12<00:10,  4.50it/s])\u001b[0m \n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:11<00:09,  4.60it/s]2)\u001b[0m \n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:12<00:09,  4.52it/s])\u001b[0m \n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:12<00:09,  4.53it/s])\u001b[0m \n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:12<00:09,  4.54it/s])\u001b[0m \n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:12<00:09,  4.54it/s])\u001b[0m \n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:13<00:09,  4.50it/s])\u001b[0m \n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:12<00:08,  4.61it/s]2)\u001b[0m \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:13<00:08,  4.48it/s])\u001b[0m \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:13<00:08,  4.61it/s]2)\u001b[0m \n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:13<00:08,  4.50it/s])\u001b[0m \n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:13<00:08,  4.61it/s]2)\u001b[0m \n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:13<00:08,  4.59it/s]2)\u001b[0m \n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:13<00:08,  4.51it/s])\u001b[0m \n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:13<00:08,  4.59it/s]2)\u001b[0m \n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:14<00:08,  4.51it/s])\u001b[0m \n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:14<00:07,  4.59it/s]2)\u001b[0m \n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:14<00:07,  4.51it/s])\u001b[0m \n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:14<00:07,  4.53it/s])\u001b[0m \n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:14<00:07,  4.60it/s]2)\u001b[0m \n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:14<00:07,  4.53it/s])\u001b[0m \n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:14<00:07,  4.60it/s]2)\u001b[0m \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:14<00:07,  4.60it/s]2)\u001b[0m \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:14<00:07,  4.52it/s])\u001b[0m \n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:14<00:06,  4.59it/s]2)\u001b[0m \n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:15<00:07,  4.52it/s])\u001b[0m \n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:15<00:06,  4.59it/s]2)\u001b[0m \n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:15<00:06,  4.49it/s])\u001b[0m \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:15<00:06,  4.60it/s]2)\u001b[0m \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:15<00:06,  4.49it/s])\u001b[0m \n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:15<00:06,  4.60it/s]2)\u001b[0m \n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:15<00:06,  4.52it/s])\u001b[0m \n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:16<00:06,  4.53it/s])\u001b[0m \n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:15<00:06,  4.60it/s]2)\u001b[0m \n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:16<00:05,  4.60it/s]2)\u001b[0m \n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:16<00:05,  4.52it/s])\u001b[0m \n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:16<00:05,  4.60it/s]2)\u001b[0m \n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:16<00:05,  4.52it/s])\u001b[0m \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:16<00:05,  4.59it/s]2)\u001b[0m \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:16<00:05,  4.52it/s])\u001b[0m \n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:16<00:05,  4.59it/s]2)\u001b[0m \n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:16<00:05,  4.51it/s])\u001b[0m \n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:16<00:05,  4.59it/s]2)\u001b[0m \n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:17<00:05,  4.52it/s])\u001b[0m \n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:17<00:04,  4.60it/s]2)\u001b[0m \n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:17<00:04,  4.50it/s])\u001b[0m \n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:17<00:04,  4.60it/s]2)\u001b[0m \n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:17<00:04,  4.49it/s])\u001b[0m \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:17<00:04,  4.59it/s]2)\u001b[0m \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:17<00:04,  4.51it/s])\u001b[0m \n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:17<00:04,  4.60it/s]2)\u001b[0m \n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:18<00:04,  4.53it/s])\u001b[0m \n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:17<00:03,  4.60it/s]2)\u001b[0m \n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:18<00:03,  4.52it/s])\u001b[0m \n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:18<00:03,  4.60it/s]2)\u001b[0m \n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:18<00:03,  4.51it/s])\u001b[0m \n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:18<00:03,  4.60it/s]2)\u001b[0m \n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:18<00:03,  4.50it/s])\u001b[0m \n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:18<00:03,  4.60it/s]2)\u001b[0m \n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:18<00:03,  4.48it/s])\u001b[0m \n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:18<00:03,  4.59it/s]2)\u001b[0m \n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:19<00:03,  4.49it/s])\u001b[0m \n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:19<00:02,  4.49it/s])\u001b[0m \n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:19<00:02,  4.48it/s])\u001b[0m \n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:19<00:02,  4.51it/s])\u001b[0m \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:20<00:02,  4.52it/s])\u001b[0m \n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:19<00:01,  4.60it/s]2)\u001b[0m \n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:20<00:01,  4.52it/s])\u001b[0m \n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:20<00:01,  4.51it/s])\u001b[0m \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:20<00:01,  4.50it/s])\u001b[0m \n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:20<00:01,  4.48it/s])\u001b[0m \n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:21<00:01,  4.49it/s])\u001b[0m \n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:21<00:00,  4.60it/s]2)\u001b[0m \n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:21<00:00,  4.51it/s])\u001b[0m \n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:21<00:00,  4.60it/s]2)\u001b[0m \n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:21<00:00,  4.48it/s])\u001b[0m \n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:21<00:00,  4.59it/s]2)\u001b[0m \n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:21<00:00,  4.49it/s])\u001b[0m \n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:21<00:00,  4.58it/s]2)\u001b[0m \n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:22<00:00,  4.52it/s])\u001b[0m \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:21<00:00,  4.59it/s])\u001b[0m \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:22<00:00,  4.53it/s]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  4.13it/s])\u001b[0m \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  4.05it/s]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#\"\"\" Finetuning a ðŸ¤— Transformers model for sequence classification.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5001')\n",
    "    \n",
    "import datasets\n",
    "import ray\n",
    "import transformers\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, load_metric\n",
    "from ray.train import Trainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Finetune a transformers model on a text classification task\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Ignore this!\",\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--train_file\",\n",
    "        type=str,\n",
    "        default=\"data/train/part-algo-1-womens_clothing_ecommerce_reviews.csv\",\n",
    "        help=\"A csv or a json file containing the training data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_file\",\n",
    "        type=str,\n",
    "        default=\"data/validation/part-algo-1-womens_clothing_ecommerce_reviews.csv\",\n",
    "        help=\"A csv or a json file containing the validation data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help=(\n",
    "            \"The maximum total input sequence length after tokenization. \"\n",
    "            \"Sequences longer than this will be truncated, sequences shorter \"\n",
    "            \"will be padded if `--pad_to_max_lengh` is passed.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pad_to_max_length\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic \"\n",
    "        \"padding is used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        help=\"Path to pretrained model or model identifier from \"\n",
    "        \"huggingface.co/models.\",\n",
    "        default=\"roberta-base\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_slow_tokenizer\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, will use a slow tokenizer (not backed by the ðŸ¤— \"\n",
    "        \"Tokenizers library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_train_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_eval_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Total number of training steps to perform. If provided, \"\n",
    "        \"overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a \"\n",
    "        \"backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler_type\",\n",
    "        type=SchedulerType,\n",
    "        default=\"linear\",\n",
    "        help=\"The scheduler type to use.\",\n",
    "        choices=[\n",
    "            \"linear\",\n",
    "            \"cosine\",\n",
    "            \"cosine_with_restarts\",\n",
    "            \"polynomial\",\n",
    "            \"constant\",\n",
    "            \"constant_with_warmup\",\n",
    "        ],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmup_steps\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Number of steps for the warmup in the lr scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", type=str, default=None, help=\"Where to store the final model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=None, help=\"A seed for reproducible training.\"\n",
    "    )\n",
    "\n",
    "    # Ray arguments.\n",
    "    parser.add_argument(\n",
    "        \"--start_local\", action=\"store_true\", help=\"Starts Ray on local machine.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--address\", \n",
    "        type=str, \n",
    "        default=\"127.0.0.1:6379\", \n",
    "        help=\"Ray address to connect to.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\", \n",
    "        type=int, \n",
    "        default=2, \n",
    "        help=\"Number of workers to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_gpu\", action=\"store_true\", help=\"If training should be done on GPUs.\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Sanity checks\n",
    "    if (\n",
    "#        args.task_name is None\n",
    "        args.train_file is None\n",
    "        and args.validation_file is None\n",
    "    ):\n",
    "        raise ValueError(\"Need a training/validation file.\")\n",
    "    else:\n",
    "        if args.train_file is not None:\n",
    "            extension = args.train_file.split(\".\")[-1]\n",
    "            assert extension in [\n",
    "                \"csv\",\n",
    "                \"json\",\n",
    "            ], \"`train_file` should be a csv or a json file.\"\n",
    "        if args.validation_file is not None:\n",
    "            extension = args.validation_file.split(\".\")[-1]\n",
    "            assert extension in [\n",
    "                \"csv\",\n",
    "                \"json\",\n",
    "            ], \"`validation_file` should be a csv or a json file.\"\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_func(config: Dict[str, Any]):\n",
    "    args = config[\"args\"]\n",
    "    # Initialize the accelerator. We will let the accelerator handle device\n",
    "    # placement for us in this example.\n",
    "    accelerator = Accelerator()\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.ERROR,\n",
    "    )\n",
    "    logger.info(accelerator.state)\n",
    "\n",
    "    # Setup logging, we only want one process per machine to log things on\n",
    "    # the screen. accelerator.is_local_main_process is only True for one\n",
    "    # process per machine.\n",
    "    logger.setLevel(\n",
    "        logging.ERROR if accelerator.is_local_main_process else logging.ERROR\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # If passed along, set the training seed now.\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON training and\n",
    "    # evaluation files (see below) or specify a GLUE benchmark task (the\n",
    "    # dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "    # For CSV/JSON files, this script will use as labels the column called\n",
    "    # 'label' and as pair of sentences the sentences in columns called\n",
    "    # 'sentence1' and 'sentence2' if such column exists or the first two\n",
    "    # columns not named label if at least two columns are provided.\n",
    "\n",
    "    # If the CSVs/JSONs contain only one non-label column, the script does\n",
    "    # single sentence classification on this single column. You can easily\n",
    "    # tweak this behavior (see below)\n",
    "\n",
    "    # In distributed training, the load_dataset function guarantee that only\n",
    "    # one local process can concurrently download the dataset.\n",
    "#    if args.task_name is not None:\n",
    "#        # Downloading and loading a dataset from the hub.\n",
    "#        raw_datasets = load_dataset(\"glue\", args.task_name)\n",
    "#    else:\n",
    "        # Loading the dataset from local csv or json file.\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = (\n",
    "        args.train_file if args.train_file is not None else args.valid_file\n",
    "    ).split(\".\")[-1]\n",
    "\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "    label_list = raw_datasets[\"train\"].unique(\"sentiment\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # In distributed training, the .from_pretrained methods guarantee that\n",
    "    # only one local process can concurrently download model & vocab.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.model_name_or_path, num_labels=num_labels, \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, use_fast=not args.use_slow_tokenizer\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    sentence1_key, sentence2_key = \"review_body\", None\n",
    "\n",
    "    # Some models have set the order of the labels to use,\n",
    "    # so let's make sure we do use it.\n",
    "    label_to_id = None\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "    if label_to_id is not None:\n",
    "        model.config.label2id = label_to_id\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "    padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        texts = (\n",
    "            (examples[sentence1_key],)\n",
    "            if sentence2_key is None\n",
    "            else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(\n",
    "            *texts, padding=padding, max_length=args.max_length, truncation=True\n",
    "        )\n",
    "\n",
    "        if \"sentiment\" in examples:\n",
    "            if label_to_id is not None:\n",
    "                # Map labels to IDs (not necessary for GLUE tasks)\n",
    "                result[\"labels\"] = [\n",
    "                    label_to_id[l] for l in examples[\"sentiment\"]  # noqa:E741\n",
    "                ]\n",
    "            else:\n",
    "                # In all cases, rename the column to labels because the model\n",
    "                # will expect that.\n",
    "                result[\"labels\"] = examples[\"sentiment\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    # DataLoaders creation:\n",
    "    if args.pad_to_max_length:\n",
    "        # If padding was already done ot max length, we use the default data\n",
    "        # collator that will just convert everything to tensors.\n",
    "        data_collator = default_data_collator\n",
    "    else:\n",
    "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for\n",
    "        # us (by padding to the maximum length of the samples passed). When\n",
    "        # using mixed precision, we add `pad_to_multiple_of=8` to pad all\n",
    "        # tensors to multiple of 8s, which will enable the use of Tensor\n",
    "        # Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "        )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=args.per_device_train_batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=args.per_device_eval_batch_size,\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    # Split weights in two groups, one with weight decay and the other not.\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "#    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "#        model, optimizer, train_dataloader\n",
    "#    )\n",
    "    # Note -> the training dataloader needs to be prepared before we grab\n",
    "    # his length below (cause its length will be shorter in multiprocess)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        args.num_train_epochs = math.ceil(\n",
    "            args.max_train_steps / num_update_steps_per_epoch\n",
    "        )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "\n",
    "    # Get the metric function\n",
    "#    if args.task_name is not None:\n",
    "#        metric = load_metric(\"glue\", args.task_name)\n",
    "#    else:\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        args.per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num epochs = {args.num_train_epochs}\")\n",
    "    logger.info(\n",
    "        f\"  Instantaneous batch size per device =\"\n",
    "        f\" {args.per_device_train_batch_size}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) \"\n",
    "        f\"= {total_batch_size}\"\n",
    "    )\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(\n",
    "        range(args.max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    completed_steps = 0\n",
    "\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if (\n",
    "                step % args.gradient_accumulation_steps == 0\n",
    "                or step == len(train_dataloader) - 1\n",
    "            ):\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            predictions = (\n",
    "                outputs.logits.argmax(dim=-1)\n",
    "#                if not is_regression\n",
    "#                else outputs.logits.squeeze()\n",
    "            )\n",
    "            metric.add_batch(\n",
    "                predictions=accelerator.gather(predictions),\n",
    "                references=accelerator.gather(batch[\"labels\"]),\n",
    "            )\n",
    "\n",
    "        mlflow.log_param(\"batch_size\", args.per_device_train_batch_size)\n",
    "        mlflow.log_param(\"learning_rate\", args.learning_rate)\n",
    "        mlflow.log_param(\"weight_decay\", args.weight_decay)\n",
    "        mlflow.log_param(\"max_length\", args.max_length)\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        mlflow.log_metric(\"accuracy\", eval_metric['accuracy'])\n",
    "        \n",
    "        logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    config = {\"args\": args}\n",
    "    args.use_gpu = True\n",
    "\n",
    "    if args.start_local or args.address or args.num_workers > 1 or args.use_gpu:\n",
    "        if args.start_local:\n",
    "            # Start a local Ray runtime.\n",
    "            ray.init(num_cpus=args.num_workers)\n",
    "        else:\n",
    "            # Connect to a Ray cluster for distributed training.\n",
    "            ray.init(address=args.address)\n",
    "        trainer = Trainer(\"torch\", num_workers=args.num_workers, use_gpu=args.use_gpu,\n",
    "                          resources_per_worker={'CPU': 4, 'GPU': 1})\n",
    "        trainer.start()\n",
    "        trainer.run(train_func, config)\n",
    "    else:\n",
    "        # Run training locally.\n",
    "        train_func(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b409397-9ae1-43f9-a238-eab3c6c2141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
