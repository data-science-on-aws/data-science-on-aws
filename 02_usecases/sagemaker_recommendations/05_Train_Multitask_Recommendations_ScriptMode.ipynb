{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TensorFlow Recommender System (TFRS) using Amazon SageMaker. \n",
    "\n",
    "We will use this model to recommend movies for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker==2.9.2\n",
    "!pip install -q sagemaker-experiments==0.1.24\n",
    "!pip install -q tensorflow==2.3.0\n",
    "!pip install -q tensorflow-recommenders==0.2.0\n",
    "!pip install -q tensorflow-datasets==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\n",
      "drwx------ 12 root nogroup 6144 Nov  2 21:14 .\n",
      "drwxr-xr-x  1 root root      39 Nov  2 21:24 ..\n",
      "-rw-------  1 root root    2230 Nov  2 21:14 .bash_history\n",
      "drwxr-xr-x  7 root root    6144 Nov  2 20:59 .cache\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 15:44 .config\n",
      "-rw-r--r--  1 root root      54 Nov  2 15:39 .gitconfig\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:31 .ipynb_checkpoints\n",
      "drwxr-xr-x  5 root root    6144 Oct 10 23:13 .ipython\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:24 .jupyter\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:59 .keras\n",
      "drwxr-xr-x  3 root root    6144 Oct 10 22:41 .local\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:33 .ssh\n",
      "-rw-r--r--  1 root root     111 Oct 10 22:41 .yarnrc\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:12 exported_models\n",
      "drwxr-xr-x 19 root root    6144 Nov  1 19:35 workshop\n"
     ]
    }
   ],
   "source": [
    "!ls -al ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Input Data S3 URI and `Distribution Strategy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "input_train_data_s3_uri ='s3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/'\n",
    "\n",
    "s3_input_train_data = TrainingInput(s3_data=input_train_data_s3_uri,\n",
    "                                    distribution='ShardedByS3Key')\n",
    "print(s3_input_train_data.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance\n",
    "\n",
    "These sample log lines...\n",
    "```\n",
    "499/500 [=====>..] - ETA: 3s - root_mean_squared_error: 1.1198 - factorized_top_k/top_10_categorical_accuracy: 0.481 - factorized_top_k/top_50_categorical_accuracy: 0.607 - factorized_top_k/top_100_categorical_accuracy: 0.885\n",
    "```\n",
    "...will produce the following metrics in CloudWatch:\n",
    "\n",
    "`root_mean_squared_error` = 1.1198\n",
    "\n",
    "`factorized_top_k/top_10_categorical_accuracy` = 0.481\n",
    "\n",
    "`factorized_top_k/top_50_categorical_accuracy` = 0.607\n",
    "\n",
    "`factorized_top_k/top_100_categorical_accuracy` = 0.885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [    \n",
    "     {'Name': 'root_mean_squared_error', 'Regex': 'root_mean_squared_error: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_10_categorical_accuracy', 'Regex': 'factorized_top_k/top_10_categorical_accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_50_categorical_accuracy', 'Regex': 'factorized_top_k/top_50_categorical_accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'top_100_categorical_accuracy', 'Regex': 'factorized_top_k/top_100_categorical_accuracy: ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters for Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000\n",
    "learning_rate=0.5\n",
    "dataset_variant='100k' # movielens 100k, 1m, 20m, 25m, etc\n",
    "embedding_dimension=256 # dimension (k) of our user and item embeddings\n",
    "enable_tensorboard=True\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Our TensorFlow Script to Run on SageMaker\n",
    "Prepare our TensorFlow model to run on the managed SageMaker service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\n",
      "drwx------ 12 root nogroup 6144 Nov  2 21:14 .\n",
      "drwxr-xr-x  1 root root      39 Nov  2 21:24 ..\n",
      "-rw-------  1 root root    2230 Nov  2 21:14 .bash_history\n",
      "drwxr-xr-x  7 root root    6144 Nov  2 20:59 .cache\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 15:44 .config\n",
      "-rw-r--r--  1 root root      54 Nov  2 15:39 .gitconfig\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:31 .ipynb_checkpoints\n",
      "drwxr-xr-x  5 root root    6144 Oct 10 23:13 .ipython\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:24 .jupyter\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:59 .keras\n",
      "drwxr-xr-x  3 root root    6144 Oct 10 22:41 .local\n",
      "drwxr-xr-x  2 root root    6144 Nov  1 19:33 .ssh\n",
      "-rw-r--r--  1 root root     111 Oct 10 22:41 .yarnrc\n",
      "drwxr-xr-x  3 root root    6144 Nov  2 21:12 exported_models\n",
      "drwxr-xr-x 19 root root    6144 Nov  1 19:35 workshop\n"
     ]
    }
   ],
   "source": [
    "!ls -al ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-tensorflow==2.3.0.1.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow-recommenders==0.2.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow-datasets==4.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict, Text\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow_datasets\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfds\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow_recommenders\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfrs\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMovielensModel\u001b[39;49;00m(tfrs.models.Model):\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, rating_weight: \u001b[36mfloat\u001b[39;49;00m, retrieval_weight: \u001b[36mfloat\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "    \u001b[37m# We take the loss weights in the constructor: this allows us to instantiate\u001b[39;49;00m\n",
      "    \u001b[37m# several model objects with different loss weights.\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36msuper\u001b[39;49;00m().\u001b[32m__init__\u001b[39;49;00m()\n",
      "\n",
      "    embedding_dimension = \u001b[34m32\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# User and movie models.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
      "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
      "        vocabulary=unique_movie_titles, mask_token=\u001b[34mNone\u001b[39;49;00m),\n",
      "      tf.keras.layers.Embedding(\u001b[36mlen\u001b[39;49;00m(unique_movie_titles) + \u001b[34m1\u001b[39;49;00m, embedding_dimension)\n",
      "    ])\n",
      "    \u001b[36mself\u001b[39;49;00m.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
      "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
      "        vocabulary=unique_user_ids, mask_token=\u001b[34mNone\u001b[39;49;00m),\n",
      "      tf.keras.layers.Embedding(\u001b[36mlen\u001b[39;49;00m(unique_user_ids) + \u001b[34m1\u001b[39;49;00m, embedding_dimension)\n",
      "    ])\n",
      "\n",
      "    \u001b[37m# A small model to take in user and movie embeddings and predict ratings.\u001b[39;49;00m\n",
      "    \u001b[37m# We can make this as complicated as we want as long as we output a scalar\u001b[39;49;00m\n",
      "    \u001b[37m# as our prediction.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.rating_model = tf.keras.Sequential([\n",
      "        tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        tf.keras.layers.Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        tf.keras.layers.Dense(\u001b[34m1\u001b[39;49;00m),\n",
      "    ])\n",
      "\n",
      "    \u001b[37m# The tasks.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
      "        loss=tf.keras.losses.MeanSquaredError(),\n",
      "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
      "    )\n",
      "    \u001b[36mself\u001b[39;49;00m.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
      "        metrics=tfrs.metrics.FactorizedTopK(\n",
      "            candidates=movies.batch(\u001b[34m128\u001b[39;49;00m).map(\u001b[36mself\u001b[39;49;00m.movie_model)\n",
      "        )\n",
      "    )\n",
      "\n",
      "    \u001b[37m# The loss weights.\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.rating_weight = rating_weight\n",
      "    \u001b[36mself\u001b[39;49;00m.retrieval_weight = retrieval_weight\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mcall\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
      "    \u001b[37m# We pick out the user features and pass them into the user model.\u001b[39;49;00m\n",
      "    user_embeddings = \u001b[36mself\u001b[39;49;00m.user_model(features[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[37m# And pick out the movie features and pass them into the movie model.\u001b[39;49;00m\n",
      "    movie_embeddings = \u001b[36mself\u001b[39;49;00m.movie_model(features[\u001b[33m\"\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m (\n",
      "        user_embeddings,\n",
      "        movie_embeddings,\n",
      "        \u001b[37m# We apply the multi-layered rating model to a concatentation of\u001b[39;49;00m\n",
      "        \u001b[37m# user and movie embeddings.\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.rating_model(\n",
      "            tf.concat([user_embeddings, movie_embeddings], axis=\u001b[34m1\u001b[39;49;00m)\n",
      "        ),\n",
      "    )\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_loss\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, features: Dict[Text, tf.Tensor], training=\u001b[34mFalse\u001b[39;49;00m) -> tf.Tensor:\n",
      "\n",
      "    user_embeddings, movie_embeddings, rating_predictions = \u001b[36mself\u001b[39;49;00m(features)\n",
      "\n",
      "    \u001b[37m# We compute the loss for each task.\u001b[39;49;00m\n",
      "    rating_loss = \u001b[36mself\u001b[39;49;00m.rating_task(\n",
      "        labels=features[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        predictions=rating_predictions,\n",
      "    )\n",
      "    retrieval_loss = \u001b[36mself\u001b[39;49;00m.retrieval_task(user_embeddings, movie_embeddings)\n",
      "\n",
      "    \u001b[37m# And combine them using the loss weights.\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.rating_weight * rating_loss\n",
      "            + \u001b[36mself\u001b[39;49;00m.retrieval_weight * retrieval_loss)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:    \n",
      "    env_var = os.environ \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m) \n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, \n",
      "                        default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m1\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)        \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m# This is unused\u001b[39;49;00m\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset_variant\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33m100k\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--embedding_dimension\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcommand line args:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    local_model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlocal_model_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(local_model_dir))\n",
      "    output_dir = args.output_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output_dir))    \n",
      "    hosts = args.hosts\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(hosts))    \n",
      "    current_host = args.current_host\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_host))    \n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))\n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))\n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))\n",
      "    enable_tensorboard = args.enable_tensorboard\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_tensorboard))\n",
      "    dataset_variant = args.dataset_variant\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdataset_variant \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant))\n",
      "    embedding_dimension = \u001b[36mint\u001b[39;49;00m(args.embedding_dimension)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dimension \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(embedding_dimension))    \n",
      "             \n",
      "    \u001b[37m# Load the ratings data to use for training\u001b[39;49;00m\n",
      "    ratings = tfds.load(\u001b[33m'\u001b[39;49;00m\u001b[33mmovielens/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-ratings\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant), \n",
      "                        download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        data_dir=train_data,\n",
      "                        split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRatings raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ratings)\n",
      "\n",
      "    \u001b[37m# Transform the ratings data specific to our training task\u001b[39;49;00m\n",
      "    ratings = ratings.map(\u001b[34mlambda\u001b[39;49;00m x: {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: x[\u001b[33m'\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: x[\u001b[33m'\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    })\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRatings transformed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, ratings)    \n",
      "\n",
      "    \u001b[37m# Load the movies data to use for training\u001b[39;49;00m\n",
      "    movies = tfds.load(\u001b[33m'\u001b[39;49;00m\u001b[33mmovielens/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-movies\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset_variant),\n",
      "                       download=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                       data_dir=train_data,\n",
      "                       split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mMovies raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, movies)\n",
      "    \n",
      "    \u001b[37m# Transform the movies data specific to our training task\u001b[39;49;00m\n",
      "    movies = movies.map(\u001b[34mlambda\u001b[39;49;00m x: x[\u001b[33m'\u001b[39;49;00m\u001b[33mmovie_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mMovies transformed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, movies)\n",
      "\n",
      "    \u001b[37m# Randomly shuffle data and split between train and test.\u001b[39;49;00m\n",
      "    tf.random.set_seed(\u001b[34m42\u001b[39;49;00m)\n",
      "    shuffled = ratings.shuffle(\u001b[34m100_000\u001b[39;49;00m, seed=\u001b[34m42\u001b[39;49;00m, reshuffle_each_iteration=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    train = shuffled.take(\u001b[34m80_000\u001b[39;49;00m)\n",
      "    test = shuffled.skip(\u001b[34m80_000\u001b[39;49;00m).take(\u001b[34m20_000\u001b[39;49;00m)\n",
      "\n",
      "    cached_train = train.shuffle(\u001b[34m100_000\u001b[39;49;00m).batch(\u001b[34m8192\u001b[39;49;00m).cache()\n",
      "    cached_test = test.batch(\u001b[34m4096\u001b[39;49;00m).cache()\n",
      "    \n",
      "    movie_titles = movies.batch(\u001b[34m1_000\u001b[39;49;00m)\n",
      "    user_ids = ratings.batch(\u001b[34m1_000_000\u001b[39;49;00m).map(\u001b[34mlambda\u001b[39;49;00m x: x[\u001b[33m\"\u001b[39;49;00m\u001b[33muser_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    unique_movie_titles = np.unique(np.concatenate(\u001b[36mlist\u001b[39;49;00m(movie_titles)))\n",
      "    unique_user_ids = np.unique(np.concatenate(\u001b[36mlist\u001b[39;49;00m(user_ids)))\n",
      "\n",
      "    \u001b[37m# Define the optimizer and hyper-parameters\u001b[39;49;00m\n",
      "    optimizer = tf.keras.optimizers.Adagrad(learning_rate)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizer:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\n",
      "\n",
      "    \u001b[37m# Setup the callbacks to use during training\u001b[39;49;00m\n",
      "    callbacks = []\n",
      "\n",
      "    \u001b[37m# Setup the Tensorboard callback if Tensorboard is enabled\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m enable_tensorboard: \n",
      "        \u001b[37m# Tensorboard Logs \u001b[39;49;00m\n",
      "        tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAdding Tensorboard callback \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorboard_callback))\n",
      "        callbacks.append(tensorboard_callback)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCallbacks: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(callbacks))\n",
      "\n",
      "    \u001b[37m# Create and compile a custom Keras model specialized for rating\u001b[39;49;00m\n",
      "    model = MovielensModel(rating_weight=\u001b[34m1.0\u001b[39;49;00m, retrieval_weight=\u001b[34m0.0\u001b[39;49;00m)\n",
      "    model.compile(optimizer=optimizer)\n",
      "\n",
      "    \u001b[37m# Train the model\u001b[39;49;00m\n",
      "    model.fit(cached_train, epochs=epochs, callbacks=callbacks)\n",
      "    metrics = model.evaluate(cached_test, return_dict=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mretrieval-top-100-accuracy: \u001b[39;49;00m\u001b[33m{metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mranking-rmse: \u001b[39;49;00m\u001b[33m{metrics['root_mean_squared_error']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Create and compile a custom Keras model specialized for retrieval\u001b[39;49;00m\n",
      "    model = MovielensModel(rating_weight=\u001b[34m0.0\u001b[39;49;00m, retrieval_weight=\u001b[34m1.0\u001b[39;49;00m)   \n",
      "    model.compile(optimizer=optimizer)\n",
      "\n",
      "    model.fit(cached_train, epochs=epochs, callbacks=callbacks)\n",
      "    metrics = model.evaluate(cached_test, return_dict=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mretrieval-top-100-accuracy: \u001b[39;49;00m\u001b[33m{metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mranking-rmse: \u001b[39;49;00m\u001b[33m{metrics['root_mean_squared_error']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Create and compile a custom Keras model for both rating and retrieval\u001b[39;49;00m\n",
      "    model = MovielensModel(rating_weight=\u001b[34m1.0\u001b[39;49;00m, retrieval_weight=\u001b[34m1.0\u001b[39;49;00m)\n",
      "    model.compile(optimizer=optimizer)\n",
      "    \n",
      "    model.fit(cached_train, epochs=epochs, callbacks=callbacks)\n",
      "    metrics = model.evaluate(cached_test, return_dict=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mRetrieval top-100 accuracy: \u001b[39;49;00m\u001b[33m{metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mRanking RMSE: \u001b[39;49;00m\u001b[33m{metrics['root_mean_squared_error']:.3f}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \n",
      "    \n",
      "    \u001b[37m# Make some sample predictions to test our model\u001b[39;49;00m\n",
      "    \u001b[37m# Note:  This is required to save and server our model with TensorFlow Serving\u001b[39;49;00m\n",
      "    \u001b[37m#        See https://github.com/tensorflow/tensorflow/issues/31057 for more  details.\u001b[39;49;00m\n",
      "\u001b[37m# Create a model that takes in raw query features, and returns the predicted movie titles\u001b[39;49;00m\n",
      "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
      "    index.index(movies.batch(\u001b[34m100\u001b[39;49;00m).map(model.movie_model), movies)\n",
      "\n",
      "    k = \u001b[34m5\u001b[39;49;00m\n",
      "    user_id = \u001b[33m\"\u001b[39;49;00m\u001b[33m42\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "    _, titles = index(np.array([user_id]))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTop \u001b[39;49;00m\u001b[33m{k}\u001b[39;49;00m\u001b[33m recommendations for user \u001b[39;49;00m\u001b[33m{user_id}\u001b[39;49;00m\u001b[33m: \u001b[39;49;00m\u001b[33m{titles[0, :k]}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Print a summary of our recommender model\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrained index \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(index))\n",
      "    \u001b[36mprint\u001b[39;49;00m(index.summary())\n",
      "\n",
      "    \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir,\n",
      "                                               \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))\n",
      "    index.save(tensorflow_saved_model_path, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\n",
      "    \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\n",
      "    \u001b[37m#         This directory must be named `code/`\u001b[39;49;00m\n",
      "    inference_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying inference.py to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "    os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)               \n",
      "    os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "    \u001b[36mprint\u001b[39;49;00m(glob(inference_path))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize /root/workshop/02_usecases/sagemaker_recommendations/src/train_multitask.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='train_multitask.py',\n",
    "                       source_dir='/root/workshop/02_usecases/sagemaker_recommendations/src',\n",
    "                       role=role,\n",
    "                       instance_count=train_instance_count,\n",
    "                       instance_type=train_instance_type,\n",
    "                       py_version='py37',\n",
    "                       framework_version='2.3.0',\n",
    "                       hyperparameters={\n",
    "                           'epochs': epochs,\n",
    "                           'learning_rate': learning_rate,\n",
    "                           'dataset_variant': dataset_variant,\n",
    "                           'embedding_dimension': embedding_dimension,                           \n",
    "                           'enable_tensorboard': enable_tensorboard\n",
    "                       },\n",
    "                       metric_definitions=metrics_definitions,\n",
    "                       debugger_hook_config=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: MovieLens-Recommender-1604352479\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "recommender_experiment = Experiment.create(\n",
    "                         experiment_name='MovieLens-Recommender-{}'.format(timestamp),\n",
    "                         description='MovieLens Recommender', \n",
    "                         sagemaker_boto_client=sm)\n",
    "\n",
    "recommender_experiment_name = recommender_experiment.experiment_name\n",
    "print('Experiment name: {}'.format(recommender_experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1604352479-1000-100k-256\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "trial_name = 'trial-{}-{}-{}-{}'.format(timestamp, epochs, dataset_variant, embedding_dimension)\n",
    "\n",
    "trial = Trial.create(trial_name=trial_name,\n",
    "                     experiment_name=recommender_experiment_name,\n",
    "                     sagemaker_boto_client=sm)\n",
    "\n",
    "trial_name = trial.trial_name\n",
    "print('Trial name: {}'.format(trial_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_experiment_config = {\n",
    "    'ExperimentName': recommender_experiment_name,\n",
    "    'TrialName': trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tensorflow-training-2020-11-02-21-27-59-737\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "              inputs={\n",
    "                  'train': s3_input_train_data, \n",
    "              },              \n",
    "              experiment_config=recommender_experiment_config,                   \n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job Name:  tensorflow-training-2020-11-02-21-27-59-737\n"
     ]
    }
   ],
   "source": [
    "recommender_training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(recommender_training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/tensorflow-training-2020-11-02-21-27-59-737\">Training Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a></b>'.format(region, recommender_training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2020-11-02-21-27-59-737;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'.format(region, recommender_training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, recommender_training_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for Training Job to Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-02 21:28:01 Starting - Starting the training job\n",
      "2020-11-02 21:28:03 Starting - Launching requested ML instances.................\n",
      "2020-11-02 21:29:33 Starting - Preparing the instances for training.........\n",
      "2020-11-02 21:30:23 Downloading - Downloading input data...............\n",
      "2020-11-02 21:31:46 Training - Downloading the training image.....\n",
      "2020-11-02 21:32:14 Training - Training image download completed. Training in progress..........\n",
      "2020-11-02 21:33:05 Uploading - Uploading generated training model\n",
      "2020-11-02 21:33:12 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job tensorflow-training-2020-11-02-21-27-59-737: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/local/bin/python3.7 train_multitask.py --dataset_variant 100k --embedding_dimension 256 --enable_tensorboard True --epochs 1000 --learning_rate 0.5 --model_dir s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/model\"\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   2735\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mlast_desc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_train_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m         )\n\u001b[0;32m-> 2737\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2863\u001b[0m                 ),\n\u001b[1;32m   2864\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2865\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2866\u001b[0m             )\n\u001b[1;32m   2867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job tensorflow-training-2020-11-02-21-27-59-737: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/local/bin/python3.7 train_multitask.py --dataset_variant 100k --embedding_dimension 256 --enable_tensorboard True --epochs 1000 --learning_rate 0.5 --model_dir s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/model\"\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the Trained Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$recommender_training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard/\n",
      "tensorboard/train/\n",
      "tensorboard/train/events.out.tfevents.1604352782.ip-10-0-235-59.ec2.internal.33.543.v2\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./model/\n",
    "!tar -xvzf ./model.tar.gz -C ./model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 21:33:17.524011: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2020-11-02 21:33:17.524168: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/saved_model_cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1185, in main\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 715, in show\n",
      "    _show_all(args.dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 296, in _show_all\n",
      "    tag_sets = saved_model_utils.get_saved_model_tag_sets(saved_model_dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_utils.py\", line 89, in get_saved_model_tag_sets\n",
      "    saved_model = read_saved_model(saved_model_dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_utils.py\", line 55, in read_saved_model\n",
      "    raise IOError(\"SavedModel file does not exist at: %s\" % saved_model_dir)\n",
      "OSError: SavedModel file does not exist at: ./model/tensorflow/saved_model/0/\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir ./model/tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 21:33:23.992231: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2020-11-02 21:33:23.992519: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/saved_model_cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1185, in main\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 748, in run\n",
      "    init_tpu=args.init_tpu, tf_debug=args.tf_debug)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 408, in run_saved_model_with_feed_dict\n",
      "    tag_set)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_utils.py\", line 115, in get_meta_graph_def\n",
      "    saved_model = read_saved_model(saved_model_dir)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_utils.py\", line 55, in read_saved_model\n",
      "    raise IOError(\"SavedModel file does not exist at: %s\" % saved_model_dir)\n",
      "OSError: SavedModel file does not exist at: ./model/tensorflow/saved_model/0\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli run --input_exprs 'input_1=np.array([\"$user_id\"])' --tag_set serve --signature_def serving_default --dir ./model/tensorflow/saved_model/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Experiment Tracking Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=recommender_experiment_name,\n",
    "    metric_names=[\n",
    "        'top_10_categorical_accuracy',\n",
    "        'top_50_categorical_accuracy',\n",
    "        'top_100_categorical_accuracy'\n",
    "    ],\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "\n",
    "lineage_df = lineage_table.dataframe()\n",
    "lineage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TrialComponentName', 'DisplayName', 'SourceArn', 'SageMaker.ImageUri',\n",
       "       'SageMaker.InstanceCount', 'SageMaker.InstanceType',\n",
       "       'SageMaker.VolumeSizeInGB', 'dataset_variant', 'embedding_dimension',\n",
       "       'enable_tensorboard', 'epochs', 'learning_rate', 'model_dir',\n",
       "       'sagemaker_container_log_level', 'sagemaker_job_name',\n",
       "       'sagemaker_program', 'sagemaker_region', 'sagemaker_submit_directory',\n",
       "       'train - MediaType', 'train - Value',\n",
       "       'SageMaker.ModelArtifact - MediaType',\n",
       "       'SageMaker.ModelArtifact - Value', 'Trials', 'Experiments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineage_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>dataset_variant</th>\n",
       "      <th>embedding_dimension</th>\n",
       "      <th>enable_tensorboard</th>\n",
       "      <th>...</th>\n",
       "      <th>sagemaker_job_name</th>\n",
       "      <th>sagemaker_program</th>\n",
       "      <th>sagemaker_region</th>\n",
       "      <th>sagemaker_submit_directory</th>\n",
       "      <th>train - MediaType</th>\n",
       "      <th>train - Value</th>\n",
       "      <th>SageMaker.ModelArtifact - MediaType</th>\n",
       "      <th>SageMaker.ModelArtifact - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow-training-2020-11-02-21-27-59-737-aw...</td>\n",
       "      <td>train</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:835319576252:train...</td>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.p3.2xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>\"100k\"</td>\n",
       "      <td>256.0</td>\n",
       "      <td>true</td>\n",
       "      <td>...</td>\n",
       "      <td>\"tensorflow-training-2020-11-02-21-27-59-737\"</td>\n",
       "      <td>\"train_multitask.py\"</td>\n",
       "      <td>\"us-east-1\"</td>\n",
       "      <td>\"s3://sagemaker-us-east-1-835319576252/tensorf...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-835319576252/tensorfl...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-835319576252/tensorfl...</td>\n",
       "      <td>[trial-1604352479-1000-100k-256]</td>\n",
       "      <td>[MovieLens-Recommender-1604352479]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0  tensorflow-training-2020-11-02-21-27-59-737-aw...       train   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:835319576252:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0  763104351884.dkr.ecr.us-east-1.amazonaws.com/t...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB dataset_variant  \\\n",
       "0          ml.p3.2xlarge                      30.0          \"100k\"   \n",
       "\n",
       "   embedding_dimension enable_tensorboard  ...  \\\n",
       "0                256.0               true  ...   \n",
       "\n",
       "                              sagemaker_job_name     sagemaker_program  \\\n",
       "0  \"tensorflow-training-2020-11-02-21-27-59-737\"  \"train_multitask.py\"   \n",
       "\n",
       "  sagemaker_region                         sagemaker_submit_directory  \\\n",
       "0      \"us-east-1\"  \"s3://sagemaker-us-east-1-835319576252/tensorf...   \n",
       "\n",
       "  train - MediaType                                      train - Value  \\\n",
       "0              None  s3://sagemaker-us-east-1-835319576252/tensorfl...   \n",
       "\n",
       "  SageMaker.ModelArtifact - MediaType  \\\n",
       "0                                None   \n",
       "\n",
       "                     SageMaker.ModelArtifact - Value  \\\n",
       "0  s3://sagemaker-us-east-1-835319576252/tensorfl...   \n",
       "\n",
       "                             Trials                         Experiments  \n",
       "0  [trial-1604352479-1000-100k-256]  [MovieLens-Recommender-1604352479]  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrialComponentName': 'tensorflow-training-2020-11-02-21-27-59-737-aws-training-job',\n",
       " 'TrialComponentArn': 'arn:aws:sagemaker:us-east-1:835319576252:experiment-trial-component/tensorflow-training-2020-11-02-21-27-59-737-aws-training-job',\n",
       " 'DisplayName': 'train',\n",
       " 'Source': {'SourceArn': 'arn:aws:sagemaker:us-east-1:835319576252:training-job/tensorflow-training-2020-11-02-21-27-59-737',\n",
       "  'SourceType': 'SageMakerTrainingJob'},\n",
       " 'Status': {'PrimaryStatus': 'Failed',\n",
       "  'Message': 'Status: Failed, secondary status: Failed, failure reason: AlgorithmError: ExecuteUserScriptError:\\nCommand \"/usr/local/bin/python3.7 train_multitask.py --dataset_variant 100k --embedding_dimension 256 --enable_tensorboard True --epochs 1000 --learning_rate 0.5 --model_dir s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/model\"\\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\\nYou should consider upgrading via the \\'/usr/local/bin/python3.7 -m pip install --upgrade pip\\' command.\\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\\nYou should consider upgrading via the \\'/usr/local/bin/python3.7 -m pip install --upgrade pip\\' command.\\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\\nYou should consider upgrading via the \\'/usr/local/bin/python3.7 -m pip install --upgrade pip\\' command.\\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\\nYou should consider upgrading via the \\'/usr/local/bin/python3.7 -m pip install --upgrade.'},\n",
       " 'StartTime': datetime.datetime(2020, 11, 2, 21, 30, 23, tzinfo=tzlocal()),\n",
       " 'EndTime': datetime.datetime(2020, 11, 2, 21, 33, 12, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2020, 11, 2, 21, 28, 2, 25000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:835319576252:user-profile/d-dsxoghy6ztwy/default-1602368497083',\n",
       "  'UserProfileName': 'default-1602368497083',\n",
       "  'DomainId': 'd-dsxoghy6ztwy'},\n",
       " 'LastModifiedTime': datetime.datetime(2020, 11, 2, 21, 33, 13, 168000, tzinfo=tzlocal()),\n",
       " 'LastModifiedBy': {},\n",
       " 'Parameters': {'SageMaker.ImageUri': {'StringValue': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.0-gpu-py37'},\n",
       "  'SageMaker.InstanceCount': {'NumberValue': 1.0},\n",
       "  'SageMaker.InstanceType': {'StringValue': 'ml.p3.2xlarge'},\n",
       "  'SageMaker.VolumeSizeInGB': {'NumberValue': 30.0},\n",
       "  'dataset_variant': {'StringValue': '\"100k\"'},\n",
       "  'embedding_dimension': {'StringValue': '256', 'NumberValue': 256.0},\n",
       "  'enable_tensorboard': {'StringValue': 'true'},\n",
       "  'epochs': {'StringValue': '1000', 'NumberValue': 1000.0},\n",
       "  'learning_rate': {'StringValue': '0.5', 'NumberValue': 0.5},\n",
       "  'model_dir': {'StringValue': '\"s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/model\"'},\n",
       "  'sagemaker_container_log_level': {'StringValue': '20', 'NumberValue': 20.0},\n",
       "  'sagemaker_job_name': {'StringValue': '\"tensorflow-training-2020-11-02-21-27-59-737\"'},\n",
       "  'sagemaker_program': {'StringValue': '\"train_multitask.py\"'},\n",
       "  'sagemaker_region': {'StringValue': '\"us-east-1\"'},\n",
       "  'sagemaker_submit_directory': {'StringValue': '\"s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/source/sourcedir.tar.gz\"'}},\n",
       " 'InputArtifacts': {'train': {'Value': 's3://sagemaker-us-east-1-835319576252/tensorflow_datasets/train/'}},\n",
       " 'OutputArtifacts': {'SageMaker.ModelArtifact': {'Value': 's3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-11-02-21-27-59-737/output/model.tar.gz'}},\n",
       " 'Metrics': [],\n",
       " 'ResponseMetadata': {'RequestId': '80326b2f-da46-4f40-a7b5-937b9defc412',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '80326b2f-da46-4f40-a7b5-937b9defc412',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3266',\n",
       "   'date': 'Mon, 02 Nov 2020 21:33:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.describe_trial_component(TrialComponentName=lineage_df.TrialComponentName[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'recommender_training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store recommender_training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
