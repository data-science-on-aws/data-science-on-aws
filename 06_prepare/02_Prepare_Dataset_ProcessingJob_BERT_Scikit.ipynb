{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon a SageMaker Processing Job and Scikit-Learn\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Scikit-Learn are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Scikit-Learn in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Setup Environment\n",
    "1. Setup Input Data\n",
    "1. Setup Output Data\n",
    "1. Build a Spark container for running the processing job\n",
    "1. Run the Processing Job using Amazon SageMaker\n",
    "1. Inspect the Processed Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv-all/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = 's3://{}/amazon-reviews-pds/tsv-all/'.format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-18 20:56:00  648641286 amazon_reviews_us_Apparel_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:00  582145299 amazon_reviews_us_Automotive_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:00  357392893 amazon_reviews_us_Baby_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:00  914070021 amazon_reviews_us_Beauty_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:00 2740337188 amazon_reviews_us_Books_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:08 2692708591 amazon_reviews_us_Books_v1_01.tsv.gz\r\n",
      "2020-08-18 20:56:13 1329539135 amazon_reviews_us_Books_v1_02.tsv.gz\r\n",
      "2020-08-18 20:56:14  442653086 amazon_reviews_us_Camera_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:20 2689739299 amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:24 1294879074 amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz\r\n",
      "2020-08-18 20:56:41  253570168 amazon_reviews_us_Digital_Music_Purchase_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:46   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:47  506979922 amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:51   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:52  698828243 amazon_reviews_us_Electronics_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:57  148982796 amazon_reviews_us_Furniture_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:58   12134676 amazon_reviews_us_Gift_Card_v1_00.tsv.gz\r\n",
      "2020-08-18 20:56:59  401337166 amazon_reviews_us_Grocery_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:01 1011180212 amazon_reviews_us_Health_Personal_Care_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:04  193168458 amazon_reviews_us_Home_Entertainment_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:06  503339178 amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:07 1081002012 amazon_reviews_us_Home_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:10  247022254 amazon_reviews_us_Jewelry_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:16  930744854 amazon_reviews_us_Kitchen_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:17  486772662 amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:20   60320191 amazon_reviews_us_Luggage_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:22   24359816 amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:23  557959415 amazon_reviews_us_Mobile_Apps_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:25   22870508 amazon_reviews_us_Mobile_Electronics_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:25 1521994296 amazon_reviews_us_Music_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:28  193389086 amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:31  512323500 amazon_reviews_us_Office_Products_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:32  448963100 amazon_reviews_us_Outdoors_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:33 1512903923 amazon_reviews_us_PC_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:34   17634794 amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:35  515815253 amazon_reviews_us_Pet_Products_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:41  642255314 amazon_reviews_us_Shoes_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:42   94010685 amazon_reviews_us_Software_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:46  872478735 amazon_reviews_us_Sports_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:49  333782939 amazon_reviews_us_Tools_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:57  838451398 amazon_reviews_us_Toys_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:58 1512355451 amazon_reviews_us_Video_DVD_v1_00.tsv.gz\r\n",
      "2020-08-18 20:57:59  475199894 amazon_reviews_us_Video_Games_v1_00.tsv.gz\r\n",
      "2020-08-18 20:58:07  138929896 amazon_reviews_us_Video_v1_00.tsv.gz\r\n",
      "2020-08-18 20:58:09  162973819 amazon_reviews_us_Watches_v1_00.tsv.gz\r\n",
      "2020-08-18 20:58:10 1704713674 amazon_reviews_us_Wireless_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Processing Job using Amazon SageMaker\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job using our custom python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Processing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.1.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(tf.__version__)\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==2.8.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "DATA_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "    \r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "    \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(text_input, max_seq_length):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    tokens = tokenizer.tokenize(text_input.text)    \r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=max_seq_length)\r\n",
      "\r\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\r\n",
      "\r\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[text_input.label]\r\n",
      "\r\n",
      "    features = InputFeatures(\r\n",
      "        input_ids=input_ids,\r\n",
      "        input_mask=input_mask,\r\n",
      "        segment_ids=segment_ids,\r\n",
      "        label_id=label_id)\r\n",
      "\r\n",
      "\u001b[37m#    print('**tokens**\\n{}\\n'.format(tokens))    \u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_features_to_tfrecord\u001b[39;49;00m(inputs,\r\n",
      "                                 output_file,\r\n",
      "                                 max_seq_length):\r\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, text_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\r\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m1000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting example \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\r\n",
      "\r\n",
      "            bert_features = convert_input(text_input, max_seq_length)\r\n",
      "        \r\n",
      "            tfrecord_features = collections.OrderedDict()\r\n",
      "            \r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\r\n",
      "\r\n",
      "            tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\r\n",
      "            \r\n",
      "            tfrecord_writer.write(tfrecord.SerializeToString())\r\n",
      "\r\n",
      "    tfrecord_writer.close()\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m128\u001b[39;49;00m,\r\n",
      "    )  \r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, \r\n",
      "                               max_seq_length, \r\n",
      "                               balance_dataset):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(file))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(balance_dataset))\r\n",
      "\r\n",
      "    filename_without_extension = Path(Path(file).stem).stem\r\n",
      "\r\n",
      "    df = pd.read_csv(file, \r\n",
      "                     delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                     quoting=csv.QUOTE_NONE,\r\n",
      "                     compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    df.isna().values.any()\r\n",
      "    df = df.dropna()\r\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:  \r\n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\r\n",
      "        five_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        four_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        three_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        two_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        one_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        minority_count = \u001b[36mmin\u001b[39;49;00m(five_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             four_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             three_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             two_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             one_star_df.shape[\u001b[34m0\u001b[39;49;00m]) \r\n",
      "\r\n",
      "        five_star_df = resample(five_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        four_star_df = resample(four_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        three_star_df = resample(three_star_df,\r\n",
      "                                 replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                 n_samples = minority_count,\r\n",
      "                                 random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        two_star_df = resample(two_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        one_star_df = resample(one_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        df_balanced = pd.concat([five_star_df, four_star_df, three_star_df, two_star_df, one_star_df])\r\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_balanced.shape))\r\n",
      "        df = df_balanced\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.train_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.validation_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.test_split_percentage))    \r\n",
      "    \r\n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(holdout_percentage))\r\n",
      "    df_train, df_holdout = train_test_split(df, \r\n",
      "                                            test_size=holdout_percentage, \r\n",
      "                                            stratify=df[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_holdout_percentage))\r\n",
      "    df_validation, df_test = train_test_split(df_holdout, \r\n",
      "                                              test_size=test_holdout_percentage,\r\n",
      "                                              stratify=df_holdout[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_train.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_validation.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_test.shape))\r\n",
      "\r\n",
      "    train_inputs = df_train.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                         label = x[LABEL_COLUMN]), axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    validation_inputs = df_validation.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                            label = x[LABEL_COLUMN]), axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    test_inputs = df_test.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                label = x[LABEL_COLUMN]), axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "\r\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\r\n",
      "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \r\n",
      "                                                       \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension), \r\n",
      "                                                       max_seq_length)\r\n",
      "\r\n",
      "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\r\n",
      "\r\n",
      "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension), max_seq_length)\r\n",
      "        \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\r\n",
      "    \r\n",
      "    train_data = \u001b[34mNone\u001b[39;49;00m\r\n",
      "    validation_data = \u001b[34mNone\u001b[39;49;00m\r\n",
      "    test_data = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \r\n",
      "                                                 max_seq_length=args.max_seq_length,\r\n",
      "                                                 balance_dataset=args.balance_dataset\r\n",
      "\r\n",
      "    )\r\n",
      "    input_files = glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\r\n",
      "\r\n",
      "    num_cpus = multiprocessing.cpu_count()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_cpus))\r\n",
      "\r\n",
      "    p = multiprocessing.Pool(num_cpus)\r\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\r\n",
      "    dirs_output = os.listdir(args.output_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\r\n",
      "    dirs_output = os.listdir(train_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\r\n",
      "    dirs_output = os.listdir(validation_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))\r\n",
      "    dirs_output = os.listdir(test_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\r\n",
      "\r\n",
      "    process(args)    \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-scikit-text-to-bert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job.  You also need to specify one `ProcessingInput` with the `source` argument of the Amazon S3 bucket and `destination` is where the script reads this data from `/opt/ml/processing/input` (inside the Docker container.)  All local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to.  For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name>/`.  You also give the `ProcessingOutput` value for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The arguments parameter in the `run()` method are command-line arguments in our `preprocess-scikit-text-to-bert.py` script.\n",
    "\n",
    "Note that we sharding the data using `ShardedByS3Key` to spread the transformations across all worker nodes in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Processing Job Hyper-Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage=0.90\n",
    "validation_split_percentage=0.05\n",
    "test_split_percentage=0.05\n",
    "max_seq_length=64\n",
    "balance_dataset=True\n",
    "processing_instance_type='ml.c5.2xlarge'\n",
    "processing_instance_count=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                             role=role,\n",
    "                             instance_type=processing_instance_type,\n",
    "                             instance_count=processing_instance_count,\n",
    "                             max_runtime_in_seconds=7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-08-18-21-30-21-410\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv-all/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-scikit-text-to-bert.py',\n",
    "              inputs=[\n",
    "                    ProcessingInput(source=raw_input_data_s3_uri,\n",
    "                                    destination='/opt/ml/processing/input/data/',\n",
    "                                    s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                    ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                     output_name='bert-train',\n",
    "                                     source='/opt/ml/processing/output/bert/train'),\n",
    "                    ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                     output_name='bert-validation',\n",
    "                                     source='/opt/ml/processing/output/bert/validation'),\n",
    "                    ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                     output_name='bert-test',\n",
    "                                     source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset)\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-08-18-21-30-21-410\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/sagemaker-scikit-learn-2020-08-18-21-30-21-410\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, scikit_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2020-08-18-21-30-21-410;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv-all/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2020-08-18-21-30-21-410', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 20, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 7200}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '64', '--balance-dataset', 'True']}, 'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:835319576252:processing-job/sagemaker-scikit-learn-2020-08-18-21-30-21-410', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 8, 18, 21, 30, 22, 92000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 8, 18, 21, 30, 21, 815000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '04f7647f-3027-4288-9633-e3d0aab38d90', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '04f7647f-3027-4288-9633-e3d0aab38d90', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2324', 'date': 'Tue, 18 Aug 2020 21:30:23 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................________________________________________________!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Data\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-train\n",
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-validation\n",
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-08-18-21-30-21-410/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-18 21:37:52     544110 part-algo-1-amazon_reviews_us_Apparel_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:52     188249 part-algo-1-amazon_reviews_us_Home_Improvement_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:52     344551 part-algo-1-amazon_reviews_us_Toys_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:57     353794 part-algo-10-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord\r\n",
      "2020-08-18 21:37:57     241462 part-algo-10-amazon_reviews_us_Music_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:37      37469 part-algo-11-amazon_reviews_us_Digital_Music_Purchase_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:37      60966 part-algo-11-amazon_reviews_us_Musical_Instruments_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:12      10713 part-algo-12-amazon_reviews_us_Digital_Software_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:12     209024 part-algo-12-amazon_reviews_us_Office_Products_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:17     289336 part-algo-13-amazon_reviews_us_Digital_Video_Download_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:17     166156 part-algo-13-amazon_reviews_us_Outdoors_v1_00.tfrecord\r\n",
      "2020-08-18 21:38:32      11826 part-algo-14-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\r\n",
      "2020-08-18 21:38:32     549098 part-algo-14-amazon_reviews_us_PC_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:17     272740 part-algo-15-amazon_reviews_us_Electronics_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:17       8639 part-algo-15-amazon_reviews_us_Personal_Care_Appliances_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:12      66787 part-algo-16-amazon_reviews_us_Furniture_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:12     229153 part-algo-16-amazon_reviews_us_Pet_Products_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:27       2616 part-algo-17-amazon_reviews_us_Gift_Card_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:27     347623 part-algo-17-amazon_reviews_us_Shoes_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:02     158427 part-algo-18-amazon_reviews_us_Grocery_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:02      38072 part-algo-18-amazon_reviews_us_Software_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:58     416680 part-algo-19-amazon_reviews_us_Health_Personal_Care_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:58     344644 part-algo-19-amazon_reviews_us_Sports_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00     219525 part-algo-2-amazon_reviews_us_Automotive_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00     485227 part-algo-2-amazon_reviews_us_Home_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:52      67583 part-algo-20-amazon_reviews_us_Home_Entertainment_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:52     112444 part-algo-20-amazon_reviews_us_Tools_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:57     154720 part-algo-3-amazon_reviews_us_Baby_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:57     148930 part-algo-3-amazon_reviews_us_Jewelry_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:57     144975 part-algo-3-amazon_reviews_us_Video_Games_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47     395576 part-algo-4-amazon_reviews_us_Beauty_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47     365495 part-algo-4-amazon_reviews_us_Kitchen_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47      30334 part-algo-4-amazon_reviews_us_Video_v1_00.tfrecord\r\n",
      "2020-08-18 23:37:59     203375 part-algo-5-amazon_reviews_us_Lawn_and_Garden_v1_00.tfrecord\r\n",
      "2020-08-18 23:37:59      78270 part-algo-5-amazon_reviews_us_Watches_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00     479073 part-algo-6-amazon_reviews_us_Books_v1_01.tfrecord\r\n",
      "2020-08-18 23:38:00      27031 part-algo-6-amazon_reviews_us_Luggage_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47     266623 part-algo-7-amazon_reviews_us_Books_v1_02.tfrecord\r\n",
      "2020-08-18 21:37:47       8382 part-algo-7-amazon_reviews_us_Major_Appliances_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:32     139257 part-algo-8-amazon_reviews_us_Camera_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:32     359454 part-algo-8-amazon_reviews_us_Mobile_Apps_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      11289 part-algo-9-amazon_reviews_us_Mobile_Electronics_v1_00.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-18 21:37:52      30193 part-algo-1-amazon_reviews_us_Apparel_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:52      10653 part-algo-1-amazon_reviews_us_Home_Improvement_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:52      19451 part-algo-1-amazon_reviews_us_Toys_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:57      19887 part-algo-10-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord\r\n",
      "2020-08-18 21:37:57      13552 part-algo-10-amazon_reviews_us_Music_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:38       2283 part-algo-11-amazon_reviews_us_Digital_Music_Purchase_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:38       3754 part-algo-11-amazon_reviews_us_Musical_Instruments_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13        669 part-algo-12-amazon_reviews_us_Digital_Software_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13      11527 part-algo-12-amazon_reviews_us_Office_Products_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18      16304 part-algo-13-amazon_reviews_us_Digital_Video_Download_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18       9387 part-algo-13-amazon_reviews_us_Outdoors_v1_00.tfrecord\r\n",
      "2020-08-18 21:38:33        709 part-algo-14-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\r\n",
      "2020-08-18 21:38:33      30755 part-algo-14-amazon_reviews_us_PC_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18      15011 part-algo-15-amazon_reviews_us_Electronics_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18        715 part-algo-15-amazon_reviews_us_Personal_Care_Appliances_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13       3665 part-algo-16-amazon_reviews_us_Furniture_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13      12722 part-algo-16-amazon_reviews_us_Pet_Products_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:28        313 part-algo-17-amazon_reviews_us_Gift_Card_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:28      19423 part-algo-17-amazon_reviews_us_Shoes_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:03       9397 part-algo-18-amazon_reviews_us_Grocery_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:03       2513 part-algo-18-amazon_reviews_us_Software_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:58      23325 part-algo-19-amazon_reviews_us_Health_Personal_Care_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:58      19150 part-algo-19-amazon_reviews_us_Sports_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      12078 part-algo-2-amazon_reviews_us_Automotive_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      27161 part-algo-2-amazon_reviews_us_Home_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:53       3819 part-algo-20-amazon_reviews_us_Home_Entertainment_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:53       6283 part-algo-20-amazon_reviews_us_Tools_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:57       8678 part-algo-3-amazon_reviews_us_Baby_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:57       8190 part-algo-3-amazon_reviews_us_Jewelry_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:57       8213 part-algo-3-amazon_reviews_us_Video_Games_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47      22325 part-algo-4-amazon_reviews_us_Beauty_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47      20807 part-algo-4-amazon_reviews_us_Kitchen_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47       1725 part-algo-4-amazon_reviews_us_Video_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      11644 part-algo-5-amazon_reviews_us_Lawn_and_Garden_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00       4569 part-algo-5-amazon_reviews_us_Watches_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      27043 part-algo-6-amazon_reviews_us_Books_v1_01.tfrecord\r\n",
      "2020-08-18 23:38:00       1703 part-algo-6-amazon_reviews_us_Luggage_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:47      15001 part-algo-7-amazon_reviews_us_Books_v1_02.tfrecord\r\n",
      "2020-08-18 21:37:47        679 part-algo-7-amazon_reviews_us_Major_Appliances_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:33       7766 part-algo-8-amazon_reviews_us_Camera_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:33      20272 part-algo-8-amazon_reviews_us_Mobile_Apps_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00        717 part-algo-9-amazon_reviews_us_Mobile_Electronics_v1_00.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-18 21:37:53      30244 part-algo-1-amazon_reviews_us_Apparel_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:53      10434 part-algo-1-amazon_reviews_us_Home_Improvement_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:53      19832 part-algo-1-amazon_reviews_us_Toys_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:58      19811 part-algo-10-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord\r\n",
      "2020-08-18 21:37:58      13406 part-algo-10-amazon_reviews_us_Music_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:38       2358 part-algo-11-amazon_reviews_us_Digital_Music_Purchase_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:38       3725 part-algo-11-amazon_reviews_us_Musical_Instruments_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13        713 part-algo-12-amazon_reviews_us_Digital_Software_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13      11842 part-algo-12-amazon_reviews_us_Office_Products_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18      16321 part-algo-13-amazon_reviews_us_Digital_Video_Download_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18       9469 part-algo-13-amazon_reviews_us_Outdoors_v1_00.tfrecord\r\n",
      "2020-08-18 21:38:33        666 part-algo-14-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\r\n",
      "2020-08-18 21:38:33      30762 part-algo-14-amazon_reviews_us_PC_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18      15040 part-algo-15-amazon_reviews_us_Electronics_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:18        693 part-algo-15-amazon_reviews_us_Personal_Care_Appliances_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13       3624 part-algo-16-amazon_reviews_us_Furniture_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:13      12519 part-algo-16-amazon_reviews_us_Pet_Products_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:28        324 part-algo-17-amazon_reviews_us_Gift_Card_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:28      19756 part-algo-17-amazon_reviews_us_Shoes_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:03       9119 part-algo-18-amazon_reviews_us_Grocery_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:03       2394 part-algo-18-amazon_reviews_us_Software_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:58      23671 part-algo-19-amazon_reviews_us_Health_Personal_Care_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:58      19185 part-algo-19-amazon_reviews_us_Sports_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      12324 part-algo-2-amazon_reviews_us_Automotive_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      27156 part-algo-2-amazon_reviews_us_Home_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:53       3730 part-algo-20-amazon_reviews_us_Home_Entertainment_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:53       6368 part-algo-20-amazon_reviews_us_Tools_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:58       8819 part-algo-3-amazon_reviews_us_Baby_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:58       8498 part-algo-3-amazon_reviews_us_Jewelry_v1_00.tfrecord\r\n",
      "2020-08-18 21:36:58       8189 part-algo-3-amazon_reviews_us_Video_Games_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:48      22357 part-algo-4-amazon_reviews_us_Beauty_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:48      20363 part-algo-4-amazon_reviews_us_Kitchen_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:48       1761 part-algo-4-amazon_reviews_us_Video_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      11360 part-algo-5-amazon_reviews_us_Lawn_and_Garden_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00       4566 part-algo-5-amazon_reviews_us_Watches_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00      26840 part-algo-6-amazon_reviews_us_Books_v1_01.tfrecord\r\n",
      "2020-08-18 23:38:00       1668 part-algo-6-amazon_reviews_us_Luggage_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:48      14982 part-algo-7-amazon_reviews_us_Books_v1_02.tfrecord\r\n",
      "2020-08-18 21:37:48        668 part-algo-7-amazon_reviews_us_Major_Appliances_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:33       7894 part-algo-8-amazon_reviews_us_Camera_v1_00.tfrecord\r\n",
      "2020-08-18 21:37:33      20275 part-algo-8-amazon_reviews_us_Mobile_Apps_v1_00.tfrecord\r\n",
      "2020-08-18 23:38:00        683 part-algo-9-amazon_reviews_us_Mobile_Electronics_v1_00.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'raw_input_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'max_seq_length' (int)\n"
     ]
    }
   ],
   "source": [
    "%store max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store train_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'validation_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store validation_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store test_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'balance_dataset' (bool)\n"
     ]
    }
   ],
   "source": [
    "%store balance_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_train_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_train_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_validation_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_validation_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "balance_dataset                                   -> True\n",
      "experiment_name                                   -> 'Amazon-Customer-Reviews-BERT-Experiment-159772236\n",
      "max_seq_length                                    -> 64\n",
      "model_ab_endpoint_name                            -> 'tensorflow-training-2020-08-18-03-46-11-116-abtes\n",
      "prepare_trial_component_name                      -> 'TrialComponent-2020-08-18-034608-bgtf'\n",
      "processed_test_data_s3_uri                        -> 's3://sagemaker-us-east-1-835319576252/sagemaker-s\n",
      "processed_train_data_s3_uri                       -> 's3://sagemaker-us-east-1-835319576252/sagemaker-s\n",
      "processed_validation_data_s3_uri                  -> 's3://sagemaker-us-east-1-835319576252/sagemaker-s\n",
      "processing_code_s3_prefix                         -> 'pipeline_sklearn_processing/1597769474/code'\n",
      "pytorch_endpoint_name                             -> 'tensorflow-training-2020-08-18-03-46-11-116-pt-15\n",
      "pytorch_model_name                                -> 'tensorflow-training-2020-08-18-03-46-11-116-pt-15\n",
      "raw_input_data_s3_uri                             -> 's3://sagemaker-us-east-1-835319576252/amazon-revi\n",
      "step_functions_pipeline_endpoint_name             -> 'training-pipeline-2020-08-18-16-51-28'\n",
      "stepfunction_arn                                  -> 'arn:aws:states:us-east-1:835319576252:stateMachin\n",
      "stepfunction_name                                 -> 'training-pipeline-2020-08-18-16-51-16'\n",
      "test_split_percentage                             -> 0.05\n",
      "train_split_percentage                            -> 0.9\n",
      "training_job_debugger_artifacts_path              -> 's3://sagemaker-us-east-1-835319576252/tensorflow-\n",
      "training_job_name                                 -> 'tensorflow-training-2020-08-18-03-46-11-116'\n",
      "transformer_pytorch_model_name                    -> 'pytorch_model.bin'\n",
      "transformer_pytorch_model_s3_uri                  -> 's3://sagemaker-us-east-1-835319576252/models/tran\n",
      "trial_name                                        -> 'trial-1597722368'\n",
      "tuning_job_name                                   -> 'tensorflow-training-200818-0212'\n",
      "validation_split_percentage                       -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
