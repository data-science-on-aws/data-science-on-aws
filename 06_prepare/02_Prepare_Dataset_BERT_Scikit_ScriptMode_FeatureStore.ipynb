{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon a SageMaker Processing Job and Scikit-Learn\n",
    "\n",
    "In this notebook, we convert raw text into BERT embeddings.  This will allow us to perform natural language processing tasks such as text classification.\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Scikit-Learn are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Scikit-Learn in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  THIS NOTEBOOK WILL TAKE A 5-10 MINUTES TO COMPLETE.\n",
    "\n",
    "# PLEASE BE PATIENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Setup Environment\n",
    "1. Setup Input Data\n",
    "1. Setup Output Data\n",
    "1. Build a Scikit-Learn container for running the processing job\n",
    "1. Run the Processing Job using Amazon SageMaker\n",
    "1. Inspect the Processed Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "s3 = boto3.Session().client(service_name='s3', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r s3_public_path_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s3_public_path_tsv\n",
    "except NameError:\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the INGEST section before you continue.')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://amazon-reviews-pds/tsv\n"
     ]
    }
   ],
   "source": [
    "print(s3_public_path_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r s3_private_path_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s3_private_path_tsv\n",
    "except NameError:\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the INGEST section before you continue.')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv\n"
     ]
    }
   ],
   "source": [
    "print(s3_private_path_tsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Copy 1 More Large Data File to Use For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-18 17:44:16   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n",
      "2020-12-18 17:44:18   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Processing Job using Amazon SageMaker\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job using our custom python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Processing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime, strftime, sleep\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m## PIP INSTALLS ##\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpandas==1.0.5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.1.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==2.8.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker==2.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Session\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_group\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m FeatureGroup\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_definition\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    FeatureDefinition,\n",
      "    FeatureTypeEnum,\n",
      ")\n",
      "\n",
      "\n",
      "\u001b[37m############################\u001b[39;49;00m\n",
      "\u001b[37m#region='us-east-1'\u001b[39;49;00m\n",
      "\u001b[37m#os.environ['AWS_DEFAULT_REGION'] = region\u001b[39;49;00m\n",
      "\n",
      "region = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mAWS_DEFAULT_REGION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRegion: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(region))\n",
      "\u001b[37m############################\u001b[39;49;00m\n",
      "\n",
      "sm = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "featurestore_runtime = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-featurestore-runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "s3 = boto3.Session(region_name=region).client(service_name=\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name=region), sagemaker_client=sm, sagemaker_featurestore_runtime_client=featurestore_runtime)\n",
      "\n",
      "role = sagemaker.get_execution_role()\n",
      "bucket = sagemaker_session.default_bucket()\n",
      "\n",
      "\u001b[37m############################\u001b[39;49;00m\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "REVIEW_BODY_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "REVIEW_ID_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "\u001b[37m# DATE_COLUMN = 'date'\u001b[39;49;00m\n",
      "\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "    \n",
      "label_map = {}\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\n",
      "    label_map[label] = i\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcast_object_to_string\u001b[39;49;00m(data_frame):\n",
      "    \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m data_frame.columns:\n",
      "        \u001b[34mif\u001b[39;49;00m data_frame.dtypes[label] == \u001b[33m'\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            data_frame[label] = data_frame[label].astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstring\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m data_frame\n",
      "            \n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mwait_for_feature_group_creation_complete\u001b[39;49;00m(feature_group):\n",
      "    status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mwhile\u001b[39;49;00m status == \u001b[33m\"\u001b[39;49;00m\u001b[33mCreating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for Feature Group Creation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        time.sleep(\u001b[34m5\u001b[39;49;00m)\n",
      "        status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m status != \u001b[33m\"\u001b[39;49;00m\u001b[33mCreated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFailed to create feature group \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroup \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m successfully created.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "            \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_or_load_feature_group\u001b[39;49;00m(prefix, feature_group_name):\n",
      "\n",
      "    \u001b[37m# Feature Definitions for our records\u001b[39;49;00m\n",
      "    feature_definitions= [\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING)            \n",
      "    ]\n",
      "    \n",
      "    feature_group = FeatureGroup(\n",
      "        name=feature_group_name,\n",
      "        feature_definitions=feature_definitions,\n",
      "        sagemaker_session=sagemaker_session)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group))\n",
      "    \n",
      "    \u001b[34mtry\u001b[39;49;00m:                \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for existing Feature Group to become available if it is being created by another instance in our cluster...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        wait_for_feature_group_creation_complete(feature_group)\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        record_identifier_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        event_time_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        feature_group.create(\n",
      "            s3_uri=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{bucket}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{prefix}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            record_identifier_name=record_identifier_feature_name,\n",
      "            event_time_feature_name=event_time_feature_name,\n",
      "            role_arn=role,\n",
      "            enable_online_store=\u001b[34mTrue\u001b[39;49;00m\n",
      "        )\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating Feature Group. Completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for new Feature Group to become available...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        wait_for_feature_group_creation_complete(feature_group)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature Group available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)        \n",
      "    \u001b[34mexcept\u001b[39;49;00m:\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "        \n",
      "    feature_group.describe()        \n",
      "        \n",
      "    \u001b[34mreturn\u001b[39;49;00m feature_group\n",
      "\n",
      "\n",
      "    \n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\n",
      "               input_ids,\n",
      "               input_mask,\n",
      "               segment_ids,\n",
      "               label_id,\n",
      "               review_id,\n",
      "               date,\n",
      "               label,\n",
      "               review_body):\n",
      "        \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\n",
      "        \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.label_id = label_id\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "        \u001b[36mself\u001b[39;49;00m.review_body = review_body\n",
      "    \n",
      "    \n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, review_id, date, label=\u001b[34mNone\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\n",
      "    \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "    \u001b[36mself\u001b[39;49;00m.date = date\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\n",
      "    \n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(the_input, max_seq_length):\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    tokens = tokenizer.tokenize(the_input.text)    \n",
      "\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    encode_plus_tokens = tokenizer.encode_plus(the_input.text,\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                               max_length=max_seq_length,\n",
      "\u001b[37m#                                               truncation=True\u001b[39;49;00m\n",
      "                                              )\n",
      "\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\n",
      "\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\n",
      "    label_id = label_map[the_input.label]\n",
      "\n",
      "    features = InputFeatures(\n",
      "        input_ids=input_ids,\n",
      "        input_mask=input_mask,\n",
      "        segment_ids=segment_ids,\n",
      "        label_id=label_id,\n",
      "        review_id=the_input.review_id,\n",
      "        date=the_input.date,\n",
      "        label=the_input.label,\n",
      "        review_body=the_input.text)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**input_ids**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.input_ids))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**input_mask**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.input_mask))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**segment_ids**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.segment_ids))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**label_id**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.label_id))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**review_id**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.review_id))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**date**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.date))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**label**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.label))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**review_body**\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(features.review_body))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_inputs_to_tfrecord\u001b[39;49;00m(inputs,\n",
      "                                 output_file,\n",
      "                                 max_seq_length):\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    records = []\n",
      "\n",
      "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, the_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m10000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWriting input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\n",
      "\n",
      "        features = convert_input(the_input, max_seq_length)\n",
      "\n",
      "        all_features = collections.OrderedDict()\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
      "        all_features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
      "\n",
      "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
      "        tf_record_writer.write(tf_record.SerializeToString())\n",
      "\n",
      "        records.append({\u001b[37m#'tf_record': tf_record.SerializeToString(),\u001b[39;49;00m\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_ids,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.input_mask,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.segment_ids,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label_id,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.review_id,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: the_input.date,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.label,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features.review_body\n",
      "                       })\n",
      "\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m####### TODO:  REMOVE THIS BREAK #######\u001b[39;49;00m\n",
      "        \u001b[37m#####################################            \u001b[39;49;00m\n",
      "        \u001b[34mbreak\u001b[39;49;00m\n",
      "        \n",
      "    tf_record_writer.close()\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m records\n",
      "\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\n",
      "    resconfig = {}\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\n",
      "            resconfig = json.load(cfgfile)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "        default=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "    )  \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    ) \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--reviews-feature-group-name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    ) \n",
      "        \n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, \n",
      "                               max_seq_length, \n",
      "                               balance_dataset,\n",
      "                               prefix,\n",
      "                               feature_group_name):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(file))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(balance_dataset))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mprefix \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(prefix)) \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfeature_group_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(feature_group_name))    \n",
      "\n",
      "    \u001b[37m# need to re-load since we can't pass feature_group object in _partial functions for some reason\u001b[39;49;00m\n",
      "    reviews_feature_group = create_or_load_feature_group(prefix, feature_group_name)\n",
      "    \n",
      "    filename_without_extension = Path(Path(file).stem).stem\n",
      "\n",
      "    df = pd.read_csv(file, \n",
      "                     delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                     quoting=csv.QUOTE_NONE,\n",
      "                     compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    df.isna().values.any()\n",
      "    df = df.dropna()\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:  \n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\n",
      "        five_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        four_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        three_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        two_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        one_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        minority_count = \u001b[36mmin\u001b[39;49;00m(five_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             four_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             three_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             two_star_df.shape[\u001b[34m0\u001b[39;49;00m], \n",
      "                             one_star_df.shape[\u001b[34m0\u001b[39;49;00m]) \n",
      "\n",
      "        five_star_df = resample(five_star_df,\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                n_samples = minority_count,\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        four_star_df = resample(four_star_df,\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                n_samples = minority_count,\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        three_star_df = resample(three_star_df,\n",
      "                                 replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                 n_samples = minority_count,\n",
      "                                 random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        two_star_df = resample(two_star_df,\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                               n_samples = minority_count,\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        one_star_df = resample(one_star_df,\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                               n_samples = minority_count,\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\n",
      "\n",
      "        df_balanced = pd.concat([five_star_df, four_star_df, three_star_df, two_star_df, one_star_df])\n",
      "\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_balanced.shape))\n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\n",
      "\n",
      "        df = df_balanced\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.train_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.validation_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.test_split_percentage))    \n",
      "    \n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(holdout_percentage))\n",
      "    df_train, df_holdout = train_test_split(df, \n",
      "                                            test_size=holdout_percentage, \n",
      "                                            stratify=df[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_holdout_percentage))\n",
      "    df_validation, df_test = train_test_split(df_holdout, \n",
      "                                              test_size=test_holdout_percentage,\n",
      "                                              stratify=df_holdout[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_train.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_validation.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_test.shape))\n",
      "\n",
      "    timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mSZ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(timestamp)\n",
      "\n",
      "    train_inputs = df_train.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "                                    label = x[LABEL_COLUMN],\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\n",
      "                                    date = timestamp\n",
      "                            ),\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    validation_inputs = df_validation.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "                                    label = x[LABEL_COLUMN],\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\n",
      "                                    date = timestamp\n",
      "                            ),\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    test_inputs = df_test.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "                                    label = x[LABEL_COLUMN],\n",
      "                                    text = x[REVIEW_BODY_COLUMN],\n",
      "                                    review_id = x[REVIEW_ID_COLUMN],\n",
      "                                    date = timestamp\n",
      "                            ),\n",
      "                  axis = \u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\n",
      "    \u001b[37m# \u001b[39;49;00m\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\n",
      "    train_records = transform_inputs_to_tfrecord(train_inputs, \n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension), \n",
      "                                                         max_seq_length)\n",
      "\n",
      "    validation_records = transform_inputs_to_tfrecord(validation_inputs, \n",
      "                                                              \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension), \n",
      "                                                              max_seq_length)\n",
      "\n",
      "    test_records = transform_inputs_to_tfrecord(test_inputs, \n",
      "                                                        \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension), \n",
      "                                                        max_seq_length)    \n",
      "                \n",
      "    df_train_records = pd.DataFrame.from_dict(train_records)\n",
      "    df_train_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    df_train_records.head()   \n",
      "    \n",
      "    df_validation_records = pd.DataFrame.from_dict(validation_records)\n",
      "    df_validation_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \n",
      "    df_validation_records.head()   \n",
      "\n",
      "    df_test_records = pd.DataFrame.from_dict(test_records)\n",
      "    df_test_records[\u001b[33m'\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m    \n",
      "    df_test_records.head()   \n",
      "    \n",
      "    \u001b[37m# Add record to feature store    \u001b[39;49;00m\n",
      "    df_fs_train_records = cast_object_to_string(df_train_records)\n",
      "    df_fs_validation_records = cast_object_to_string(df_validation_records)\n",
      "    df_fs_test_records = cast_object_to_string(df_test_records)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mIngesting Features...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    reviews_feature_group.ingest(\n",
      "        data_frame=df_fs_train_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )        \n",
      "    reviews_feature_group.ingest(\n",
      "        data_frame=df_fs_validation_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )        \n",
      "    reviews_feature_group.ingest(\n",
      "        data_frame=df_fs_test_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )            \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFeature ingest completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\n",
      "    \n",
      "    reviews_feature_group = create_or_load_feature_group(prefix=args.feature_store_offline_prefix, \n",
      "                                                         feature_group_name=args.reviews_feature_group_name)\n",
      "\n",
      "    reviews_feature_group.describe()\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(reviews_feature_group.as_hive_ddl())\n",
      "    \n",
      "    train_data = \u001b[34mNone\u001b[39;49;00m\n",
      "    validation_data = \u001b[34mNone\u001b[39;49;00m\n",
      "    test_data = \u001b[34mNone\u001b[39;49;00m\n",
      "    \n",
      "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \n",
      "                                                  max_seq_length=args.max_seq_length,\n",
      "                                                  balance_dataset=args.balance_dataset,\n",
      "                                                  prefix=args.feature_store_offline_prefix,\n",
      "                                                  feature_group_name=args.reviews_feature_group_name)\n",
      "\n",
      "    input_files = glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\n",
      "\n",
      "    num_cpus = multiprocessing.cpu_count()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_cpus))\n",
      "\n",
      "    p = multiprocessing.Pool(num_cpus)\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\n",
      "    dirs_output = os.listdir(args.output_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    dirs_output = os.listdir(train_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\n",
      "    dirs_output = os.listdir(validation_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))\n",
      "    dirs_output = os.listdir(test_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "        \n",
      "    offline_store_contents = \u001b[34mNone\u001b[39;49;00m\n",
      "    \u001b[34mwhile\u001b[39;49;00m (offline_store_contents \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m):\n",
      "        objects_in_bucket = s3.list_objects(Bucket=bucket,\n",
      "                                            Prefix=args.feature_store_offline_prefix)\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m objects_in_bucket \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) > \u001b[34m1\u001b[39;49;00m):\n",
      "            offline_store_contents = objects_in_bucket[\u001b[33m'\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWaiting for data in offline store...\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            sleep(\u001b[34m60\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mData available.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "\u001b[37m#     print('QUERY FEATURE STORE...')\u001b[39;49;00m\n",
      "\u001b[37m#     reviews_query = reviews_feature_group.athena_query()\u001b[39;49;00m\n",
      "\u001b[37m#     reviews_table = reviews_query.table_name\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     query_string = 'SELECT * FROM \"'+reviews_table+'\" LIMIT 1'\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print('Running ' + query_string)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     # run Athena query. The output is loaded to a Pandas dataframe.\u001b[39;49;00m\n",
      "\u001b[37m#     dataset = pd.DataFrame()\u001b[39;49;00m\n",
      "\u001b[37m#     reviews_query.run(query_string=query_string, output_location='s3://'+bucket+'/'+prefix+'/query_results/')\u001b[39;49;00m\n",
      "\u001b[37m#     reviews_query.wait()\u001b[39;49;00m\n",
      "\u001b[37m#     dataset = reviews_query.as_dataframe()\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print('Data From Feature Store: {}:'.format(dataset))\u001b[39;49;00m\n",
      "\u001b[37m#     print('DONE!')\u001b[39;49;00m\n",
      "    \n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\n",
      "\n",
      "    process(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job.  You also need to specify one `ProcessingInput` with the `source` argument of the Amazon S3 bucket and `destination` is where the script reads this data from `/opt/ml/processing/input` (inside the Docker container.)  All local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to.  For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name>/`.  You also give the `ProcessingOutput` value for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The arguments parameter in the `run()` method are command-line arguments in our `preprocess-scikit-text-to-bert-feature-store.py` script.\n",
    "\n",
    "Note that we sharding the data using `ShardedByS3Key` to spread the transformations across all worker nodes in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track the `Experiment`\n",
    "We will track every step of this experiment throughout the `prepare`, `train`, `optimize`, and `deploy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "\n",
    "**Experiment**: A collection of related Trials.  Add Trials to an Experiment that you wish to compare together.\n",
    "\n",
    "**Trial**: A description of a multi-step machine learning workflow. Each step in the workflow is described by a Trial Component. There is no relationship between Trial Components such as ordering.\n",
    "\n",
    "**Trial Component**: A description of a single step in a machine learning workflow. For example data cleaning, feature extraction, model training, model evaluation, etc.\n",
    "\n",
    "**Tracker**: A logger of information about a single TrialComponent.\n",
    "\n",
    "<img src=\"img/sagemaker-experiments.png\" width=\"90%\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: Amazon-Customer-Reviews-BERT-Experiment-1609564468\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "experiment = Experiment.create(\n",
    "                experiment_name='Amazon-Customer-Reviews-BERT-Experiment-{}'.format(timestamp),\n",
    "                description='Amazon Customer Reviews BERT Experiment', \n",
    "                sagemaker_boto_client=sm)\n",
    "\n",
    "experiment_name = experiment.experiment_name\n",
    "print('Experiment name: {}'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1609564486\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "trial = Trial.create(trial_name='trial-{}'.format(timestamp),\n",
    "                     experiment_name=experiment_name,\n",
    "                     sagemaker_boto_client=sm)\n",
    "\n",
    "trial_name = trial.trial_name\n",
    "print('Trial name: {}'.format(trial_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Experiment Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    'ExperimentName': experiment_name,\n",
    "    'TrialName': trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'prepare'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon-Customer-Reviews-BERT-Experiment-1609564468\n"
     ]
    }
   ],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'experiment_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial-1609564486\n"
     ]
    }
   ],
   "source": [
    "print(trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Feature Store and Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto3.Session().client(service_name='sagemaker-featurestore-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-store-1609564580\n"
     ]
    }
   ],
   "source": [
    "timestamp = int(time.time())\n",
    "\n",
    "feature_store_offline_prefix = 'reviews-feature-store-' + str(timestamp)\n",
    "print(feature_store_offline_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'feature_store_offline_prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "%store feature_store_offline_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-1609564585\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "# reviews_feature_group_name = 'reviews-feature-group-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "reviews_feature_group_name = 'reviews-feature-group-' + str(timestamp)\n",
    "\n",
    "print(reviews_feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'reviews_feature_group_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store reviews_feature_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "feature_definitions= [\n",
    "    FeatureDefinition(feature_name='input_ids', feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name='input_mask', feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name='segment_ids', feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name='label_id', feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name='review_id', feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name='date', feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name='label', feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "#    FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-1609564585', sagemaker_session=<sagemaker.session.Session object at 0x7f80bdfc9f98>, feature_definitions=[FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='input_mask', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='segment_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>), FeatureDefinition(feature_name='review_id', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>)])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "reviews_feature_group = FeatureGroup(\n",
    "    name=reviews_feature_group_name, \n",
    "    feature_definitions=feature_definitions,\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "print(reviews_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Processing Job Hyper-Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processing_instance_type='ml.c5.2xlarge'\n",
    "processing_instance_count=2\n",
    "train_split_percentage=0.90\n",
    "validation_split_percentage=0.05\n",
    "test_split_percentage=0.05\n",
    "balance_dataset=True\n",
    "max_seq_length=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a `max_seq_length` for BERT\n",
    "Since a smaller `max_seq_length` leads to faster training and lower resource utilization, we want to find the smallest review length that captures `70%` of our reviews.\n",
    "\n",
    "Remember our distribution of review lengths from a previous section?\n",
    "\n",
    "```\n",
    "mean         67.930174\n",
    "std         130.954079\n",
    "min           1.000000\n",
    "10%           4.000000\n",
    "20%          14.000000\n",
    "30%          21.000000\n",
    "40%          25.000000\n",
    "50%          31.000000\n",
    "60%          42.000000\n",
    "70%          59.000000\n",
    "80%          87.000000\n",
    "90%         149.000000\n",
    "100%       5347.000000\n",
    "max        5347.000000\n",
    "```\n",
    "\n",
    "![](img/review_word_count_distribution.png)\n",
    "\n",
    "Review length `59` represents the `70th` percentile for this dataset.  However, it's best to stick with powers-of-2 when using BERT.  So let's choose `64` as this is the smallest power-of-2 greater than `59`.  Reviews with length > `64` will be truncated to `64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(framework_version='0.23-1',\n",
    "                             role=role,\n",
    "                             instance_type=processing_instance_type,\n",
    "                             instance_count=processing_instance_count,\n",
    "                             env={'AWS_DEFAULT_REGION': region},\n",
    "                             max_runtime_in_seconds=7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2021-01-02-05-21-11-395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-01-02-05-21-11-395\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/input/code/preprocess-scikit-text-to-bert-feature-store.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-scikit-text-to-bert-feature-store.py',\n",
    "              inputs=[\n",
    "                    ProcessingInput(source=raw_input_data_s3_uri,\n",
    "                                    destination='/opt/ml/processing/input/data/',\n",
    "                                    s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                    ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                     output_name='bert-train',\n",
    "                                     source='/opt/ml/processing/output/bert/train'),\n",
    "                    ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                     output_name='bert-validation',\n",
    "                                     source='/opt/ml/processing/output/bert/validation'),\n",
    "                    ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                     output_name='bert-test',\n",
    "                                     source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset),\n",
    "                         '--feature-store-offline-prefix', str(feature_store_offline_prefix),\n",
    "                         '--reviews-feature-group-name', str(reviews_feature_group_name)\n",
    "              ],\n",
    "              experiment_config=experiment_config,\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2021-01-02-05-21-11-395\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/sagemaker-scikit-learn-2021-01-02-05-21-11-395\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, scikit_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2021-01-02-05-21-11-395;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor the Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ProcessingInputs': [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/input/code/preprocess-scikit-text-to-bert-feature-store.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2021-01-02-05-21-11-395', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 7200}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '64', '--balance-dataset', 'True', '--feature-store-offline-prefix', 'reviews-feature-store-1609564580', '--reviews-feature-group-name', 'reviews-feature-group-1609564585']}, 'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'}, 'RoleArn': 'arn:aws:iam::231218423789:role/TeamRole', 'ExperimentConfig': {'ExperimentName': 'Amazon-Customer-Reviews-BERT-Experiment-1609564468', 'TrialName': 'trial-1609564486', 'TrialComponentDisplayName': 'prepare'}, 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:231218423789:processing-job/sagemaker-scikit-learn-2021-01-02-05-21-11-395', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2021, 1, 2, 5, 21, 12, 583000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2021, 1, 2, 5, 21, 11, 970000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '6b131107-47af-4c2d-b28c-b7763452e9fd', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '6b131107-47af-4c2d-b28c-b7763452e9fd', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2734', 'date': 'Sat, 02 Jan 2021 05:21:16 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sess)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Data\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train\n",
      "s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation\n",
      "s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-02 05:29:21   10478088 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-01-02 05:29:51   11712201 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-02 05:29:22     581775 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-01-02 05:29:52     649157 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-02 05:29:22     583108 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-01-02 05:29:52     653038 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'input-1',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv/',\n",
       "    'LocalPath': '/opt/ml/processing/input/data/',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'ShardedByS3Key',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'code',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/input/code/preprocess-scikit-text-to-bert-feature-store.py',\n",
       "    'LocalPath': '/opt/ml/processing/input/code',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train',\n",
       "     'LocalPath': '/opt/ml/processing/output/bert/train',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False},\n",
       "   {'OutputName': 'bert-validation',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation',\n",
       "     'LocalPath': '/opt/ml/processing/output/bert/validation',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False},\n",
       "   {'OutputName': 'bert-test',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test',\n",
       "     'LocalPath': '/opt/ml/processing/output/bert/test',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'sagemaker-scikit-learn-2021-01-02-05-21-11-395',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2,\n",
       "   'InstanceType': 'ml.c5.2xlarge',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 7200},\n",
       " 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "  'ContainerEntrypoint': ['python3',\n",
       "   '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py'],\n",
       "  'ContainerArguments': ['--train-split-percentage',\n",
       "   '0.9',\n",
       "   '--validation-split-percentage',\n",
       "   '0.05',\n",
       "   '--test-split-percentage',\n",
       "   '0.05',\n",
       "   '--max-seq-length',\n",
       "   '64',\n",
       "   '--balance-dataset',\n",
       "   'True',\n",
       "   '--feature-store-offline-prefix',\n",
       "   'reviews-feature-store-1609564580',\n",
       "   '--reviews-feature-group-name',\n",
       "   'reviews-feature-group-1609564585']},\n",
       " 'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
       " 'RoleArn': 'arn:aws:iam::231218423789:role/TeamRole',\n",
       " 'ExperimentConfig': {'ExperimentName': 'Amazon-Customer-Reviews-BERT-Experiment-1609564468',\n",
       "  'TrialName': 'trial-1609564486',\n",
       "  'TrialComponentDisplayName': 'prepare'},\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:231218423789:processing-job/sagemaker-scikit-learn-2021-01-02-05-21-11-395',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2021, 1, 2, 5, 29, 56, 887000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2021, 1, 2, 5, 24, 35, 746000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 1, 2, 5, 29, 56, 890000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2021, 1, 2, 5, 21, 11, 970000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'd0706973-878a-43b4-952c-0c878cbeb6b3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd0706973-878a-43b4-952c-0c878cbeb6b3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2808',\n",
       "   'date': 'Sat, 02 Jan 2021 05:30:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'raw_input_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'max_seq_length' (int)\n"
     ]
    }
   ],
   "source": [
    "%store max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store train_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'validation_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store validation_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store test_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'balance_dataset' (bool)\n"
     ]
    }
   ],
   "source": [
    "%store balance_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_train_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_train_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_validation_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_validation_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "balance_dataset                                                 -> True\n",
      "experiment_name                                                 -> 'Amazon-Customer-Reviews-BERT-Experiment-160956446\n",
      "feature_store_offline_prefix                                    -> 'reviews-feature-store-1609564580'\n",
      "ingest_create_athena_db_passed                                  -> True\n",
      "ingest_create_athena_table_parquet_passed                       -> True\n",
      "ingest_create_athena_table_tsv_passed                           -> True\n",
      "max_seq_length                                                  -> 64\n",
      "prepare_trial_component_name                                    -> 'TrialComponent-2020-12-30-035536-zxfy'\n",
      "processed_test_data_s3_uri                                      -> 's3://sagemaker-us-east-1-231218423789/sagemaker-s\n",
      "processed_train_data_s3_uri                                     -> 's3://sagemaker-us-east-1-231218423789/sagemaker-s\n",
      "processed_validation_data_s3_uri                                -> 's3://sagemaker-us-east-1-231218423789/sagemaker-s\n",
      "raw_input_data_s3_uri                                           -> 's3://sagemaker-us-east-1-231218423789/amazon-revi\n",
      "reviews_feature_group_name                                      -> 'reviews-feature-group-1609564585'\n",
      "s3_private_path_tsv                                             -> 's3://sagemaker-us-east-1-231218423789/amazon-revi\n",
      "s3_public_path_tsv                                              -> 's3://amazon-reviews-pds/tsv'\n",
      "sagemaker_pipeline_product_id                                   -> 'prod-j3ufw6hl7utxm'\n",
      "sagemaker_pipeline_product_provisioning_artifact_id             -> 'pa-oacphmo7m2bji'\n",
      "sagemaker_project_arn                                           -> 'arn:aws:sagemaker:us-east-1:231218423789:project/\n",
      "sagemaker_project_id                                            -> 'p-ya8gtyvhaqfr'\n",
      "sagemaker_project_name                                          -> 'antje-16086671327493656'\n",
      "sagemaker_project_name_and_id                                   -> 'antje-16086671327493656-p-ya8gtyvhaqfr'\n",
      "setup_dependencies_passed                                       -> True\n",
      "setup_iam_roles_passed                                          -> True\n",
      "setup_instance_check_passed                                     -> True\n",
      "setup_s3_bucket_passed                                          -> True\n",
      "test_split_percentage                                           -> 0.05\n",
      "train_split_percentage                                          -> 0.9\n",
      "training_job_debugger_artifacts_path                            -> 's3://sagemaker-us-east-1-231218423789/tensorflow-\n",
      "training_job_name                                               -> 'tensorflow-training-2020-12-30-03-55-44-288'\n",
      "trial_name                                                      -> 'trial-1609564486'\n",
      "validation_split_percentage                                     -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query The Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_feature_store_query = reviews_feature_group.athena_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_feature_store_table = reviews_feature_store_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \n",
      "SELECT input_ids, input_mask, segment_ids, label_id, split_type  FROM \"reviews-feature-group-1609564585-1609565128\" WHERE split_type='train' LIMIT 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT input_ids, input_mask, segment_ids, label_id, split_type  FROM \"{}\" WHERE split_type='train' LIMIT 5\n",
    "\"\"\".format(reviews_feature_store_table)\n",
    "\n",
    "print('Running ' + query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query f5dea4b6-bf2a-4779-a7bc-f26e2d2307bc is being executed.\n",
      "INFO:sagemaker:Query f5dea4b6-bf2a-4779-a7bc-f26e2d2307bc successfully executed.\n"
     ]
    }
   ],
   "source": [
    "reviews_feature_store_query.run(query_string=query_string, output_location='s3://'+bucket+'/'+feature_store_offline_prefix+'/query_results/')\n",
    "\n",
    "reviews_feature_store_query.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>split_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 2023, 2515, 2054, 2009, 2003, 4011, 2000...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 6195, 2027, 3030, 2437, 24890, 1998, 709...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 4761, 3343, 2003, 2008, 4268, 2323, 2022...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 1996, 2208, 2993, 2003, 2025, 1037, 2919...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2005, 1037, 3477, 2000, 2377, 2944, 2515...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 2023, 2515, 2054, 2009, 2003, 4011, 2000...   \n",
       "1  [101, 6195, 2027, 3030, 2437, 24890, 1998, 709...   \n",
       "2  [101, 4761, 3343, 2003, 2008, 4268, 2323, 2022...   \n",
       "3  [101, 1996, 2208, 2993, 2003, 2025, 1037, 2919...   \n",
       "4  [101, 2005, 1037, 3477, 2000, 2377, 2944, 2515...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id split_type  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4      train  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         3      train  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0      train  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         3      train  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2      train  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "dataset = reviews_feature_store_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Experiment Tracking Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>AWS_DEFAULT_REGION</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>SageMaker.ImageUri - MediaType</th>\n",
       "      <th>SageMaker.ImageUri - Value</th>\n",
       "      <th>code - MediaType</th>\n",
       "      <th>...</th>\n",
       "      <th>input-1 - MediaType</th>\n",
       "      <th>input-1 - Value</th>\n",
       "      <th>bert-test - MediaType</th>\n",
       "      <th>bert-test - Value</th>\n",
       "      <th>bert-train - MediaType</th>\n",
       "      <th>bert-train - Value</th>\n",
       "      <th>bert-validation - MediaType</th>\n",
       "      <th>bert-validation - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sagemaker-scikit-learn-2021-01-02-05-21-11-395-aws-processing-job</td>\n",
       "      <td>prepare</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:processing-job/sagemaker-scikit-learn-2021-01-02-05-21-11-395</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ml.c5.2xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv/</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation</td>\n",
       "      <td>[trial-1609564486]</td>\n",
       "      <td>[Amazon-Customer-Reviews-BERT-Experiment-1609564468]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TrialComponentName  \\\n",
       "0  sagemaker-scikit-learn-2021-01-02-05-21-11-395-aws-processing-job   \n",
       "\n",
       "  DisplayName  \\\n",
       "0     prepare   \n",
       "\n",
       "                                                                                                SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:231218423789:processing-job/sagemaker-scikit-learn-2021-01-02-05-21-11-395   \n",
       "\n",
       "  AWS_DEFAULT_REGION  SageMaker.InstanceCount SageMaker.InstanceType  \\\n",
       "0          us-east-1                      2.0          ml.c5.2xlarge   \n",
       "\n",
       "   SageMaker.VolumeSizeInGB SageMaker.ImageUri - MediaType  \\\n",
       "0                      30.0                           None   \n",
       "\n",
       "                                                           SageMaker.ImageUri - Value  \\\n",
       "0  683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3   \n",
       "\n",
       "  code - MediaType  ... input-1 - MediaType  \\\n",
       "0             None  ...                None   \n",
       "\n",
       "                                                 input-1 - Value  \\\n",
       "0  s3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv/   \n",
       "\n",
       "  bert-test - MediaType  \\\n",
       "0                  None   \n",
       "\n",
       "                                                                                       bert-test - Value  \\\n",
       "0  s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test   \n",
       "\n",
       "  bert-train - MediaType  \\\n",
       "0                   None   \n",
       "\n",
       "                                                                                       bert-train - Value  \\\n",
       "0  s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train   \n",
       "\n",
       "  bert-validation - MediaType  \\\n",
       "0                        None   \n",
       "\n",
       "                                                                                       bert-validation - Value  \\\n",
       "0  s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation   \n",
       "\n",
       "               Trials                                           Experiments  \n",
       "0  [trial-1609564486]  [Amazon-Customer-Reviews-BERT-Experiment-1609564468]  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "#pd.set_option(\"max_rows\", 100)\n",
    "\n",
    "experiment_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=experiment_name,\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Descending\"\n",
    ")\n",
    "\n",
    "experiment_analytics_df = experiment_analytics.dataframe()\n",
    "experiment_analytics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2021-01-02-05-21-11-395-aws-processing-job\n"
     ]
    }
   ],
   "source": [
    "trial_component_name=experiment_analytics_df.TrialComponentName[0]\n",
    "print(trial_component_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrialComponentName': 'sagemaker-scikit-learn-2021-01-02-05-21-11-395-aws-processing-job',\n",
       " 'TrialComponentArn': 'arn:aws:sagemaker:us-east-1:231218423789:experiment-trial-component/sagemaker-scikit-learn-2021-01-02-05-21-11-395-aws-processing-job',\n",
       " 'DisplayName': 'prepare',\n",
       " 'Source': {'SourceArn': 'arn:aws:sagemaker:us-east-1:231218423789:processing-job/sagemaker-scikit-learn-2021-01-02-05-21-11-395',\n",
       "  'SourceType': 'SageMakerProcessingJob'},\n",
       " 'Status': {'PrimaryStatus': 'Completed',\n",
       "  'Message': 'Status: Completed, exit message: null, failure reason: null'},\n",
       " 'StartTime': datetime.datetime(2021, 1, 2, 5, 24, 35, tzinfo=tzlocal()),\n",
       " 'EndTime': datetime.datetime(2021, 1, 2, 5, 29, 56, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2021, 1, 2, 5, 21, 12, 372000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedTime': datetime.datetime(2021, 1, 2, 5, 29, 57, 361000, tzinfo=tzlocal()),\n",
       " 'LastModifiedBy': {},\n",
       " 'Parameters': {'AWS_DEFAULT_REGION': {'StringValue': 'us-east-1'},\n",
       "  'SageMaker.InstanceCount': {'NumberValue': 2.0},\n",
       "  'SageMaker.InstanceType': {'StringValue': 'ml.c5.2xlarge'},\n",
       "  'SageMaker.VolumeSizeInGB': {'NumberValue': 30.0}},\n",
       " 'InputArtifacts': {'SageMaker.ImageUri': {'Value': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "  'code': {'Value': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/input/code/preprocess-scikit-text-to-bert-feature-store.py'},\n",
       "  'input-1': {'Value': 's3://sagemaker-us-east-1-231218423789/amazon-reviews-pds/tsv/'}},\n",
       " 'OutputArtifacts': {'bert-test': {'Value': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test'},\n",
       "  'bert-train': {'Value': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train'},\n",
       "  'bert-validation': {'Value': 's3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation'}},\n",
       " 'Metrics': [],\n",
       " 'ResponseMetadata': {'RequestId': '2830a7d6-8893-40ed-9bcb-648adbd7bbf3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2830a7d6-8893-40ed-9bcb-648adbd7bbf3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1722',\n",
       "   'date': 'Sat, 02 Jan 2021 05:31:26 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_component_description=sm.describe_trial_component(TrialComponentName=trial_component_name)\n",
    "trial_component_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show SageMaker ML Lineage Tracking \n",
    "\n",
    "Amazon SageMaker ML Lineage Tracking creates and stores information about the steps of a machine learning (ML) workflow from data preparation to model deployment. \n",
    "\n",
    "Amazon SageMaker Lineage enables events that happen within SageMaker to be traced via a graph structure. The data simplifies generating reports, making comparisons, or discovering relationships between events. For example easily trace both how a model was generated and where the model was deployed.\n",
    "\n",
    "The lineage graph is created automatically by SageMaker and you can directly create or modify your own graphs.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "* **Lineage Graph** - A connected graph tracing your machine learning workflow end to end.\n",
    "\n",
    "* **Artifacts** - Represents a URI addressable object or data. Artifacts are typically inputs or outputs to Actions.\n",
    "\n",
    "* **Actions** - Represents an action taken such as a computation, transformation, or job.\n",
    "\n",
    "* **Contexts** - Provides a method to logically group other entities.\n",
    "\n",
    "* **Associations** - A directed edge in the lineage graph that links two entities.\n",
    "\n",
    "* **Lineage Traversal** - Starting from an arbitrary point trace the lineage graph to discover and analyze relationships between steps in your workflow.\n",
    "\n",
    "* **Experiments** - Experiment entites (Experiments, Trials, and Trial Components) are also part of the lineage graph and can be associated wtih Artifacts, Actions, or Contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Lineage Artifacts For Our Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>preprocess-scikit-text-to-bert-feature-store.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...t-1-231218423789/amazon-reviews-pds/tsv/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...2021-01-02-05-21-11-395/output/bert-test</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...1-02-05-21-11-395/output/bert-validation</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s3://...021-01-02-05-21-11-395/output/bert-train</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0   preprocess-scikit-text-to-bert-feature-store.py     Input  DataSet   \n",
       "1  s3://...t-1-231218423789/amazon-reviews-pds/tsv/     Input  DataSet   \n",
       "2  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "3  s3://...2021-01-02-05-21-11-395/output/bert-test    Output  DataSet   \n",
       "4  s3://...1-02-05-21-11-395/output/bert-validation    Output  DataSet   \n",
       "5  s3://...021-01-02-05-21-11-395/output/bert-train    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  \n",
       "4         Produced     artifact  \n",
       "5         Produced     artifact  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "lineage_table_viz = LineageTableVisualizer(sess)\n",
    "lineage_table_viz_df = lineage_table_viz.show(processing_job_name=scikit_processing_job_name)\n",
    "lineage_table_viz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Lineage Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtifactName</th>\n",
       "      <th>ArtifactArn</th>\n",
       "      <th>ArtifactType</th>\n",
       "      <th>ArtifactSourceUri</th>\n",
       "      <th>CreationTime</th>\n",
       "      <th>LastModifiedTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/ecd15c0438612224d61f3589e0917461</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test</td>\n",
       "      <td>2021-01-02 05:21:12.696000+00:00</td>\n",
       "      <td>2021-01-02 05:21:12.696000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/727df95432a9b3c5748f9d1ade3119c9</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation</td>\n",
       "      <td>2021-01-02 05:21:12.638000+00:00</td>\n",
       "      <td>2021-01-02 05:21:12.638000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/fb3a97f7277d53d86793db5090d25ba5</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train</td>\n",
       "      <td>2021-01-02 05:21:12.594000+00:00</td>\n",
       "      <td>2021-01-02 05:21:12.594000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/984631fd545f00e867994db12bfd6ffa</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/input/code/preprocess-scikit-text-to-bert-feature-store.py</td>\n",
       "      <td>2021-01-02 05:21:12.480000+00:00</td>\n",
       "      <td>2021-01-02 05:21:12.480000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/64e2869a3ffd419f8254134927265bc3</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-03-55-51-199/output/bert-test</td>\n",
       "      <td>2021-01-02 03:55:52.559000+00:00</td>\n",
       "      <td>2021-01-02 03:55:52.559000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/432a98996416d8da9e2c5896d48bbac5</td>\n",
       "      <td>Image</td>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3</td>\n",
       "      <td>2020-12-29 22:26:39.342000+00:00</td>\n",
       "      <td>2020-12-29 22:26:39.342000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/e0df32aa65a0811e000f812f7fba2301</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-21-922/output/bert-test</td>\n",
       "      <td>2020-12-29 22:19:45.672000+00:00</td>\n",
       "      <td>2020-12-29 22:19:45.672000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/19aa3a35fa1e72f5c30c45df85ba5d88</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-21-922/output/bert-validation</td>\n",
       "      <td>2020-12-29 22:19:45.629000+00:00</td>\n",
       "      <td>2020-12-29 22:19:45.629000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/bcef4b9317ff561f6b9c4b5d8dc7e62f</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-21-922/output/bert-train</td>\n",
       "      <td>2020-12-29 22:19:45.562000+00:00</td>\n",
       "      <td>2020-12-29 22:19:45.562000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>None</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:231218423789:artifact/958a3ca35ffc284a9172dca18ac81166</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-40-176/input/code/preprocess-scikit-text-to-bert.py</td>\n",
       "      <td>2020-12-29 22:19:45.476000+00:00</td>\n",
       "      <td>2020-12-29 22:19:45.476000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ArtifactName  \\\n",
       "0           None   \n",
       "1           None   \n",
       "2           None   \n",
       "3           None   \n",
       "4           None   \n",
       "..           ...   \n",
       "170         None   \n",
       "171         None   \n",
       "172         None   \n",
       "173         None   \n",
       "174         None   \n",
       "\n",
       "                                                                            ArtifactArn  \\\n",
       "0    arn:aws:sagemaker:us-east-1:231218423789:artifact/ecd15c0438612224d61f3589e0917461   \n",
       "1    arn:aws:sagemaker:us-east-1:231218423789:artifact/727df95432a9b3c5748f9d1ade3119c9   \n",
       "2    arn:aws:sagemaker:us-east-1:231218423789:artifact/fb3a97f7277d53d86793db5090d25ba5   \n",
       "3    arn:aws:sagemaker:us-east-1:231218423789:artifact/984631fd545f00e867994db12bfd6ffa   \n",
       "4    arn:aws:sagemaker:us-east-1:231218423789:artifact/64e2869a3ffd419f8254134927265bc3   \n",
       "..                                                                                  ...   \n",
       "170  arn:aws:sagemaker:us-east-1:231218423789:artifact/432a98996416d8da9e2c5896d48bbac5   \n",
       "171  arn:aws:sagemaker:us-east-1:231218423789:artifact/e0df32aa65a0811e000f812f7fba2301   \n",
       "172  arn:aws:sagemaker:us-east-1:231218423789:artifact/19aa3a35fa1e72f5c30c45df85ba5d88   \n",
       "173  arn:aws:sagemaker:us-east-1:231218423789:artifact/bcef4b9317ff561f6b9c4b5d8dc7e62f   \n",
       "174  arn:aws:sagemaker:us-east-1:231218423789:artifact/958a3ca35ffc284a9172dca18ac81166   \n",
       "\n",
       "    ArtifactType  \\\n",
       "0        DataSet   \n",
       "1        DataSet   \n",
       "2        DataSet   \n",
       "3        DataSet   \n",
       "4        DataSet   \n",
       "..           ...   \n",
       "170        Image   \n",
       "171      DataSet   \n",
       "172      DataSet   \n",
       "173      DataSet   \n",
       "174      DataSet   \n",
       "\n",
       "                                                                                                                                   ArtifactSourceUri  \\\n",
       "0                                              s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-test   \n",
       "1                                        s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-validation   \n",
       "2                                             s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/output/bert-train   \n",
       "3    s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-05-21-11-395/input/code/preprocess-scikit-text-to-bert-feature-store.py   \n",
       "4                                              s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2021-01-02-03-55-51-199/output/bert-test   \n",
       "..                                                                                                                                               ...   \n",
       "170                                                                   763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3   \n",
       "171                                            s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-21-922/output/bert-test   \n",
       "172                                      s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-21-922/output/bert-validation   \n",
       "173                                           s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-21-922/output/bert-train   \n",
       "174                s3://sagemaker-us-east-1-231218423789/sagemaker-scikit-learn-2020-12-29-22-19-40-176/input/code/preprocess-scikit-text-to-bert.py   \n",
       "\n",
       "                        CreationTime                 LastModifiedTime  \n",
       "0   2021-01-02 05:21:12.696000+00:00 2021-01-02 05:21:12.696000+00:00  \n",
       "1   2021-01-02 05:21:12.638000+00:00 2021-01-02 05:21:12.638000+00:00  \n",
       "2   2021-01-02 05:21:12.594000+00:00 2021-01-02 05:21:12.594000+00:00  \n",
       "3   2021-01-02 05:21:12.480000+00:00 2021-01-02 05:21:12.480000+00:00  \n",
       "4   2021-01-02 03:55:52.559000+00:00 2021-01-02 03:55:52.559000+00:00  \n",
       "..                               ...                              ...  \n",
       "170 2020-12-29 22:26:39.342000+00:00 2020-12-29 22:26:39.342000+00:00  \n",
       "171 2020-12-29 22:19:45.672000+00:00 2020-12-29 22:19:45.672000+00:00  \n",
       "172 2020-12-29 22:19:45.629000+00:00 2020-12-29 22:19:45.629000+00:00  \n",
       "173 2020-12-29 22:19:45.562000+00:00 2020-12-29 22:19:45.562000+00:00  \n",
       "174 2020-12-29 22:19:45.476000+00:00 2020-12-29 22:19:45.476000+00:00  \n",
       "\n",
       "[175 rows x 6 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ArtifactAnalytics\n",
    "\n",
    "artifact_analytics = ArtifactAnalytics()\n",
    "\n",
    "artifacts_df = artifact_analytics.dataframe()\n",
    "artifacts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
