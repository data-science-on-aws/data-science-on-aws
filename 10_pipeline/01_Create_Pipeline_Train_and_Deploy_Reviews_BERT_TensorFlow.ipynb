{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Training Pipeline with the Step Functions Data Science SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step Functions SageMaker Pipeline](img/stepfunctions_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install StepFunctions\n",
    "_Note:  This will downgrade SageMaker SDK to 1.x instead of 2.x.  You cannot go back at this point!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  The Data Science SDK only supports 1.x at this time.\n",
    "#        Waiting on this:  https://github.com/aws/aws-step-functions-data-science-sdk-python/issues/69\n",
    "\n",
    "!pip install -q stepfunctions==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stepfunctions\n",
    "import logging\n",
    "from stepfunctions.template.pipeline import TrainingPipeline\n",
    "\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an IAM Execution Role for Step Functions\n",
    "We need a StepFunctionsWorkflowExecutionRole so that you can create and execute workflows in Step Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.Session().client(service_name='iam', region_name=region)\n",
    "sts = boto3.Session().client(service_name='sts', region_name=region)\n",
    "sfn = boto3.Session().client(service_name='stepfunctions', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepfunction_role_name = 'DSOAWS_StepFunctionsExecutionRole'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an AssumeRolePolicyDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_doc = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"states.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `DSOAWS_StepFunctionsExecutionRole`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role already exists. This is OK.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "try:\n",
    "    iam.create_role(\n",
    "        RoleName=stepfunction_role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "        Description='DSOAWS Step Function Workflow Execution Role'\n",
    "    )\n",
    "    time.sleep(10)\n",
    "    print(\"Role created.\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Role already exists. This is OK.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Role ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::835319576252:role/DSOAWS_StepFunctionsExecutionRole\n"
     ]
    }
   ],
   "source": [
    "stepfunction_role = iam.get_role(RoleName=stepfunction_role_name)\n",
    "stepfunction_role_arn = stepfunction_role['Role']['Arn']\n",
    "print(stepfunction_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a Policy to the Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepfunction_permissions = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateTransformJob\",\n",
    "                \"sagemaker:DescribeTransformJob\",\n",
    "                \"sagemaker:StopTransformJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:CreateHyperParameterTuningJob\",\n",
    "                \"sagemaker:DescribeHyperParameterTuningJob\",\n",
    "                \"sagemaker:StopHyperParameterTuningJob\",\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DeleteEndpoint\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sagemaker:CreateProcessingJob\",\n",
    "                \"sagemaker:DescribeProcessingJob\",\n",
    "                \"sagemaker:ListProcessingJobs\",\n",
    "                \"sagemaker:StopProcessingJob\",                \n",
    "                \"sagemaker:ListTags\",\n",
    "                \"lambda:InvokeFunction\",\n",
    "                \"sqs:SendMessage\",\n",
    "                \"sns:Publish\",\n",
    "                \"ecs:RunTask\",\n",
    "                \"ecs:StopTask\",\n",
    "                \"ecs:DescribeTasks\",\n",
    "                \"dynamodb:GetItem\",\n",
    "                \"dynamodb:PutItem\",\n",
    "                \"dynamodb:UpdateItem\",\n",
    "                \"dynamodb:DeleteItem\",\n",
    "                \"batch:SubmitJob\",\n",
    "                \"batch:DescribeJobs\",\n",
    "                \"batch:TerminateJob\",\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:GetJobRuns\",\n",
    "                \"glue:BatchStopJobRun\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:PutTargets\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:DescribeRule\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTransformJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTuningJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForECSTaskRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForBatchJobsRule\",\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn into Policy Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepfunction_policy_name = 'DSOAWS_StepFunctionsWorkflowExecutionPolicy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = sts.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy already exists.  Updating policy...\n",
      "** Policy cannot have more than 5 versions.  This is likely OK.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "try:\n",
    "    stepfunction_policy = iam.create_policy(\n",
    "      PolicyName=stepfunction_policy_name,\n",
    "      PolicyDocument=json.dumps(stepfunction_permissions)\n",
    "    )\n",
    "    stepfunction_policy_arn = f'arn:aws:iam::{account_id}:policy/{stepfunction_policy_name}'\n",
    "    print(\"Policy created.\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy already exists.  Updating policy...\")\n",
    "        stepfunction_policy_arn = f'arn:aws:iam::{account_id}:policy/{stepfunction_policy_name}'\n",
    "        try:\n",
    "            stepfunction_policy = iam.create_policy_version(\n",
    "                PolicyArn=stepfunction_policy_arn,\n",
    "                PolicyDocument=json.dumps(stepfunction_permissions),\n",
    "                SetAsDefault=True)\n",
    "            print('Policy updated.')\n",
    "        except:\n",
    "            print('** Policy cannot have more than 5 versions.  This is likely OK.')\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::835319576252:policy/DSOAWS_StepFunctionsWorkflowExecutionPolicy\n"
     ]
    }
   ],
   "source": [
    "print(stepfunction_policy_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Policy To Step Function Workflow Execution Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn=stepfunction_policy_arn,\n",
    "        RoleName=stepfunction_role_name\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is OK.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaRole',\n",
    "        RoleName=stepfunction_role_name\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is OK.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "try:\n",
    "    response = iam.attach_role_policy(\n",
    "        PolicyArn='arn:aws:iam::aws:policy/CloudWatchEventsFullAccess',\n",
    "        RoleName=stepfunction_role_name\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(\"Policy is already attached. This is OK.\")\n",
    "    else:\n",
    "        print(\"Unexpected error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Processing Script to S3 for the Pipeline to Consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow==2.1.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(tf.__version__)\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==2.8.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "DATA_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "    \r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "    \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(text_input, max_seq_length):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    tokens = tokenizer.tokenize(text_input.text)    \r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=max_seq_length,\r\n",
      "\u001b[37m#                                               truncation=True\u001b[39;49;00m\r\n",
      "                                              )\r\n",
      "\r\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.    \u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\r\n",
      "\r\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[text_input.label]\r\n",
      "\r\n",
      "    features = InputFeatures(\r\n",
      "        input_ids=input_ids,\r\n",
      "        input_mask=input_mask,\r\n",
      "        segment_ids=segment_ids,\r\n",
      "        label_id=label_id)\r\n",
      "\r\n",
      "\u001b[37m#    print('**tokens**\\n{}\\n'.format(tokens))    \u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_features_to_tfrecord\u001b[39;49;00m(inputs,\r\n",
      "                                 output_file,\r\n",
      "                                 max_seq_length):\r\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, text_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\r\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m1000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting example \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\r\n",
      "\r\n",
      "            bert_features = convert_input(text_input, max_seq_length)\r\n",
      "        \r\n",
      "            tfrecord_features = collections.OrderedDict()\r\n",
      "            \r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\r\n",
      "            tfrecord_features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\r\n",
      "\r\n",
      "            tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\r\n",
      "            \r\n",
      "            tfrecord_writer.write(tfrecord.SerializeToString())\r\n",
      "\r\n",
      "    tfrecord_writer.close()\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m128\u001b[39;49;00m,\r\n",
      "    )  \r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, \r\n",
      "                               max_seq_length, \r\n",
      "                               balance_dataset):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(file))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(balance_dataset))\r\n",
      "\r\n",
      "    filename_without_extension = Path(Path(file).stem).stem\r\n",
      "\r\n",
      "    df = pd.read_csv(file, \r\n",
      "                     delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                     quoting=csv.QUOTE_NONE,\r\n",
      "                     compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    df.isna().values.any()\r\n",
      "    df = df.dropna()\r\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:  \r\n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\r\n",
      "\r\n",
      "        five_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        four_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        three_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        two_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        one_star_df = df.query(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating == 1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        minority_count = \u001b[36mmin\u001b[39;49;00m(five_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             four_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             three_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             two_star_df.shape[\u001b[34m0\u001b[39;49;00m], \r\n",
      "                             one_star_df.shape[\u001b[34m0\u001b[39;49;00m]) \r\n",
      "\r\n",
      "        five_star_df = resample(five_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        four_star_df = resample(four_star_df,\r\n",
      "                                replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        three_star_df = resample(three_star_df,\r\n",
      "                                 replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                                 n_samples = minority_count,\r\n",
      "                                 random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        two_star_df = resample(two_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        one_star_df = resample(one_star_df,\r\n",
      "                               replace = \u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = \u001b[34m27\u001b[39;49;00m)\r\n",
      "\r\n",
      "        df_balanced = pd.concat([five_star_df, four_star_df, three_star_df, two_star_df, one_star_df])\r\n",
      "\r\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)        \r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_balanced.shape))\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\r\n",
      "\r\n",
      "        df = df_balanced\r\n",
      "        \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df.shape))\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.train_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.validation_split_percentage))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.test_split_percentage))    \r\n",
      "    \r\n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(holdout_percentage))\r\n",
      "    df_train, df_holdout = train_test_split(df, \r\n",
      "                                            test_size=holdout_percentage, \r\n",
      "                                            stratify=df[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_holdout_percentage))\r\n",
      "    df_validation, df_test = train_test_split(df_holdout, \r\n",
      "                                              test_size=test_holdout_percentage,\r\n",
      "                                              stratify=df_holdout[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_train.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_validation.shape))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(df_test.shape))\r\n",
      "\r\n",
      "    train_inputs = df_train.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                         label = x[LABEL_COLUMN]), axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    validation_inputs = df_validation.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                            label = x[LABEL_COLUMN]), axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    test_inputs = df_test.apply(\u001b[34mlambda\u001b[39;49;00m x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                label = x[LABEL_COLUMN]), axis = \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\r\n",
      "    \u001b[37m# \u001b[39;49;00m\r\n",
      "    train_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    validation_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "    test_data = \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data)\r\n",
      "\r\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\r\n",
      "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \r\n",
      "                                                       \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension), \r\n",
      "                                                       max_seq_length)\r\n",
      "\r\n",
      "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\r\n",
      "\r\n",
      "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, \u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension), max_seq_length)\r\n",
      "        \r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.current_host))\r\n",
      "    \r\n",
      "    train_data = \u001b[34mNone\u001b[39;49;00m\r\n",
      "    validation_data = \u001b[34mNone\u001b[39;49;00m\r\n",
      "    test_data = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \r\n",
      "                                                 max_seq_length=args.max_seq_length,\r\n",
      "                                                 balance_dataset=args.balance_dataset\r\n",
      "\r\n",
      "    )\r\n",
      "    input_files = glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.input_data))\r\n",
      "\r\n",
      "    num_cpus = multiprocessing.cpu_count()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_cpus))\r\n",
      "\r\n",
      "    p = multiprocessing.Pool(num_cpus)\r\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.output_data))\r\n",
      "    dirs_output = os.listdir(args.output_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\r\n",
      "    dirs_output = os.listdir(train_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\r\n",
      "    dirs_output = os.listdir(validation_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))\r\n",
      "    dirs_output = os.listdir(test_data)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\r\n",
      "\r\n",
      "    process(args)    \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./preprocess-scikit-text-to-bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "processing_code_s3_prefix = 'pipeline_sklearn_processing/{}/code'.format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_code = sess.upload_data(\n",
    "    './preprocess-scikit-text-to-bert.py',\n",
    "    bucket=bucket,\n",
    "    key_prefix=processing_code_s3_prefix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processing_code_s3_prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processing_code_s3_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_sklearn_processing/1600051223/code\n"
     ]
    }
   ],
   "source": [
    "print(processing_code_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Processing Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=64\n",
    "train_split_percentage=0.90\n",
    "validation_split_percentage=0.05\n",
    "test_split_percentage=0.05\n",
    "balance_dataset=True\n",
    "processing_instance_count=2\n",
    "processing_instance_type='ml.c5.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Raw Inputs S3 Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-14 00:17:44 1294879074 amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz\r\n",
      "2020-09-12 19:06:16   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-09-12 19:06:18   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# TODO:  Update to 0.23-1 when we upgrade to SageMaker SDK 2.5.5\n",
    "processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                             role=role,\n",
    "                             instance_type=processing_instance_type,\n",
    "                             instance_count=processing_instance_count,\n",
    "                             max_runtime_in_seconds=7200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==2.8.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-tensorflow==2.1.0.1.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msmdebug==0.9.3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertForSequenceClassification\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TextClassificationPipeline\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mconfiguration_distilbert\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcallbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelCheckpoint\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_model\n",
      "\u001b[37m#from tensorflow.keras.mixed_precision import experimental as mixed_precision\u001b[39;49;00m\n",
      "\n",
      "\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mselect_data_and_label_from_record\u001b[39;49;00m(record):\n",
      "    x = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    }\n",
      "\n",
      "    y = record[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x, y)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfile_based_input_dataset_builder\u001b[39;49;00m(channel,\n",
      "                                     input_filenames,\n",
      "                                     pipe_mode,\n",
      "                                     is_training,\n",
      "                                     drop_remainder,\n",
      "                                     batch_size,\n",
      "                                     epochs,\n",
      "                                     steps_per_epoch,\n",
      "                                     max_seq_length):\n",
      "\n",
      "    \u001b[37m# For training, we want a lot of parallel reading and shuffling.\u001b[39;49;00m\n",
      "    \u001b[37m# For eval, we want no shuffling and parallel reading doesn't matter.\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m pipe_mode:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using pipe_mode with channel \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\n",
      "        dataset = PipeModeDataset(channel=channel,\n",
      "                                  record_format=\u001b[33m'\u001b[39;49;00m\u001b[33mTFRecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using input_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_filenames))\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
      "\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch * \u001b[34m100\u001b[39;49;00m)\n",
      "\u001b[37m#    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\u001b[39;49;00m\n",
      "\n",
      "    name_to_features = {\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_decode_record\u001b[39;49;00m(record, name_to_features):\n",
      "        \u001b[33m\"\"\"Decodes a record to a TensorFlow example.\"\"\"\u001b[39;49;00m\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\n",
      "        \u001b[37m# TODO:  wip/bert/bert_attention_head_view/train.py\u001b[39;49;00m\n",
      "        \u001b[37m# Convert input_ids into input_tokens with DistilBert vocabulary \u001b[39;49;00m\n",
      "        \u001b[37m#  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\u001b[39;49;00m\n",
      "        \u001b[37m#    hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m record\n",
      "    \n",
      "    dataset = dataset.apply(\n",
      "        tf.data.experimental.map_and_batch(\n",
      "          \u001b[34mlambda\u001b[39;49;00m record: _decode_record(record, name_to_features),\n",
      "          batch_size=batch_size,\n",
      "          drop_remainder=drop_remainder,\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
      "\n",
      "\u001b[37m#    dataset.cache()\u001b[39;49;00m\n",
      "\n",
      "    dataset = dataset.shuffle(buffer_size=\u001b[34m1000\u001b[39;49;00m,\n",
      "                              reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    row_count = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m**************** \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****************\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\n",
      "    \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m dataset.as_numpy_iterator():\n",
      "        \u001b[36mprint\u001b[39;49;00m(row)\n",
      "        \u001b[34mif\u001b[39;49;00m row_count == \u001b[34m5\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "        row_count = row_count + \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_checkpoint_model\u001b[39;49;00m(checkpoint_path):\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "    \n",
      "    glob_pattern = os.path.join(checkpoint_path, \u001b[33m'\u001b[39;49;00m\u001b[33m*.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mglob pattern \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(glob_pattern))\n",
      "\n",
      "    list_of_checkpoint_files = glob.glob(glob_pattern)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mList of checkpoint files \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(list_of_checkpoint_files))\n",
      "    \n",
      "    latest_checkpoint_file = \u001b[36mmax\u001b[39;49;00m(list_of_checkpoint_files)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLatest checkpoint file \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(latest_checkpoint_file))\n",
      "\n",
      "    initial_epoch_number_str = latest_checkpoint_file.rsplit(\u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m].split(\u001b[33m'\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "    initial_epoch_number = \u001b[36mint\u001b[39;49;00m(initial_epoch_number_str)\n",
      "\n",
      "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
      "                                               latest_checkpoint_file,\n",
      "                                               config=config)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mloaded_model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(loaded_model))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minitial_epoch_number \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(initial_epoch_number))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m loaded_model, initial_epoch_number\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, \n",
      "                        default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_base_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \n",
      "                        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m2\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.00003\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "                        default=\u001b[34m0.00000001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_steps_per_epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--freeze_bert_layer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_sagemaker_debugger\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_sample_predictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)        \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_checkpointing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m# This is unused\u001b[39;49;00m\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[37m# This points to the S3 location - this should not be used by our code\u001b[39;49;00m\n",
      "    \u001b[37m# We should use /opt/ml/model/ instead\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument('--model_dir', \u001b[39;49;00m\n",
      "    \u001b[37m#                     type=str, \u001b[39;49;00m\n",
      "    \u001b[37m#                     default=os.environ['SM_MODEL_DIR'])\u001b[39;49;00m\n",
      "     \n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "    \n",
      "    env_var = os.environ \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m) \n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(env_var[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    sm_training_env_json = json.loads(env_var[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    is_master = sm_training_env_json[\u001b[33m'\u001b[39;49;00m\u001b[33mis_master\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mis_master \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(is_master))\n",
      "    \n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\n",
      "    validation_data = args.validation_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\n",
      "    test_data = args.test_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))    \n",
      "    local_model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    output_dir = args.output_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output_dir))    \n",
      "    hosts = args.hosts\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(hosts))    \n",
      "    current_host = args.current_host\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_host))    \n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))\n",
      "    job_name = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSAGEMAKER_JOB_NAME\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mjob_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(job_name))    \n",
      "    use_xla = args.use_xla\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_xla \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_xla))    \n",
      "    use_amp = args.use_amp\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))    \n",
      "    max_seq_length = args.max_seq_length\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))    \n",
      "    train_batch_size = args.train_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_batch_size))    \n",
      "    validation_batch_size = args.validation_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_batch_size))    \n",
      "    test_batch_size = args.test_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_batch_size))    \n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))    \n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))    \n",
      "    epsilon = args.epsilon\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepsilon \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epsilon))    \n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_steps_per_epoch))    \n",
      "    validation_steps = args.validation_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_steps))    \n",
      "    test_steps = args.test_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_steps))    \n",
      "    freeze_bert_layer = args.freeze_bert_layer\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfreeze_bert_layer \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(freeze_bert_layer))    \n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))    \n",
      "    run_validation = args.run_validation\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_validation \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_validation))    \n",
      "    run_test = args.run_test\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_test \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_test))    \n",
      "    run_sample_predictions = args.run_sample_predictions\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_sample_predictions \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_sample_predictions))\n",
      "    enable_tensorboard = args.enable_tensorboard\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_tensorboard))       \n",
      "    enable_checkpointing = args.enable_checkpointing\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_checkpointing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_checkpointing))    \n",
      "\n",
      "    checkpoint_base_path = args.checkpoint_base_path\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcheckpoint_base_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_base_path))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_master:\n",
      "        checkpoint_path = checkpoint_base_path\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        checkpoint_path = \u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcheckpoint_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_path))\n",
      "    \n",
      "    \u001b[37m# Determine if PipeMode is enabled \u001b[39;49;00m\n",
      "    pipe_mode_str = os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    pipe_mode = (pipe_mode_str.find(\u001b[33m'\u001b[39;49;00m\u001b[33mPipe\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) >= \u001b[34m0\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUsing pipe_mode: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(pipe_mode))\n",
      " \n",
      "    \u001b[37m# Model Output \u001b[39;49;00m\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Tensorboard Logs \u001b[39;49;00m\n",
      "    tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Commented out due to incompatibility with transformers library (possibly)\u001b[39;49;00m\n",
      "    \u001b[37m# Set the global precision mixed_precision policy to \"mixed_float16\"    \u001b[39;49;00m\n",
      "\u001b[37m#    mixed_precision_policy = 'mixed_float16'\u001b[39;49;00m\n",
      "\u001b[37m#    print('Mixed precision policy {}'.format(mixed_precision_policy))\u001b[39;49;00m\n",
      "\u001b[37m#    policy = mixed_precision.Policy(mixed_precision_policy)\u001b[39;49;00m\n",
      "\u001b[37m#    mixed_precision.set_policy(policy)    \u001b[39;49;00m\n",
      "    \n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\n",
      "    \u001b[37m# Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\u001b[39;49;00m\n",
      "    \u001b[37m#distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m distributed_strategy.scope():\n",
      "        tf.config.optimizer.set_jit(use_xla)\n",
      "        tf.config.optimizer.set_experimental_options({\u001b[33m\"\u001b[39;49;00m\u001b[33mauto_mixed_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp})\n",
      "\n",
      "        train_data_filenames = glob(os.path.join(train_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data_filenames))\n",
      "        train_dataset = file_based_input_dataset_builder(\n",
      "            channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "            input_filenames=train_data_filenames,\n",
      "            pipe_mode=pipe_mode,\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            batch_size=train_batch_size,\n",
      "            epochs=epochs,\n",
      "            steps_per_epoch=train_steps_per_epoch,\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "\n",
      "        tokenizer = \u001b[34mNone\u001b[39;49;00m\n",
      "        config = \u001b[34mNone\u001b[39;49;00m\n",
      "        model = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# This is required when launching many instances at once...  the urllib request seems to get denied periodically\u001b[39;49;00m\n",
      "        successful_download = \u001b[34mFalse\u001b[39;49;00m\n",
      "        retries = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[34mwhile\u001b[39;49;00m (retries < \u001b[34m5\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m successful_download):\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                config = DistilBertConfig.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                          num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES))\n",
      "                model = TFDistilBertForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                                              config=config)\n",
      "                successful_download = \u001b[34mTrue\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSucessfully downloaded after \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m retries.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries))\n",
      "            \u001b[34mexcept\u001b[39;49;00m:\n",
      "                retries = retries + \u001b[34m1\u001b[39;49;00m\n",
      "                random_sleep = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m30\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRetry #\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.  Sleeping for \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m seconds\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries, random_sleep))\n",
      "                time.sleep(random_sleep)\n",
      "\n",
      "        callbacks = []\n",
      "\n",
      "        initial_epoch_number = \u001b[34m0\u001b[39;49;00m \n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_checkpointing:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Checkpoint enabled *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \n",
      "            os.makedirs(checkpoint_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)        \n",
      "            \u001b[34mif\u001b[39;49;00m os.listdir(checkpoint_path):\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Found checkpoint *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(checkpoint_path)\n",
      "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using checkpoint model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))\n",
      "                \n",
      "            checkpoint_callback = ModelCheckpoint(\n",
      "                    filepath=os.path.join(checkpoint_path, \u001b[33m'\u001b[39;49;00m\u001b[33mtf_model_\u001b[39;49;00m\u001b[33m{epoch:05d}\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "                    save_weights_only=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                    verbose=\u001b[34m1\u001b[39;49;00m,\n",
      "                    monitor=\u001b[33m'\u001b[39;49;00m\u001b[33mval_accuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** CHECKPOINT CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(checkpoint_callback))\n",
      "            callbacks.append(checkpoint_callback)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m tokenizer \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m config:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNot properly initialized...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m** use_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))        \n",
      "        \u001b[34mif\u001b[39;49;00m use_amp:\n",
      "            \u001b[37m# loss scaling is currently required when using mixed precision\u001b[39;49;00m\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \u001b[33m'\u001b[39;49;00m\u001b[33mdynamic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))\n",
      "        \u001b[34mif\u001b[39;49;00m enable_sagemaker_debugger:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** DEBUGGING ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdebug\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36msmd\u001b[39;49;00m\n",
      "            \u001b[37m# This assumes that we specified debugger_hook_config\u001b[39;49;00m\n",
      "            debugger_callback = smd.KerasHook.create_from_json_file()\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** DEBUGGER CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(debugger_callback))            \n",
      "            callbacks.append(debugger_callback)\n",
      "            optimizer = debugger_callback.wrap_optimizer(optimizer)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_tensorboard:            \n",
      "            tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
      "                                                        log_dir=tensorboard_logs_path)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** TENSORBOARD CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorboard_callback))\n",
      "            callbacks.append(tensorboard_callback)\n",
      "  \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** OPTIMIZER \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\n",
      "        \n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCompiled model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))          \n",
      "        model.layers[\u001b[34m0\u001b[39;49;00m].trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_validation:\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data_filenames))\n",
      "            validation_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                input_filenames=validation_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=validation_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=validation_steps,\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "            \n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training and Validation...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\n",
      "            train_and_validation_history = model.fit(train_dataset,\n",
      "                                                     shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                     epochs=epochs,\n",
      "                                                     initial_epoch=initial_epoch_number,\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\n",
      "                                                     validation_data=validation_dataset,\n",
      "                                                     validation_steps=validation_steps,\n",
      "                                                     callbacks=callbacks)                                \n",
      "            \u001b[36mprint\u001b[39;49;00m(train_and_validation_history)\n",
      "        \u001b[34melse\u001b[39;49;00m: \u001b[37m# Not running validation\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting Training (Without Validation)...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            train_history = model.fit(train_dataset,\n",
      "                                      shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                      epochs=epochs,\n",
      "                                      initial_epoch=initial_epoch_number,\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\n",
      "                                      callbacks=callbacks)                \n",
      "            \u001b[36mprint\u001b[39;49;00m(train_history)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_test:\n",
      "            test_data_filenames = glob(os.path.join(test_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data_filenames))\n",
      "            test_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                input_filenames=test_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=test_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=test_steps,\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting test...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            test_history = model.evaluate(test_dataset,\n",
      "                                          steps=test_steps,\n",
      "                                          callbacks=callbacks)\n",
      "                                 \n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTest history \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_history))\n",
      "            \n",
      "        \u001b[37m# Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtransformer_fine_tuned_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(transformer_fine_tuned_model_path))   \n",
      "        model.save_pretrained(transformer_fine_tuned_model_path)\n",
      "\n",
      "        \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))   \n",
      "        model.save(tensorflow_saved_model_path, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                \n",
      "        \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\n",
      "        \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\n",
      "        \u001b[37m#         This appears to be hard-coded and must be called code/\u001b[39;49;00m\n",
      "        inference_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying inference source files to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "        os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)               \n",
      "        os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_path))\n",
      "        \u001b[36mprint\u001b[39;49;00m(glob(inference_path))        \n",
      "\u001b[37m#        os.system('cp requirements.txt {}/code'.format(inference_path))\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[34mif\u001b[39;49;00m run_sample_predictions:\n",
      "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_fine_tuned_model_path,\n",
      "                                                                       id2label={\n",
      "                                                                        \u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m\n",
      "                                                                       },\n",
      "                                                                       label2id={\n",
      "                                                                        \u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n",
      "                                                                        \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m\n",
      "                                                                       })\n",
      "\n",
      "        tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m num_gpus >= \u001b[34m1\u001b[39;49;00m:\n",
      "            inference_device = \u001b[34m0\u001b[39;49;00m \u001b[37m# GPU 0\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            inference_device = -\u001b[34m1\u001b[39;49;00m \u001b[37m# CPU\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minference_device \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_device))\n",
      "\n",
      "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \n",
      "                                                        tokenizer=tokenizer,\n",
      "                                                        framework=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                        device=inference_device)  \n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\n",
      "        df_test_reviews = pd.read_csv(\u001b[33m'\u001b[39;49;00m\u001b[33m./test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                        delimiter=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                                        quoting=csv.QUOTE_NONE,\n",
      "                                        compression=\u001b[33m'\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "\n",
      "        df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\n",
      "        df_test_reviews.shape\n",
      "        df_test_reviews.head()\n",
      "        \n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(review_body):\n",
      "            prediction_map = inference_pipeline(review_body)\n",
      "            \u001b[34mreturn\u001b[39;49;00m prediction_map[\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "        y_test = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].map(predict)\n",
      "        y_test\n",
      "        \n",
      "        y_actual = df_test_reviews[\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        y_actual\n",
      "\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\n",
      "        \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\n",
      "        \n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAccuracy: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, accuracy_score(y_true=y_test, y_pred=y_actual))\n",
      "        \n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mplot_conf_mat\u001b[39;49;00m(cm, classes, title, cmap = plt.cm.Greens):\n",
      "            \u001b[36mprint\u001b[39;49;00m(cm)\n",
      "            plt.imshow(cm, interpolation=\u001b[33m'\u001b[39;49;00m\u001b[33mnearest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, cmap=cmap)\n",
      "            plt.title(title)\n",
      "            plt.colorbar()\n",
      "            tick_marks = np.arange(\u001b[36mlen\u001b[39;49;00m(classes))\n",
      "            plt.xticks(tick_marks, classes, rotation=\u001b[34m45\u001b[39;49;00m)\n",
      "            plt.yticks(tick_marks, classes)\n",
      "\n",
      "            fmt = \u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "            thresh = cm.max() / \u001b[34m2.\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itertools.product(\u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m0\u001b[39;49;00m]), \u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m1\u001b[39;49;00m])):\n",
      "                plt.text(j, i, \u001b[36mformat\u001b[39;49;00m(cm[i, j], fmt),\n",
      "                horizontalalignment=\u001b[33m\"\u001b[39;49;00m\u001b[33mcenter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                color=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m cm[i, j] > thresh \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "                plt.tight_layout()\n",
      "                plt.ylabel(\u001b[33m'\u001b[39;49;00m\u001b[33mTrue label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                plt.xlabel(\u001b[33m'\u001b[39;49;00m\u001b[33mPredicted label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "                \n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "        \u001b[37m#%matplotlib inline\u001b[39;49;00m\n",
      "        \u001b[37m#%config InlineBackend.figure_format='retina'\u001b[39;49;00m\n",
      "\n",
      "        cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\n",
      "\n",
      "        plt.figure()\n",
      "        fig, ax = plt.subplots(figsize=(\u001b[34m10\u001b[39;49;00m,\u001b[34m5\u001b[39;49;00m))\n",
      "        plot_conf_mat(cm, \n",
      "                      classes=[\u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                      title=\u001b[33m'\u001b[39;49;00m\u001b[33mConfusion Matrix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Save the confusion matrix        \u001b[39;49;00m\n",
      "        plt.show()\n",
      "        \n",
      "        \u001b[37m# Model Output \u001b[39;49;00m\n",
      "        metrics_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        plt.savefig(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/confusion_matrix.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metrics_path))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Hyper-Parameters\n",
    "Note that `max_seq_length` is re-used from the processing hyper-parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3\n",
    "learning_rate=0.00001\n",
    "epsilon=0.00000001\n",
    "train_batch_size=128\n",
    "validation_batch_size=128\n",
    "test_batch_size=128\n",
    "train_steps_per_epoch=100\n",
    "validation_steps=100\n",
    "test_steps=100\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.c5.9xlarge'\n",
    "train_volume_size=1024\n",
    "use_xla=True\n",
    "use_amp=True\n",
    "freeze_bert_layer=False\n",
    "enable_sagemaker_debugger=False\n",
    "enable_checkpointing=False\n",
    "enable_tensorboard=False\n",
    "input_mode='Pipe'\n",
    "run_validation=True\n",
    "run_test=True\n",
    "run_sample_predictions=True\n",
    "deploy_instance_count=1\n",
    "deploy_instance_type='ml.m5.4xlarge'\n",
    "#deploy_instance_type='ml.m5.large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "# TODO:  Change these after updated to SageMaker SDK 2.5.5                       \n",
    "#                       instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "#                       instance_type=train_instance_type,\n",
    "#                       volume_size=train_volume_size,                       \n",
    "                       train_instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_volume_size=train_volume_size,\n",
    "                       py_version='py3',\n",
    "                       framework_version='2.1.0',\n",
    "                       hyperparameters={'epochs': epochs,\n",
    "                                        'learning_rate': learning_rate,\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'train_batch_size': train_batch_size,\n",
    "                                        'validation_batch_size': validation_batch_size,\n",
    "                                        'test_batch_size': test_batch_size,                                             \n",
    "                                        'train_steps_per_epoch': train_steps_per_epoch,\n",
    "                                        'validation_steps': validation_steps,\n",
    "                                        'test_steps': test_steps,\n",
    "                                        'use_xla': use_xla,\n",
    "                                        'use_amp': use_amp,                                             \n",
    "                                        'max_seq_length': max_seq_length,\n",
    "                                        'freeze_bert_layer': freeze_bert_layer,\n",
    "                                        'enable_sagemaker_debugger': enable_sagemaker_debugger,\n",
    "                                        'enable_checkpointing': enable_checkpointing,\n",
    "                                        'enable_tensorboard': enable_tensorboard,                                        \n",
    "                                        'run_validation': run_validation,\n",
    "                                        'run_test': run_test,\n",
    "                                        'run_sample_predictions': run_sample_predictions},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "#                       max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pipeline with the Step Functions SDK\n",
    "\n",
    "A typical task for a data scientist is to train a model and deploy that model to an endpoint. Without the Step Functions SDK, this is a four step process on SageMaker that includes the following.\n",
    "\n",
    "1. Training the model\n",
    "2. Creating the model on SageMaker\n",
    "3. Creating an endpoint configuration\n",
    "4. Deploying the trained model to the configured endpoint\n",
    "\n",
    "The Step Functions SDK provides the [TrainingPipeline](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/pipelines.html#stepfunctions.template.pipeline.train.TrainingPipeline) API to simplify this procedure. The following configures `pipeline` with the necessary parameters to define a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline name bert-pipeline-1600051442\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "pipeline_name = 'bert-pipeline-{}'.format(timestamp)\n",
    "\n",
    "print('Pipeline name {}'.format(pipeline_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from sagemaker.utils import base_name_from_image\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "# TODO:  Change this when we upgrade to SageMaker SDK 2.5.5\n",
    "#from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from stepfunctions.steps import (\n",
    "    TrainingStep, \n",
    "    TransformStep, \n",
    "    ModelStep, \n",
    "    EndpointConfigStep, \n",
    "    EndpointStep, \n",
    "    Chain, \n",
    "    Fail, \n",
    "    Catch,\n",
    "    ProcessingStep\n",
    ")\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.template.pipeline.common import WorkflowTemplate\n",
    "from stepfunctions.template.pipeline.common import StepId\n",
    "\n",
    "class TrainingPipelineWithDifferentDeployInstanceTypeAndProcessingJob(WorkflowTemplate):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a standard training pipeline with the following steps in order:\n",
    "        1. Train estimator\n",
    "        2. Create estimator model\n",
    "        3. Endpoint configuration\n",
    "        4. Deploy model\n",
    "    \"\"\"\n",
    "\n",
    "    __allowed_kwargs = ('pipeline_name',)\n",
    "    \n",
    "    def __init__(self, \n",
    "                 processor,\n",
    "                 raw_input_data_s3_uri,\n",
    "                 train_split_percentage,\n",
    "                 validation_split_percentage,\n",
    "                 test_split_percentage,\n",
    "                 max_seq_length,\n",
    "                 balance_dataset,\n",
    "                 estimator, \n",
    "                 role, \n",
    "                 bucket,                  \n",
    "                 client, \n",
    "                 deploy_instance_count, \n",
    "                 deploy_instance_type, \n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            estimator (sagemaker.estimator.EstimatorBase): The estimator to use for training. Can be a BYO estimator, Framework estimator or Amazon algorithm estimator.\n",
    "            role (str): An AWS IAM role (either name or full Amazon Resource Name (ARN)). This role is used to create, manage, and execute the Step Functions workflows.\n",
    "            inputs: Information about the training data. Please refer to the `fit()` method of the associated estimator, as this can take any of the following forms:\n",
    "                * (str) - The S3 location where training data is saved.\n",
    "                * (dict[str, str] or dict[str, `sagemaker.session.s3_input`]) - If using multiple channels for training data, you can specify a dict mapping channel names to strings or `sagemaker.session.s3_input` objects.\n",
    "                * (`sagemaker.session.s3_input`) - Channel configuration for S3 data sources that can provide additional information about the training dataset. See `sagemaker.session.s3_input` for full details.\n",
    "                * (`sagemaker.amazon.amazon_estimator.RecordSet`) - A collection of Amazon `Record` objects serialized and stored in S3. For use with an estimator for an Amazon algorithm.\n",
    "                * (list[`sagemaker.amazon.amazon_estimator.RecordSet`]) - A list of `sagemaker.amazon.amazon_estimator.RecordSet` objects, where each instance is a different channel of training data.\n",
    "            bucket (str): S3 bucket under which the output artifacts from the training job will be stored. The parent path used is built using the format: ``s3://{bucket}/{pipeline_name}/models/{job_name}/``. In this format, `pipeline_name` refers to the keyword argument provided for TrainingPipeline. If a `pipeline_name` argument was not provided, one is auto-generated by the pipeline as `training-pipeline-<timestamp>`. Also, in the format, `job_name` refers to the job name provided when calling the :meth:`TrainingPipeline.run()` method.\n",
    "            client (SFN.Client, optional): boto3 client to use for creating and interacting with the training pipeline in Step Functions. (default: None)\n",
    "        Keyword Args:\n",
    "            pipeline_name (str, optional): Name of the pipeline. This name will be used to name jobs (if not provided when calling execute()), models, endpoints, and S3 objects created by the pipeline. If a `pipeline_name` argument was not provided, one is auto-generated by the pipeline as `training-pipeline-<timestamp>`. (default:None)\n",
    "        \"\"\"\n",
    "        self.processor = processor \n",
    "        self.raw_input_data_s3_uri = raw_input_data_s3_uri\n",
    "        self.train_split_percentage = train_split_percentage\n",
    "        self.validation_split_percentage = validation_split_percentage\n",
    "        self.test_split_percentage = test_split_percentage\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.balance_dataset = balance_dataset\n",
    "        self.estimator = estimator\n",
    "        self.role = role        \n",
    "        self.bucket = bucket\n",
    "        self.deploy_instance_count = deploy_instance_count\n",
    "        self.deploy_instance_type = deploy_instance_type\n",
    "\n",
    "        for key in self.__class__.__allowed_kwargs:\n",
    "            setattr(self, key, kwargs.pop(key, None))\n",
    "\n",
    "        if not self.pipeline_name:\n",
    "            self.__pipeline_name_unique = True\n",
    "            self.pipeline_name = 'training-pipeline-{date}'.format(date=self._generate_timestamp())\n",
    "\n",
    "        self.definition = self.build_workflow_definition()\n",
    "        self.input_template = self._extract_input_template(self.definition)\n",
    "\n",
    "        workflow = Workflow(name=self.pipeline_name, \n",
    "                            definition=self.definition, \n",
    "                            role=role, \n",
    "                            format_json=True, \n",
    "                            client=client)\n",
    "\n",
    "        super(TrainingPipelineWithDifferentDeployInstanceTypeAndProcessingJob, self).__init__(s3_bucket=bucket, \n",
    "                                                                                              workflow=workflow, \n",
    "                                                                                              role=role, \n",
    "                                                                                              client=client)\n",
    "    \n",
    "    def build_workflow_definition(self):\n",
    "        \"\"\"\n",
    "        Build the workflow definition for the training pipeline with all the states involved.\n",
    "        Returns:\n",
    "            :class:`~stepfunctions.steps.states.Chain`: Workflow definition as a chain of states involved in the the training pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        processing_inputs=[\n",
    "                ProcessingInput(\n",
    "                    input_name='raw_input',\n",
    "                    source=raw_input_data_s3_uri,\n",
    "                    destination='/opt/ml/processing/input/data/',\n",
    "                    s3_data_distribution_type='ShardedByS3Key'\n",
    "                ),\n",
    "                ProcessingInput(\n",
    "                    input_name='code',            \n",
    "                    source=input_code,\n",
    "                    destination='/opt/ml/processing/input/code',\n",
    "                )\n",
    "        ]\n",
    "\n",
    "        processed_train_data_s3_uri = 's3://{}/{}/processing/output/bert-train'.format(self.bucket, self.pipeline_name)        \n",
    "        processed_validation_data_s3_uri = 's3://{}/{}/processing/output/bert-validation'.format(self.bucket, self.pipeline_name)        \n",
    "        processed_test_data_s3_uri = 's3://{}/{}/processing/output/bert-test'.format(self.bucket, self.pipeline_name)\n",
    "         \n",
    "        processing_outputs=[\n",
    "                ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                 output_name='bert-train',\n",
    "                                 source='/opt/ml/processing/output/bert/train',\n",
    "                                 destination=processed_train_data_s3_uri\n",
    "                                ),\n",
    "                ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                 output_name='bert-validation',\n",
    "                                 source='/opt/ml/processing/output/bert/validation',\n",
    "                                 destination=processed_validation_data_s3_uri\n",
    "                                ),\n",
    "                ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                 output_name='bert-test',\n",
    "                                 source='/opt/ml/processing/output/bert/test',\n",
    "                                 destination=processed_test_data_s3_uri\n",
    "                                ),\n",
    "        ]        \n",
    "\n",
    "        processing_step = ProcessingStep(\n",
    "            'Processing Job', # StepId.ProcessingJob.value?\n",
    "            processor=self.processor,\n",
    "            job_name=self.pipeline_name,\n",
    "            inputs=processing_inputs,\n",
    "            outputs=processing_outputs,\n",
    "            # experiment_config=experiment_config,\n",
    "            container_arguments=['--train-split-percentage', str(self.train_split_percentage),\n",
    "                                 '--validation-split-percentage', str(self.validation_split_percentage),\n",
    "                                 '--test-split-percentage', str(self.test_split_percentage),\n",
    "                                 '--max-seq-length', str(self.max_seq_length),\n",
    "                                 '--balance-dataset', str(self.balance_dataset)],\n",
    "            container_entrypoint=['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'],\n",
    "        )        \n",
    "\n",
    "# TODO:  Update these when we upgrade to SageMaker SDK 2.5.5        \n",
    "#        s3_input_train_data = TrainingInput(s3_data=processed_train_data_s3_uri, distribution='ShardedByS3Key')\n",
    "#        s3_input_validation_data = TrainingInput(s3_data=processed_validation_data_s3_uri, distribution='ShardedByS3Key')\n",
    "#        s3_input_test_data = TrainingInput(s3_data=processed_test_data_s3_uri, distribution='ShardedByS3Key')\n",
    "\n",
    "        s3_input_train_data = sagemaker.session.s3_input(s3_data=processed_train_data_s3_uri, distribution='ShardedByS3Key')\n",
    "        s3_input_validation_data = sagemaker.session.s3_input(s3_data=processed_validation_data_s3_uri, distribution='ShardedByS3Key')\n",
    "        s3_input_test_data = sagemaker.session.s3_input(s3_data=processed_test_data_s3_uri, distribution='ShardedByS3Key')\n",
    "\n",
    "        training_step = TrainingStep(\n",
    "            StepId.Train.value,\n",
    "            estimator=self.estimator,\n",
    "            job_name=self.pipeline_name + '/estimator-source',\n",
    "            data={\n",
    "                'train': s3_input_train_data,\n",
    "                'validation': s3_input_validation_data,\n",
    "                'test': s3_input_test_data\n",
    "            },\n",
    "        )\n",
    "\n",
    "        model = self.estimator.create_model()\n",
    "        model_step = ModelStep(\n",
    "            StepId.CreateModel.value,\n",
    "            instance_type=deploy_instance_type,\n",
    "            model=model,\n",
    "            model_name=self.pipeline_name\n",
    "        )\n",
    "\n",
    "        endpoint_config_step = EndpointConfigStep(\n",
    "            StepId.ConfigureEndpoint.value,\n",
    "            endpoint_config_name=self.pipeline_name,\n",
    "            model_name=self.pipeline_name,\n",
    "            initial_instance_count=self.deploy_instance_count,\n",
    "            instance_type=self.deploy_instance_type\n",
    "        )\n",
    "        \n",
    "        deploy_step = EndpointStep(\n",
    "            StepId.Deploy.value,\n",
    "            endpoint_name=self.pipeline_name,\n",
    "            endpoint_config_name=self.pipeline_name,\n",
    "        )\n",
    "\n",
    "        return Chain([\n",
    "            processing_step, \n",
    "            training_step, \n",
    "            model_step, \n",
    "            endpoint_config_step, \n",
    "            deploy_step\n",
    "        ])\n",
    "    \n",
    "    def execute(self, job_name=None, hyperparameters=None):\n",
    "        \"\"\"\n",
    "        Run the training pipeline.\n",
    "        \n",
    "        Args:\n",
    "            job_name (str, optional): Name for the training job. If one is not provided, a job name will be auto-generated. (default: None)\n",
    "            hyperparameters (dict, optional): Hyperparameters for the estimator training. (default: None)\n",
    "        \n",
    "        Returns:\n",
    "            :py:class:`~stepfunctions.workflow.Execution`: Running instance of the training pipeline.\n",
    "        \"\"\"\n",
    "        inputs = self.input_template.copy()\n",
    "        \n",
    "        if hyperparameters is not None:\n",
    "            inputs[StepId.Train.value]['HyperParameters'] = {\n",
    "                k: str(v) for k, v in hyperparameters.items()\n",
    "            }\n",
    "        \n",
    "        if job_name is None:\n",
    "            job_name = '{base_name}-{timestamp}'.format(base_name='training-pipeline', timestamp=self._generate_timestamp())\n",
    "            \n",
    "        print(inputs)\n",
    "        \n",
    "        # Configure training and model\n",
    "        inputs[StepId.Train.value]['TrainingJobName'] = 'estimator-' + job_name\n",
    "        inputs[StepId.Train.value]['OutputDataConfig']['S3OutputPath'] = 's3://{s3_bucket}/{pipeline_name}/models'.format(\n",
    "            s3_bucket=self.s3_bucket,\n",
    "            pipeline_name=self.workflow.name\n",
    "        )\n",
    "        inputs[StepId.CreateModel.value]['ModelName'] = job_name\n",
    "\n",
    "        # Configure endpoint\n",
    "        inputs[StepId.ConfigureEndpoint.value]['EndpointConfigName'] = job_name\n",
    "        for variant in inputs[StepId.ConfigureEndpoint.value]['ProductionVariants']:\n",
    "            variant['ModelName'] = job_name\n",
    "        inputs[StepId.Deploy.value]['EndpointConfigName'] = job_name\n",
    "        inputs[StepId.Deploy.value]['EndpointName'] = job_name\n",
    "        \n",
    "        # Configure the path to model artifact\n",
    "        inputs[StepId.CreateModel.value]['PrimaryContainer']['ModelDataUrl'] = '{s3_uri}/{job}/output/model.tar.gz'.format(\n",
    "            s3_uri=inputs[StepId.Train.value]['OutputDataConfig']['S3OutputPath'],\n",
    "            job=inputs[StepId.Train.value]['TrainingJobName']\n",
    "        )\n",
    "        \n",
    "        return self.workflow.execute(inputs=inputs, name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  If you see an error about 'TensorFlowModel' object has no attribute 'image', you are using SageMaker SDK 1.x\n",
    "#        The Data Science SDK only supports 1.x at this time.\n",
    "#        Waiting on this:  https://github.com/aws/aws-step-functions-data-science-sdk-python/issues/69\n",
    "\n",
    "pipeline = TrainingPipelineWithDifferentDeployInstanceTypeAndProcessingJob(\n",
    "    processor=processor,\n",
    "    raw_input_data_s3_uri=raw_input_data_s3_uri,\n",
    "    train_split_percentage=train_split_percentage,\n",
    "    validation_split_percentage=validation_split_percentage,\n",
    "    test_split_percentage=test_split_percentage,\n",
    "    max_seq_length=max_seq_length,\n",
    "    balance_dataset=balance_dataset,\n",
    "    estimator=estimator,\n",
    "    role=stepfunction_role_arn,\n",
    "    bucket=bucket,\n",
    "    client=sfn,\n",
    "    deploy_instance_count=deploy_instance_count,\n",
    "    deploy_instance_type=deploy_instance_type,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now view the workflow definition, and also visualize it as a graph. This workflow and graph represent your training pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the workflow definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"StartAt\": \"Processing Job\",\n",
      "    \"States\": {\n",
      "        \"Processing Job\": {\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createProcessingJob.sync\",\n",
      "            \"Parameters\": {\n",
      "                \"ProcessingJobName.$\": \"$$.Execution.Input['Processing Job'].ProcessingJobName\",\n",
      "                \"ProcessingInputs.$\": \"$$.Execution.Input['Processing Job'].ProcessingInputs\",\n",
      "                \"ProcessingOutputConfig.$\": \"$$.Execution.Input['Processing Job'].ProcessingOutputConfig\",\n",
      "                \"AppSpecification.$\": \"$$.Execution.Input['Processing Job'].AppSpecification\",\n",
      "                \"RoleArn.$\": \"$$.Execution.Input['Processing Job'].RoleArn\",\n",
      "                \"ProcessingResources.$\": \"$$.Execution.Input['Processing Job'].ProcessingResources\",\n",
      "                \"StoppingCondition.$\": \"$$.Execution.Input['Processing Job'].StoppingCondition\"\n",
      "            },\n",
      "            \"Type\": \"Task\",\n",
      "            \"Next\": \"Training\"\n",
      "        },\n",
      "        \"Training\": {\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\",\n",
      "            \"Parameters\": {\n",
      "                \"AlgorithmSpecification.$\": \"$$.Execution.Input['Training'].AlgorithmSpecification\",\n",
      "                \"OutputDataConfig.$\": \"$$.Execution.Input['Training'].OutputDataConfig\",\n",
      "                \"StoppingCondition.$\": \"$$.Execution.Input['Training'].StoppingCondition\",\n",
      "                \"ResourceConfig.$\": \"$$.Execution.Input['Training'].ResourceConfig\",\n",
      "                \"RoleArn.$\": \"$$.Execution.Input['Training'].RoleArn\",\n",
      "                \"InputDataConfig.$\": \"$$.Execution.Input['Training'].InputDataConfig\",\n",
      "                \"HyperParameters.$\": \"$$.Execution.Input['Training'].HyperParameters\",\n",
      "                \"TrainingJobName.$\": \"$$.Execution.Input['Training'].TrainingJobName\",\n",
      "                \"DebugHookConfig.$\": \"$$.Execution.Input['Training'].DebugHookConfig\"\n",
      "            },\n",
      "            \"Type\": \"Task\",\n",
      "            \"Next\": \"Create Model\"\n",
      "        },\n",
      "        \"Create Model\": {\n",
      "            \"Parameters\": {\n",
      "                \"ModelName.$\": \"$$.Execution.Input['Create Model'].ModelName\",\n",
      "                \"PrimaryContainer.$\": \"$$.Execution.Input['Create Model'].PrimaryContainer\",\n",
      "                \"ExecutionRoleArn.$\": \"$$.Execution.Input['Create Model'].ExecutionRoleArn\"\n",
      "            },\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createModel\",\n",
      "            \"Type\": \"Task\",\n",
      "            \"Next\": \"Configure Endpoint\"\n",
      "        },\n",
      "        \"Configure Endpoint\": {\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\",\n",
      "            \"Parameters\": {\n",
      "                \"EndpointConfigName.$\": \"$$.Execution.Input['Configure Endpoint'].EndpointConfigName\",\n",
      "                \"ProductionVariants.$\": \"$$.Execution.Input['Configure Endpoint'].ProductionVariants\"\n",
      "            },\n",
      "            \"Type\": \"Task\",\n",
      "            \"Next\": \"Deploy\"\n",
      "        },\n",
      "        \"Deploy\": {\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\",\n",
      "            \"Parameters\": {\n",
      "                \"EndpointConfigName.$\": \"$$.Execution.Input['Deploy'].EndpointConfigName\",\n",
      "                \"EndpointName.$\": \"$$.Execution.Input['Deploy'].EndpointName\"\n",
      "            },\n",
      "            \"Type\": \"Task\",\n",
      "            \"End\": true\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.workflow.definition.to_json(pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the workflow graph\n",
    "## *Note: This only renders in Jupyter. NOT in JupyterLab.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-453\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-453')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"Processing Job\", \"States\": {\"Processing Job\": {\"Resource\": \"arn:aws:states:::sagemaker:createProcessingJob.sync\", \"Parameters\": {\"ProcessingJobName.$\": \"$$.Execution.Input['Processing Job'].ProcessingJobName\", \"ProcessingInputs.$\": \"$$.Execution.Input['Processing Job'].ProcessingInputs\", \"ProcessingOutputConfig.$\": \"$$.Execution.Input['Processing Job'].ProcessingOutputConfig\", \"AppSpecification.$\": \"$$.Execution.Input['Processing Job'].AppSpecification\", \"RoleArn.$\": \"$$.Execution.Input['Processing Job'].RoleArn\", \"ProcessingResources.$\": \"$$.Execution.Input['Processing Job'].ProcessingResources\", \"StoppingCondition.$\": \"$$.Execution.Input['Processing Job'].StoppingCondition\"}, \"Type\": \"Task\", \"Next\": \"Training\"}, \"Training\": {\"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\", \"Parameters\": {\"AlgorithmSpecification.$\": \"$$.Execution.Input['Training'].AlgorithmSpecification\", \"OutputDataConfig.$\": \"$$.Execution.Input['Training'].OutputDataConfig\", \"StoppingCondition.$\": \"$$.Execution.Input['Training'].StoppingCondition\", \"ResourceConfig.$\": \"$$.Execution.Input['Training'].ResourceConfig\", \"RoleArn.$\": \"$$.Execution.Input['Training'].RoleArn\", \"InputDataConfig.$\": \"$$.Execution.Input['Training'].InputDataConfig\", \"HyperParameters.$\": \"$$.Execution.Input['Training'].HyperParameters\", \"TrainingJobName.$\": \"$$.Execution.Input['Training'].TrainingJobName\", \"DebugHookConfig.$\": \"$$.Execution.Input['Training'].DebugHookConfig\"}, \"Type\": \"Task\", \"Next\": \"Create Model\"}, \"Create Model\": {\"Parameters\": {\"ModelName.$\": \"$$.Execution.Input['Create Model'].ModelName\", \"PrimaryContainer.$\": \"$$.Execution.Input['Create Model'].PrimaryContainer\", \"ExecutionRoleArn.$\": \"$$.Execution.Input['Create Model'].ExecutionRoleArn\"}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Configure Endpoint\"}, \"Configure Endpoint\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Configure Endpoint'].EndpointConfigName\", \"ProductionVariants.$\": \"$$.Execution.Input['Configure Endpoint'].ProductionVariants\"}, \"Type\": \"Task\", \"Next\": \"Deploy\"}, \"Deploy\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Deploy'].EndpointConfigName\", \"EndpointName.$\": \"$$.Execution.Input['Deploy'].EndpointName\"}, \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-453';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should see a graph like this:\n",
    " \n",
    "<img src=\"img/pipeline_created.png\" width=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and execute the pipeline on AWS Step Functions\n",
    "\n",
    "Create the pipeline in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:states:us-east-1:835319576252:stateMachine:training-pipeline-2020-09-14-02-45-45'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sleeping to wait for role and policy creations\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "pipeline.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the workflow with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute). A link will be provided after the following cell is executed. Following this link, you can monitor your pipeline execution on Step Functions' console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Processing Job': {'ProcessingJobName': 'training-pipeline-2020-09-14-02-45-45', 'ProcessingInputs': [{'InputName': 'raw_input', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '64', '--balance-dataset', 'True'], 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py']}, 'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 7200}}, 'Training': {'AlgorithmSpecification': {'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3', 'TrainingInputMode': 'Pipe', 'MetricDefinitions': [{'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'}, {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'}, {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'}, {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]}, 'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-835319576252/'}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'ResourceConfig': {'InstanceCount': 1, 'InstanceType': 'ml.c5.9xlarge', 'VolumeSizeInGB': 1024}, 'RoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881', 'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train', 'S3DataDistributionType': 'ShardedByS3Key'}}, 'ChannelName': 'train'}, {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation', 'S3DataDistributionType': 'ShardedByS3Key'}}, 'ChannelName': 'validation'}, {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test', 'S3DataDistributionType': 'ShardedByS3Key'}}, 'ChannelName': 'test'}], 'HyperParameters': {'epochs': '3', 'learning_rate': '1e-05', 'epsilon': '1e-08', 'train_batch_size': '128', 'validation_batch_size': '128', 'test_batch_size': '128', 'train_steps_per_epoch': '100', 'validation_steps': '100', 'test_steps': '100', 'use_xla': 'true', 'use_amp': 'true', 'max_seq_length': '64', 'freeze_bert_layer': 'false', 'enable_sagemaker_debugger': 'false', 'enable_checkpointing': 'false', 'enable_tensorboard': 'false', 'run_validation': 'true', 'run_test': 'true', 'run_sample_predictions': 'true', 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/source/sourcedir.tar.gz\"', 'sagemaker_program': '\"tf_bert_reviews.py\"', 'sagemaker_enable_cloudwatch_metrics': 'false', 'sagemaker_container_log_level': '20', 'sagemaker_job_name': '\"training-pipeline-2020-09-14-02-45-45/estimator-source\"', 'sagemaker_region': '\"us-east-1\"', 'model_dir': '\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/model\"'}, 'TrainingJobName': 'training-pipeline-2020-09-14-02-45-45/estimator-source', 'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-835319576252/'}}, 'Create Model': {'ModelName': 'training-pipeline-2020-09-14-02-45-45', 'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1-cpu', 'Environment': {'SAGEMAKER_PROGRAM': None, 'SAGEMAKER_SUBMIT_DIRECTORY': None, 'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'false', 'SAGEMAKER_CONTAINER_LOG_LEVEL': '20', 'SAGEMAKER_REGION': 'us-east-1'}, 'ModelDataUrl': 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/output/model.tar.gz'}, 'ExecutionRoleArn': 'arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881'}, 'Configure Endpoint': {'EndpointConfigName': 'training-pipeline-2020-09-14-02-45-45', 'ProductionVariants': [{'InitialInstanceCount': 1, 'InstanceType': 'ml.m5.4xlarge', 'ModelName': 'training-pipeline-2020-09-14-02-45-45', 'VariantName': 'AllTraffic'}]}, 'Deploy': {'EndpointConfigName': 'training-pipeline-2020-09-14-02-45-45', 'EndpointName': 'training-pipeline-2020-09-14-02-45-45'}}\n",
      "\u001b[32m[INFO] Workflow execution started successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.execute(job_name=None,\n",
    "                             hyperparameters=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:states:us-east-1:835319576252:stateMachine:training-pipeline-2020-09-14-02-45-45\n"
     ]
    }
   ],
   "source": [
    "stepfunction_arn = 'arn:aws:states:{}:{}:stateMachine:{}'.format(region, account_id, pipeline.pipeline_name)\n",
    "print(stepfunction_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'stepfunction_arn' (str)\n"
     ]
    }
   ],
   "source": [
    "%store stepfunction_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-pipeline-2020-09-14-02-45-45\n"
     ]
    }
   ],
   "source": [
    "stepfunction_name = pipeline.pipeline_name\n",
    "print(stepfunction_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'stepfunction_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store stepfunction_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Pipeline Progress\n",
    "_Note: This only renders in Jupyter at the moment - not in JupyterLab.  This is changing soon._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-174\" class=\"workflowgraph\">\n",
       "    \n",
       "    <style>\n",
       "        .graph-legend ul {\n",
       "            list-style-type: none;\n",
       "            padding: 10px;\n",
       "            padding-left: 0;\n",
       "            margin: 0;\n",
       "            position: absolute;\n",
       "            top: 0;\n",
       "            background: transparent;\n",
       "        }\n",
       "\n",
       "        .graph-legend li {\n",
       "            margin-left: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend li > div {\n",
       "            width: 10px;\n",
       "            height: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend .success { background-color: #2BD62E }\n",
       "        .graph-legend .failed { background-color: #DE322F }\n",
       "        .graph-legend .cancelled { background-color: #DDDDDD }\n",
       "        .graph-legend .in-progress { background-color: #53C9ED }\n",
       "        .graph-legend .caught-error { background-color: #FFA500 }\n",
       "    </style>\n",
       "    <div class=\"graph-legend\">\n",
       "        <ul>\n",
       "            <li>\n",
       "                <div class=\"success\"></div>\n",
       "                <span>Success</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"failed\"></div>\n",
       "                <span>Failed</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"cancelled\"></div>\n",
       "                <span>Cancelled</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"in-progress\"></div>\n",
       "                <span>In Progress</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"caught-error\"></div>\n",
       "                <span>Caught Error</span>\n",
       "            </li>\n",
       "        </ul>\n",
       "    </div>\n",
       "\n",
       "    <svg></svg>\n",
       "    <a href=\"https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:835319576252:execution:training-pipeline-2020-09-14-02-45-45:training-pipeline-2020-09-14-02-46-12\" target=\"_blank\"> Inspect in AWS Step Functions </a>\n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-174')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 1000,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"Processing Job\", \"States\": {\"Processing Job\": {\"Resource\": \"arn:aws:states:::sagemaker:createProcessingJob.sync\", \"Parameters\": {\"ProcessingJobName.$\": \"$$.Execution.Input['Processing Job'].ProcessingJobName\", \"ProcessingInputs.$\": \"$$.Execution.Input['Processing Job'].ProcessingInputs\", \"ProcessingOutputConfig.$\": \"$$.Execution.Input['Processing Job'].ProcessingOutputConfig\", \"AppSpecification.$\": \"$$.Execution.Input['Processing Job'].AppSpecification\", \"RoleArn.$\": \"$$.Execution.Input['Processing Job'].RoleArn\", \"ProcessingResources.$\": \"$$.Execution.Input['Processing Job'].ProcessingResources\", \"StoppingCondition.$\": \"$$.Execution.Input['Processing Job'].StoppingCondition\"}, \"Type\": \"Task\", \"Next\": \"Training\"}, \"Training\": {\"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\", \"Parameters\": {\"AlgorithmSpecification.$\": \"$$.Execution.Input['Training'].AlgorithmSpecification\", \"OutputDataConfig.$\": \"$$.Execution.Input['Training'].OutputDataConfig\", \"StoppingCondition.$\": \"$$.Execution.Input['Training'].StoppingCondition\", \"ResourceConfig.$\": \"$$.Execution.Input['Training'].ResourceConfig\", \"RoleArn.$\": \"$$.Execution.Input['Training'].RoleArn\", \"InputDataConfig.$\": \"$$.Execution.Input['Training'].InputDataConfig\", \"HyperParameters.$\": \"$$.Execution.Input['Training'].HyperParameters\", \"TrainingJobName.$\": \"$$.Execution.Input['Training'].TrainingJobName\", \"DebugHookConfig.$\": \"$$.Execution.Input['Training'].DebugHookConfig\"}, \"Type\": \"Task\", \"Next\": \"Create Model\"}, \"Create Model\": {\"Parameters\": {\"ModelName.$\": \"$$.Execution.Input['Create Model'].ModelName\", \"PrimaryContainer.$\": \"$$.Execution.Input['Create Model'].PrimaryContainer\", \"ExecutionRoleArn.$\": \"$$.Execution.Input['Create Model'].ExecutionRoleArn\"}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Configure Endpoint\"}, \"Configure Endpoint\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Configure Endpoint'].EndpointConfigName\", \"ProductionVariants.$\": \"$$.Execution.Input['Configure Endpoint'].ProductionVariants\"}, \"Type\": \"Task\", \"Next\": \"Deploy\"}, \"Deploy\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Deploy'].EndpointConfigName\", \"EndpointName.$\": \"$$.Execution.Input['Deploy'].EndpointName\"}, \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-174';\n",
       "    var events = { 'events': [{\"timestamp\": 1600051572.747, \"type\": \"ExecutionStarted\", \"id\": 1, \"previousEventId\": 0, \"executionStartedEventDetails\": {\"input\": \"{\\n    \\\"Processing Job\\\": {\\n        \\\"ProcessingJobName\\\": \\\"training-pipeline-2020-09-14-02-45-45\\\",\\n        \\\"ProcessingInputs\\\": [\\n            {\\n                \\\"InputName\\\": \\\"raw_input\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/data/\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            },\\n            {\\n                \\\"InputName\\\": \\\"code\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/code\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"FullyReplicated\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            }\\n        ],\\n        \\\"ProcessingOutputConfig\\\": {\\n            \\\"Outputs\\\": [\\n                {\\n                    \\\"OutputName\\\": \\\"bert-train\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/train\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-validation\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/validation\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-test\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/test\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                }\\n            ]\\n        },\\n        \\\"AppSpecification\\\": {\\n            \\\"ImageUri\\\": \\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\n            \\\"ContainerArguments\\\": [\\n                \\\"--train-split-percentage\\\",\\n                \\\"0.9\\\",\\n                \\\"--validation-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--test-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--max-seq-length\\\",\\n                \\\"64\\\",\\n                \\\"--balance-dataset\\\",\\n                \\\"True\\\"\\n            ],\\n            \\\"ContainerEntrypoint\\\": [\\n                \\\"python3\\\",\\n                \\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"\\n            ]\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"ProcessingResources\\\": {\\n            \\\"ClusterConfig\\\": {\\n                \\\"InstanceCount\\\": 2,\\n                \\\"InstanceType\\\": \\\"ml.c5.2xlarge\\\",\\n                \\\"VolumeSizeInGB\\\": 30\\n            }\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 7200\\n        }\\n    },\\n    \\\"Training\\\": {\\n        \\\"AlgorithmSpecification\\\": {\\n            \\\"TrainingImage\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3\\\",\\n            \\\"TrainingInputMode\\\": \\\"Pipe\\\",\\n            \\\"MetricDefinitions\\\": [\\n                {\\n                    \\\"Name\\\": \\\"train:loss\\\",\\n                    \\\"Regex\\\": \\\"loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"train:accuracy\\\",\\n                    \\\"Regex\\\": \\\"accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:loss\\\",\\n                    \\\"Regex\\\": \\\"val_loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:accuracy\\\",\\n                    \\\"Regex\\\": \\\"val_accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                }\\n            ]\\n        },\\n        \\\"OutputDataConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models\\\"\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 86400\\n        },\\n        \\\"ResourceConfig\\\": {\\n            \\\"InstanceCount\\\": 1,\\n            \\\"InstanceType\\\": \\\"ml.c5.9xlarge\\\",\\n            \\\"VolumeSizeInGB\\\": 1024\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"InputDataConfig\\\": [\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"train\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"validation\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"test\\\"\\n            }\\n        ],\\n        \\\"HyperParameters\\\": {\\n            \\\"epochs\\\": \\\"3\\\",\\n            \\\"learning_rate\\\": \\\"1e-05\\\",\\n            \\\"epsilon\\\": \\\"1e-08\\\",\\n            \\\"train_batch_size\\\": \\\"128\\\",\\n            \\\"validation_batch_size\\\": \\\"128\\\",\\n            \\\"test_batch_size\\\": \\\"128\\\",\\n            \\\"train_steps_per_epoch\\\": \\\"100\\\",\\n            \\\"validation_steps\\\": \\\"100\\\",\\n            \\\"test_steps\\\": \\\"100\\\",\\n            \\\"use_xla\\\": \\\"true\\\",\\n            \\\"use_amp\\\": \\\"true\\\",\\n            \\\"max_seq_length\\\": \\\"64\\\",\\n            \\\"freeze_bert_layer\\\": \\\"false\\\",\\n            \\\"enable_sagemaker_debugger\\\": \\\"false\\\",\\n            \\\"enable_checkpointing\\\": \\\"false\\\",\\n            \\\"enable_tensorboard\\\": \\\"false\\\",\\n            \\\"run_validation\\\": \\\"true\\\",\\n            \\\"run_test\\\": \\\"true\\\",\\n            \\\"run_sample_predictions\\\": \\\"true\\\",\\n            \\\"sagemaker_submit_directory\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/source/sourcedir.tar.gz\\\\\\\"\\\",\\n            \\\"sagemaker_program\\\": \\\"\\\\\\\"tf_bert_reviews.py\\\\\\\"\\\",\\n            \\\"sagemaker_enable_cloudwatch_metrics\\\": \\\"false\\\",\\n            \\\"sagemaker_container_log_level\\\": \\\"20\\\",\\n            \\\"sagemaker_job_name\\\": \\\"\\\\\\\"training-pipeline-2020-09-14-02-45-45/estimator-source\\\\\\\"\\\",\\n            \\\"sagemaker_region\\\": \\\"\\\\\\\"us-east-1\\\\\\\"\\\",\\n            \\\"model_dir\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/model\\\\\\\"\\\"\\n        },\\n        \\\"TrainingJobName\\\": \\\"estimator-training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"DebugHookConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/\\\"\\n        }\\n    },\\n    \\\"Create Model\\\": {\\n        \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"PrimaryContainer\\\": {\\n            \\\"Image\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1-cpu\\\",\\n            \\\"Environment\\\": {\\n                \\\"SAGEMAKER_PROGRAM\\\": null,\\n                \\\"SAGEMAKER_SUBMIT_DIRECTORY\\\": null,\\n                \\\"SAGEMAKER_ENABLE_CLOUDWATCH_METRICS\\\": \\\"false\\\",\\n                \\\"SAGEMAKER_CONTAINER_LOG_LEVEL\\\": \\\"20\\\",\\n                \\\"SAGEMAKER_REGION\\\": \\\"us-east-1\\\"\\n            },\\n            \\\"ModelDataUrl\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models/estimator-training-pipeline-2020-09-14-02-46-12/output/model.tar.gz\\\"\\n        },\\n        \\\"ExecutionRoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\"\\n    },\\n    \\\"Configure Endpoint\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"ProductionVariants\\\": [\\n            {\\n                \\\"InitialInstanceCount\\\": 1,\\n                \\\"InstanceType\\\": \\\"ml.m5.4xlarge\\\",\\n                \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n                \\\"VariantName\\\": \\\"AllTraffic\\\"\\n            }\\n        ]\\n    },\\n    \\\"Deploy\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"EndpointName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\"\\n    }\\n}\", \"inputDetails\": {\"truncated\": false}, \"roleArn\": \"arn:aws:iam::835319576252:role/DSOAWS_StepFunctionsExecutionRole\"}}, {\"timestamp\": 1600051572.814, \"type\": \"TaskStateEntered\", \"id\": 2, \"previousEventId\": 0, \"stateEnteredEventDetails\": {\"name\": \"Processing Job\", \"input\": \"{\\n    \\\"Processing Job\\\": {\\n        \\\"ProcessingJobName\\\": \\\"training-pipeline-2020-09-14-02-45-45\\\",\\n        \\\"ProcessingInputs\\\": [\\n            {\\n                \\\"InputName\\\": \\\"raw_input\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/data/\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            },\\n            {\\n                \\\"InputName\\\": \\\"code\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/code\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"FullyReplicated\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            }\\n        ],\\n        \\\"ProcessingOutputConfig\\\": {\\n            \\\"Outputs\\\": [\\n                {\\n                    \\\"OutputName\\\": \\\"bert-train\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/train\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-validation\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/validation\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-test\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/test\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                }\\n            ]\\n        },\\n        \\\"AppSpecification\\\": {\\n            \\\"ImageUri\\\": \\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\n            \\\"ContainerArguments\\\": [\\n                \\\"--train-split-percentage\\\",\\n                \\\"0.9\\\",\\n                \\\"--validation-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--test-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--max-seq-length\\\",\\n                \\\"64\\\",\\n                \\\"--balance-dataset\\\",\\n                \\\"True\\\"\\n            ],\\n            \\\"ContainerEntrypoint\\\": [\\n                \\\"python3\\\",\\n                \\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"\\n            ]\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"ProcessingResources\\\": {\\n            \\\"ClusterConfig\\\": {\\n                \\\"InstanceCount\\\": 2,\\n                \\\"InstanceType\\\": \\\"ml.c5.2xlarge\\\",\\n                \\\"VolumeSizeInGB\\\": 30\\n            }\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 7200\\n        }\\n    },\\n    \\\"Training\\\": {\\n        \\\"AlgorithmSpecification\\\": {\\n            \\\"TrainingImage\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3\\\",\\n            \\\"TrainingInputMode\\\": \\\"Pipe\\\",\\n            \\\"MetricDefinitions\\\": [\\n                {\\n                    \\\"Name\\\": \\\"train:loss\\\",\\n                    \\\"Regex\\\": \\\"loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"train:accuracy\\\",\\n                    \\\"Regex\\\": \\\"accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:loss\\\",\\n                    \\\"Regex\\\": \\\"val_loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:accuracy\\\",\\n                    \\\"Regex\\\": \\\"val_accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                }\\n            ]\\n        },\\n        \\\"OutputDataConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models\\\"\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 86400\\n        },\\n        \\\"ResourceConfig\\\": {\\n            \\\"InstanceCount\\\": 1,\\n            \\\"InstanceType\\\": \\\"ml.c5.9xlarge\\\",\\n            \\\"VolumeSizeInGB\\\": 1024\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"InputDataConfig\\\": [\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"train\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"validation\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"test\\\"\\n            }\\n        ],\\n        \\\"HyperParameters\\\": {\\n            \\\"epochs\\\": \\\"3\\\",\\n            \\\"learning_rate\\\": \\\"1e-05\\\",\\n            \\\"epsilon\\\": \\\"1e-08\\\",\\n            \\\"train_batch_size\\\": \\\"128\\\",\\n            \\\"validation_batch_size\\\": \\\"128\\\",\\n            \\\"test_batch_size\\\": \\\"128\\\",\\n            \\\"train_steps_per_epoch\\\": \\\"100\\\",\\n            \\\"validation_steps\\\": \\\"100\\\",\\n            \\\"test_steps\\\": \\\"100\\\",\\n            \\\"use_xla\\\": \\\"true\\\",\\n            \\\"use_amp\\\": \\\"true\\\",\\n            \\\"max_seq_length\\\": \\\"64\\\",\\n            \\\"freeze_bert_layer\\\": \\\"false\\\",\\n            \\\"enable_sagemaker_debugger\\\": \\\"false\\\",\\n            \\\"enable_checkpointing\\\": \\\"false\\\",\\n            \\\"enable_tensorboard\\\": \\\"false\\\",\\n            \\\"run_validation\\\": \\\"true\\\",\\n            \\\"run_test\\\": \\\"true\\\",\\n            \\\"run_sample_predictions\\\": \\\"true\\\",\\n            \\\"sagemaker_submit_directory\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/source/sourcedir.tar.gz\\\\\\\"\\\",\\n            \\\"sagemaker_program\\\": \\\"\\\\\\\"tf_bert_reviews.py\\\\\\\"\\\",\\n            \\\"sagemaker_enable_cloudwatch_metrics\\\": \\\"false\\\",\\n            \\\"sagemaker_container_log_level\\\": \\\"20\\\",\\n            \\\"sagemaker_job_name\\\": \\\"\\\\\\\"training-pipeline-2020-09-14-02-45-45/estimator-source\\\\\\\"\\\",\\n            \\\"sagemaker_region\\\": \\\"\\\\\\\"us-east-1\\\\\\\"\\\",\\n            \\\"model_dir\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/model\\\\\\\"\\\"\\n        },\\n        \\\"TrainingJobName\\\": \\\"estimator-training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"DebugHookConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/\\\"\\n        }\\n    },\\n    \\\"Create Model\\\": {\\n        \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"PrimaryContainer\\\": {\\n            \\\"Image\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1-cpu\\\",\\n            \\\"Environment\\\": {\\n                \\\"SAGEMAKER_PROGRAM\\\": null,\\n                \\\"SAGEMAKER_SUBMIT_DIRECTORY\\\": null,\\n                \\\"SAGEMAKER_ENABLE_CLOUDWATCH_METRICS\\\": \\\"false\\\",\\n                \\\"SAGEMAKER_CONTAINER_LOG_LEVEL\\\": \\\"20\\\",\\n                \\\"SAGEMAKER_REGION\\\": \\\"us-east-1\\\"\\n            },\\n            \\\"ModelDataUrl\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models/estimator-training-pipeline-2020-09-14-02-46-12/output/model.tar.gz\\\"\\n        },\\n        \\\"ExecutionRoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\"\\n    },\\n    \\\"Configure Endpoint\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"ProductionVariants\\\": [\\n            {\\n                \\\"InitialInstanceCount\\\": 1,\\n                \\\"InstanceType\\\": \\\"ml.m5.4xlarge\\\",\\n                \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n                \\\"VariantName\\\": \\\"AllTraffic\\\"\\n            }\\n        ]\\n    },\\n    \\\"Deploy\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"EndpointName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\"\\n    }\\n}\", \"inputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1600051572.814, \"type\": \"TaskScheduled\", \"id\": 3, \"previousEventId\": 2, \"taskScheduledEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\", \"region\": \"us-east-1\", \"parameters\": \"{\\\"ProcessingResources\\\":{\\\"ClusterConfig\\\":{\\\"InstanceCount\\\":2,\\\"InstanceType\\\":\\\"ml.c5.2xlarge\\\",\\\"VolumeSizeInGB\\\":30}},\\\"StoppingCondition\\\":{\\\"MaxRuntimeInSeconds\\\":7200},\\\"ProcessingInputs\\\":[{\\\"InputName\\\":\\\"raw_input\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/data/\\\",\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3InputMode\\\":\\\"File\\\",\\\"S3DataDistributionType\\\":\\\"ShardedByS3Key\\\",\\\"S3CompressionType\\\":\\\"None\\\"}},{\\\"InputName\\\":\\\"code\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/code\\\",\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3InputMode\\\":\\\"File\\\",\\\"S3DataDistributionType\\\":\\\"FullyReplicated\\\",\\\"S3CompressionType\\\":\\\"None\\\"}}],\\\"ProcessingOutputConfig\\\":{\\\"Outputs\\\":[{\\\"OutputName\\\":\\\"bert-train\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/train\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}},{\\\"OutputName\\\":\\\"bert-validation\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/validation\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}},{\\\"OutputName\\\":\\\"bert-test\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/test\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}}]},\\\"ProcessingJobName\\\":\\\"training-pipeline-2020-09-14-02-45-45\\\",\\\"AppSpecification\\\":{\\\"ImageUri\\\":\\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\\"ContainerArguments\\\":[\\\"--train-split-percentage\\\",\\\"0.9\\\",\\\"--validation-split-percentage\\\",\\\"0.05\\\",\\\"--test-split-percentage\\\",\\\"0.05\\\",\\\"--max-seq-length\\\",\\\"64\\\",\\\"--balance-dataset\\\",\\\"True\\\"],\\\"ContainerEntrypoint\\\":[\\\"python3\\\",\\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"]},\\\"RoleArn\\\":\\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\\"Tags\\\":[{\\\"Key\\\":\\\"MANAGED_BY_AWS\\\",\\\"Value\\\":\\\"STARTED_BY_STEP_FUNCTIONS\\\"}]}\"}}, {\"timestamp\": 1600051572.885, \"type\": \"TaskStarted\", \"id\": 4, \"previousEventId\": 3, \"taskStartedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\"}}, {\"timestamp\": 1600051573.093, \"type\": \"TaskSubmitted\", \"id\": 5, \"previousEventId\": 4, \"taskSubmittedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\", \"output\": \"{\\\"ProcessingJobArn\\\":\\\"arn:aws:sagemaker:us-east-1:835319576252:processing-job/training-pipeline-2020-09-14-02-45-45\\\",\\\"SdkHttpMetadata\\\":{\\\"AllHttpHeaders\\\":{\\\"x-amzn-RequestId\\\":[\\\"2841df1b-d8ec-48e1-9c4f-521c0c6beb87\\\"],\\\"Content-Length\\\":[\\\"116\\\"],\\\"Date\\\":[\\\"Mon, 14 Sep 2020 02:46:12 GMT\\\"],\\\"Content-Type\\\":[\\\"application/x-amz-json-1.1\\\"]},\\\"HttpHeaders\\\":{\\\"Content-Length\\\":\\\"116\\\",\\\"Content-Type\\\":\\\"application/x-amz-json-1.1\\\",\\\"Date\\\":\\\"Mon, 14 Sep 2020 02:46:12 GMT\\\",\\\"x-amzn-RequestId\\\":\\\"2841df1b-d8ec-48e1-9c4f-521c0c6beb87\\\"},\\\"HttpStatusCode\\\":200},\\\"SdkResponseMetadata\\\":{\\\"RequestId\\\":\\\"2841df1b-d8ec-48e1-9c4f-521c0c6beb87\\\"}}\", \"outputDetails\": {\"truncated\": false}}}] };\n",
       "\n",
       "    var graph = new sfn.StateMachineExecutionGraph(definition, events, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should see a graph like this:\n",
    "\n",
    "<img src=\"img/pipeline_executed.png\" width=\"90%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  5\n",
      "Number of events:  11\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "events = execution.list_events()\n",
    "\n",
    "while len(events) <= 5:\n",
    "    print('Number of events:  {}'.format(len(events)))\n",
    "    time.sleep(30)\n",
    "    events = execution.list_events()\n",
    "\n",
    "print('Number of events:  {}'.format(len(events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-344\" class=\"workflowgraph\">\n",
       "    \n",
       "    <style>\n",
       "        .graph-legend ul {\n",
       "            list-style-type: none;\n",
       "            padding: 10px;\n",
       "            padding-left: 0;\n",
       "            margin: 0;\n",
       "            position: absolute;\n",
       "            top: 0;\n",
       "            background: transparent;\n",
       "        }\n",
       "\n",
       "        .graph-legend li {\n",
       "            margin-left: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend li > div {\n",
       "            width: 10px;\n",
       "            height: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend .success { background-color: #2BD62E }\n",
       "        .graph-legend .failed { background-color: #DE322F }\n",
       "        .graph-legend .cancelled { background-color: #DDDDDD }\n",
       "        .graph-legend .in-progress { background-color: #53C9ED }\n",
       "        .graph-legend .caught-error { background-color: #FFA500 }\n",
       "    </style>\n",
       "    <div class=\"graph-legend\">\n",
       "        <ul>\n",
       "            <li>\n",
       "                <div class=\"success\"></div>\n",
       "                <span>Success</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"failed\"></div>\n",
       "                <span>Failed</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"cancelled\"></div>\n",
       "                <span>Cancelled</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"in-progress\"></div>\n",
       "                <span>In Progress</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"caught-error\"></div>\n",
       "                <span>Caught Error</span>\n",
       "            </li>\n",
       "        </ul>\n",
       "    </div>\n",
       "\n",
       "    <svg></svg>\n",
       "    <a href=\"https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:835319576252:execution:training-pipeline-2020-09-14-02-45-45:training-pipeline-2020-09-14-02-46-12\" target=\"_blank\"> Inspect in AWS Step Functions </a>\n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-344')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 1000,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"Processing Job\", \"States\": {\"Processing Job\": {\"Resource\": \"arn:aws:states:::sagemaker:createProcessingJob.sync\", \"Parameters\": {\"ProcessingJobName.$\": \"$$.Execution.Input['Processing Job'].ProcessingJobName\", \"ProcessingInputs.$\": \"$$.Execution.Input['Processing Job'].ProcessingInputs\", \"ProcessingOutputConfig.$\": \"$$.Execution.Input['Processing Job'].ProcessingOutputConfig\", \"AppSpecification.$\": \"$$.Execution.Input['Processing Job'].AppSpecification\", \"RoleArn.$\": \"$$.Execution.Input['Processing Job'].RoleArn\", \"ProcessingResources.$\": \"$$.Execution.Input['Processing Job'].ProcessingResources\", \"StoppingCondition.$\": \"$$.Execution.Input['Processing Job'].StoppingCondition\"}, \"Type\": \"Task\", \"Next\": \"Training\"}, \"Training\": {\"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\", \"Parameters\": {\"AlgorithmSpecification.$\": \"$$.Execution.Input['Training'].AlgorithmSpecification\", \"OutputDataConfig.$\": \"$$.Execution.Input['Training'].OutputDataConfig\", \"StoppingCondition.$\": \"$$.Execution.Input['Training'].StoppingCondition\", \"ResourceConfig.$\": \"$$.Execution.Input['Training'].ResourceConfig\", \"RoleArn.$\": \"$$.Execution.Input['Training'].RoleArn\", \"InputDataConfig.$\": \"$$.Execution.Input['Training'].InputDataConfig\", \"HyperParameters.$\": \"$$.Execution.Input['Training'].HyperParameters\", \"TrainingJobName.$\": \"$$.Execution.Input['Training'].TrainingJobName\", \"DebugHookConfig.$\": \"$$.Execution.Input['Training'].DebugHookConfig\"}, \"Type\": \"Task\", \"Next\": \"Create Model\"}, \"Create Model\": {\"Parameters\": {\"ModelName.$\": \"$$.Execution.Input['Create Model'].ModelName\", \"PrimaryContainer.$\": \"$$.Execution.Input['Create Model'].PrimaryContainer\", \"ExecutionRoleArn.$\": \"$$.Execution.Input['Create Model'].ExecutionRoleArn\"}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Configure Endpoint\"}, \"Configure Endpoint\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Configure Endpoint'].EndpointConfigName\", \"ProductionVariants.$\": \"$$.Execution.Input['Configure Endpoint'].ProductionVariants\"}, \"Type\": \"Task\", \"Next\": \"Deploy\"}, \"Deploy\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Deploy'].EndpointConfigName\", \"EndpointName.$\": \"$$.Execution.Input['Deploy'].EndpointName\"}, \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-344';\n",
       "    var events = { 'events': [{\"timestamp\": 1600051572.747, \"type\": \"ExecutionStarted\", \"id\": 1, \"previousEventId\": 0, \"executionStartedEventDetails\": {\"input\": \"{\\n    \\\"Processing Job\\\": {\\n        \\\"ProcessingJobName\\\": \\\"training-pipeline-2020-09-14-02-45-45\\\",\\n        \\\"ProcessingInputs\\\": [\\n            {\\n                \\\"InputName\\\": \\\"raw_input\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/data/\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            },\\n            {\\n                \\\"InputName\\\": \\\"code\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/code\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"FullyReplicated\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            }\\n        ],\\n        \\\"ProcessingOutputConfig\\\": {\\n            \\\"Outputs\\\": [\\n                {\\n                    \\\"OutputName\\\": \\\"bert-train\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/train\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-validation\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/validation\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-test\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/test\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                }\\n            ]\\n        },\\n        \\\"AppSpecification\\\": {\\n            \\\"ImageUri\\\": \\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\n            \\\"ContainerArguments\\\": [\\n                \\\"--train-split-percentage\\\",\\n                \\\"0.9\\\",\\n                \\\"--validation-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--test-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--max-seq-length\\\",\\n                \\\"64\\\",\\n                \\\"--balance-dataset\\\",\\n                \\\"True\\\"\\n            ],\\n            \\\"ContainerEntrypoint\\\": [\\n                \\\"python3\\\",\\n                \\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"\\n            ]\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"ProcessingResources\\\": {\\n            \\\"ClusterConfig\\\": {\\n                \\\"InstanceCount\\\": 2,\\n                \\\"InstanceType\\\": \\\"ml.c5.2xlarge\\\",\\n                \\\"VolumeSizeInGB\\\": 30\\n            }\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 7200\\n        }\\n    },\\n    \\\"Training\\\": {\\n        \\\"AlgorithmSpecification\\\": {\\n            \\\"TrainingImage\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3\\\",\\n            \\\"TrainingInputMode\\\": \\\"Pipe\\\",\\n            \\\"MetricDefinitions\\\": [\\n                {\\n                    \\\"Name\\\": \\\"train:loss\\\",\\n                    \\\"Regex\\\": \\\"loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"train:accuracy\\\",\\n                    \\\"Regex\\\": \\\"accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:loss\\\",\\n                    \\\"Regex\\\": \\\"val_loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:accuracy\\\",\\n                    \\\"Regex\\\": \\\"val_accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                }\\n            ]\\n        },\\n        \\\"OutputDataConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models\\\"\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 86400\\n        },\\n        \\\"ResourceConfig\\\": {\\n            \\\"InstanceCount\\\": 1,\\n            \\\"InstanceType\\\": \\\"ml.c5.9xlarge\\\",\\n            \\\"VolumeSizeInGB\\\": 1024\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"InputDataConfig\\\": [\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"train\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"validation\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"test\\\"\\n            }\\n        ],\\n        \\\"HyperParameters\\\": {\\n            \\\"epochs\\\": \\\"3\\\",\\n            \\\"learning_rate\\\": \\\"1e-05\\\",\\n            \\\"epsilon\\\": \\\"1e-08\\\",\\n            \\\"train_batch_size\\\": \\\"128\\\",\\n            \\\"validation_batch_size\\\": \\\"128\\\",\\n            \\\"test_batch_size\\\": \\\"128\\\",\\n            \\\"train_steps_per_epoch\\\": \\\"100\\\",\\n            \\\"validation_steps\\\": \\\"100\\\",\\n            \\\"test_steps\\\": \\\"100\\\",\\n            \\\"use_xla\\\": \\\"true\\\",\\n            \\\"use_amp\\\": \\\"true\\\",\\n            \\\"max_seq_length\\\": \\\"64\\\",\\n            \\\"freeze_bert_layer\\\": \\\"false\\\",\\n            \\\"enable_sagemaker_debugger\\\": \\\"false\\\",\\n            \\\"enable_checkpointing\\\": \\\"false\\\",\\n            \\\"enable_tensorboard\\\": \\\"false\\\",\\n            \\\"run_validation\\\": \\\"true\\\",\\n            \\\"run_test\\\": \\\"true\\\",\\n            \\\"run_sample_predictions\\\": \\\"true\\\",\\n            \\\"sagemaker_submit_directory\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/source/sourcedir.tar.gz\\\\\\\"\\\",\\n            \\\"sagemaker_program\\\": \\\"\\\\\\\"tf_bert_reviews.py\\\\\\\"\\\",\\n            \\\"sagemaker_enable_cloudwatch_metrics\\\": \\\"false\\\",\\n            \\\"sagemaker_container_log_level\\\": \\\"20\\\",\\n            \\\"sagemaker_job_name\\\": \\\"\\\\\\\"training-pipeline-2020-09-14-02-45-45/estimator-source\\\\\\\"\\\",\\n            \\\"sagemaker_region\\\": \\\"\\\\\\\"us-east-1\\\\\\\"\\\",\\n            \\\"model_dir\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/model\\\\\\\"\\\"\\n        },\\n        \\\"TrainingJobName\\\": \\\"estimator-training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"DebugHookConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/\\\"\\n        }\\n    },\\n    \\\"Create Model\\\": {\\n        \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"PrimaryContainer\\\": {\\n            \\\"Image\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1-cpu\\\",\\n            \\\"Environment\\\": {\\n                \\\"SAGEMAKER_PROGRAM\\\": null,\\n                \\\"SAGEMAKER_SUBMIT_DIRECTORY\\\": null,\\n                \\\"SAGEMAKER_ENABLE_CLOUDWATCH_METRICS\\\": \\\"false\\\",\\n                \\\"SAGEMAKER_CONTAINER_LOG_LEVEL\\\": \\\"20\\\",\\n                \\\"SAGEMAKER_REGION\\\": \\\"us-east-1\\\"\\n            },\\n            \\\"ModelDataUrl\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models/estimator-training-pipeline-2020-09-14-02-46-12/output/model.tar.gz\\\"\\n        },\\n        \\\"ExecutionRoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\"\\n    },\\n    \\\"Configure Endpoint\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"ProductionVariants\\\": [\\n            {\\n                \\\"InitialInstanceCount\\\": 1,\\n                \\\"InstanceType\\\": \\\"ml.m5.4xlarge\\\",\\n                \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n                \\\"VariantName\\\": \\\"AllTraffic\\\"\\n            }\\n        ]\\n    },\\n    \\\"Deploy\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"EndpointName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\"\\n    }\\n}\", \"inputDetails\": {\"truncated\": false}, \"roleArn\": \"arn:aws:iam::835319576252:role/DSOAWS_StepFunctionsExecutionRole\"}}, {\"timestamp\": 1600051572.814, \"type\": \"TaskStateEntered\", \"id\": 2, \"previousEventId\": 0, \"stateEnteredEventDetails\": {\"name\": \"Processing Job\", \"input\": \"{\\n    \\\"Processing Job\\\": {\\n        \\\"ProcessingJobName\\\": \\\"training-pipeline-2020-09-14-02-45-45\\\",\\n        \\\"ProcessingInputs\\\": [\\n            {\\n                \\\"InputName\\\": \\\"raw_input\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/data/\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            },\\n            {\\n                \\\"InputName\\\": \\\"code\\\",\\n                \\\"S3Input\\\": {\\n                    \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\n                    \\\"LocalPath\\\": \\\"/opt/ml/processing/input/code\\\",\\n                    \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                    \\\"S3InputMode\\\": \\\"File\\\",\\n                    \\\"S3DataDistributionType\\\": \\\"FullyReplicated\\\",\\n                    \\\"S3CompressionType\\\": \\\"None\\\"\\n                }\\n            }\\n        ],\\n        \\\"ProcessingOutputConfig\\\": {\\n            \\\"Outputs\\\": [\\n                {\\n                    \\\"OutputName\\\": \\\"bert-train\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/train\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-validation\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/validation\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                },\\n                {\\n                    \\\"OutputName\\\": \\\"bert-test\\\",\\n                    \\\"S3Output\\\": {\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"LocalPath\\\": \\\"/opt/ml/processing/output/bert/test\\\",\\n                        \\\"S3UploadMode\\\": \\\"EndOfJob\\\"\\n                    }\\n                }\\n            ]\\n        },\\n        \\\"AppSpecification\\\": {\\n            \\\"ImageUri\\\": \\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\n            \\\"ContainerArguments\\\": [\\n                \\\"--train-split-percentage\\\",\\n                \\\"0.9\\\",\\n                \\\"--validation-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--test-split-percentage\\\",\\n                \\\"0.05\\\",\\n                \\\"--max-seq-length\\\",\\n                \\\"64\\\",\\n                \\\"--balance-dataset\\\",\\n                \\\"True\\\"\\n            ],\\n            \\\"ContainerEntrypoint\\\": [\\n                \\\"python3\\\",\\n                \\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"\\n            ]\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"ProcessingResources\\\": {\\n            \\\"ClusterConfig\\\": {\\n                \\\"InstanceCount\\\": 2,\\n                \\\"InstanceType\\\": \\\"ml.c5.2xlarge\\\",\\n                \\\"VolumeSizeInGB\\\": 30\\n            }\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 7200\\n        }\\n    },\\n    \\\"Training\\\": {\\n        \\\"AlgorithmSpecification\\\": {\\n            \\\"TrainingImage\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3\\\",\\n            \\\"TrainingInputMode\\\": \\\"Pipe\\\",\\n            \\\"MetricDefinitions\\\": [\\n                {\\n                    \\\"Name\\\": \\\"train:loss\\\",\\n                    \\\"Regex\\\": \\\"loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"train:accuracy\\\",\\n                    \\\"Regex\\\": \\\"accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:loss\\\",\\n                    \\\"Regex\\\": \\\"val_loss: ([0-9\\\\\\\\.]+)\\\"\\n                },\\n                {\\n                    \\\"Name\\\": \\\"validation:accuracy\\\",\\n                    \\\"Regex\\\": \\\"val_accuracy: ([0-9\\\\\\\\.]+)\\\"\\n                }\\n            ]\\n        },\\n        \\\"OutputDataConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models\\\"\\n        },\\n        \\\"StoppingCondition\\\": {\\n            \\\"MaxRuntimeInSeconds\\\": 86400\\n        },\\n        \\\"ResourceConfig\\\": {\\n            \\\"InstanceCount\\\": 1,\\n            \\\"InstanceType\\\": \\\"ml.c5.9xlarge\\\",\\n            \\\"VolumeSizeInGB\\\": 1024\\n        },\\n        \\\"RoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\n        \\\"InputDataConfig\\\": [\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"train\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"validation\\\"\\n            },\\n            {\\n                \\\"DataSource\\\": {\\n                    \\\"S3DataSource\\\": {\\n                        \\\"S3DataType\\\": \\\"S3Prefix\\\",\\n                        \\\"S3Uri\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\n                        \\\"S3DataDistributionType\\\": \\\"ShardedByS3Key\\\"\\n                    }\\n                },\\n                \\\"ChannelName\\\": \\\"test\\\"\\n            }\\n        ],\\n        \\\"HyperParameters\\\": {\\n            \\\"epochs\\\": \\\"3\\\",\\n            \\\"learning_rate\\\": \\\"1e-05\\\",\\n            \\\"epsilon\\\": \\\"1e-08\\\",\\n            \\\"train_batch_size\\\": \\\"128\\\",\\n            \\\"validation_batch_size\\\": \\\"128\\\",\\n            \\\"test_batch_size\\\": \\\"128\\\",\\n            \\\"train_steps_per_epoch\\\": \\\"100\\\",\\n            \\\"validation_steps\\\": \\\"100\\\",\\n            \\\"test_steps\\\": \\\"100\\\",\\n            \\\"use_xla\\\": \\\"true\\\",\\n            \\\"use_amp\\\": \\\"true\\\",\\n            \\\"max_seq_length\\\": \\\"64\\\",\\n            \\\"freeze_bert_layer\\\": \\\"false\\\",\\n            \\\"enable_sagemaker_debugger\\\": \\\"false\\\",\\n            \\\"enable_checkpointing\\\": \\\"false\\\",\\n            \\\"enable_tensorboard\\\": \\\"false\\\",\\n            \\\"run_validation\\\": \\\"true\\\",\\n            \\\"run_test\\\": \\\"true\\\",\\n            \\\"run_sample_predictions\\\": \\\"true\\\",\\n            \\\"sagemaker_submit_directory\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/source/sourcedir.tar.gz\\\\\\\"\\\",\\n            \\\"sagemaker_program\\\": \\\"\\\\\\\"tf_bert_reviews.py\\\\\\\"\\\",\\n            \\\"sagemaker_enable_cloudwatch_metrics\\\": \\\"false\\\",\\n            \\\"sagemaker_container_log_level\\\": \\\"20\\\",\\n            \\\"sagemaker_job_name\\\": \\\"\\\\\\\"training-pipeline-2020-09-14-02-45-45/estimator-source\\\\\\\"\\\",\\n            \\\"sagemaker_region\\\": \\\"\\\\\\\"us-east-1\\\\\\\"\\\",\\n            \\\"model_dir\\\": \\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/model\\\\\\\"\\\"\\n        },\\n        \\\"TrainingJobName\\\": \\\"estimator-training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"DebugHookConfig\\\": {\\n            \\\"S3OutputPath\\\": \\\"s3://sagemaker-us-east-1-835319576252/\\\"\\n        }\\n    },\\n    \\\"Create Model\\\": {\\n        \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"PrimaryContainer\\\": {\\n            \\\"Image\\\": \\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1-cpu\\\",\\n            \\\"Environment\\\": {\\n                \\\"SAGEMAKER_PROGRAM\\\": null,\\n                \\\"SAGEMAKER_SUBMIT_DIRECTORY\\\": null,\\n                \\\"SAGEMAKER_ENABLE_CLOUDWATCH_METRICS\\\": \\\"false\\\",\\n                \\\"SAGEMAKER_CONTAINER_LOG_LEVEL\\\": \\\"20\\\",\\n                \\\"SAGEMAKER_REGION\\\": \\\"us-east-1\\\"\\n            },\\n            \\\"ModelDataUrl\\\": \\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models/estimator-training-pipeline-2020-09-14-02-46-12/output/model.tar.gz\\\"\\n        },\\n        \\\"ExecutionRoleArn\\\": \\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\"\\n    },\\n    \\\"Configure Endpoint\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"ProductionVariants\\\": [\\n            {\\n                \\\"InitialInstanceCount\\\": 1,\\n                \\\"InstanceType\\\": \\\"ml.m5.4xlarge\\\",\\n                \\\"ModelName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n                \\\"VariantName\\\": \\\"AllTraffic\\\"\\n            }\\n        ]\\n    },\\n    \\\"Deploy\\\": {\\n        \\\"EndpointConfigName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\",\\n        \\\"EndpointName\\\": \\\"training-pipeline-2020-09-14-02-46-12\\\"\\n    }\\n}\", \"inputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1600051572.814, \"type\": \"TaskScheduled\", \"id\": 3, \"previousEventId\": 2, \"taskScheduledEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\", \"region\": \"us-east-1\", \"parameters\": \"{\\\"ProcessingResources\\\":{\\\"ClusterConfig\\\":{\\\"InstanceCount\\\":2,\\\"InstanceType\\\":\\\"ml.c5.2xlarge\\\",\\\"VolumeSizeInGB\\\":30}},\\\"StoppingCondition\\\":{\\\"MaxRuntimeInSeconds\\\":7200},\\\"ProcessingInputs\\\":[{\\\"InputName\\\":\\\"raw_input\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/data/\\\",\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3InputMode\\\":\\\"File\\\",\\\"S3DataDistributionType\\\":\\\"ShardedByS3Key\\\",\\\"S3CompressionType\\\":\\\"None\\\"}},{\\\"InputName\\\":\\\"code\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/code\\\",\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3InputMode\\\":\\\"File\\\",\\\"S3DataDistributionType\\\":\\\"FullyReplicated\\\",\\\"S3CompressionType\\\":\\\"None\\\"}}],\\\"ProcessingOutputConfig\\\":{\\\"Outputs\\\":[{\\\"OutputName\\\":\\\"bert-train\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/train\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}},{\\\"OutputName\\\":\\\"bert-validation\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/validation\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}},{\\\"OutputName\\\":\\\"bert-test\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/test\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}}]},\\\"ProcessingJobName\\\":\\\"training-pipeline-2020-09-14-02-45-45\\\",\\\"AppSpecification\\\":{\\\"ImageUri\\\":\\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\\"ContainerArguments\\\":[\\\"--train-split-percentage\\\",\\\"0.9\\\",\\\"--validation-split-percentage\\\",\\\"0.05\\\",\\\"--test-split-percentage\\\",\\\"0.05\\\",\\\"--max-seq-length\\\",\\\"64\\\",\\\"--balance-dataset\\\",\\\"True\\\"],\\\"ContainerEntrypoint\\\":[\\\"python3\\\",\\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"]},\\\"RoleArn\\\":\\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\\"Tags\\\":[{\\\"Key\\\":\\\"MANAGED_BY_AWS\\\",\\\"Value\\\":\\\"STARTED_BY_STEP_FUNCTIONS\\\"}]}\"}}, {\"timestamp\": 1600051572.885, \"type\": \"TaskStarted\", \"id\": 4, \"previousEventId\": 3, \"taskStartedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\"}}, {\"timestamp\": 1600051573.093, \"type\": \"TaskSubmitted\", \"id\": 5, \"previousEventId\": 4, \"taskSubmittedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\", \"output\": \"{\\\"ProcessingJobArn\\\":\\\"arn:aws:sagemaker:us-east-1:835319576252:processing-job/training-pipeline-2020-09-14-02-45-45\\\",\\\"SdkHttpMetadata\\\":{\\\"AllHttpHeaders\\\":{\\\"x-amzn-RequestId\\\":[\\\"2841df1b-d8ec-48e1-9c4f-521c0c6beb87\\\"],\\\"Content-Length\\\":[\\\"116\\\"],\\\"Date\\\":[\\\"Mon, 14 Sep 2020 02:46:12 GMT\\\"],\\\"Content-Type\\\":[\\\"application/x-amz-json-1.1\\\"]},\\\"HttpHeaders\\\":{\\\"Content-Length\\\":\\\"116\\\",\\\"Content-Type\\\":\\\"application/x-amz-json-1.1\\\",\\\"Date\\\":\\\"Mon, 14 Sep 2020 02:46:12 GMT\\\",\\\"x-amzn-RequestId\\\":\\\"2841df1b-d8ec-48e1-9c4f-521c0c6beb87\\\"},\\\"HttpStatusCode\\\":200},\\\"SdkResponseMetadata\\\":{\\\"RequestId\\\":\\\"2841df1b-d8ec-48e1-9c4f-521c0c6beb87\\\"}}\", \"outputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1600051991.513, \"type\": \"TaskSucceeded\", \"id\": 6, \"previousEventId\": 5, \"taskSucceededEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\", \"output\": \"{\\\"ProcessingInputs\\\":[{\\\"InputName\\\":\\\"raw_input\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/data/\\\",\\\"S3DataType\\\":\\\"S3_PREFIX\\\",\\\"S3InputMode\\\":\\\"FILE\\\",\\\"S3DataDistributionType\\\":\\\"SHARDEDBYS3KEY\\\"}},{\\\"InputName\\\":\\\"code\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/code\\\",\\\"S3DataType\\\":\\\"S3_PREFIX\\\",\\\"S3InputMode\\\":\\\"FILE\\\",\\\"S3DataDistributionType\\\":\\\"FULLYREPLICATED\\\"}}],\\\"ProcessingOutputConfig\\\":{\\\"Outputs\\\":[{\\\"OutputName\\\":\\\"bert-train\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/train\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}},{\\\"OutputName\\\":\\\"bert-validation\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/validation\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}},{\\\"OutputName\\\":\\\"bert-test\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/test\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}}]},\\\"ProcessingJobName\\\":\\\"training-pipeline-2020-09-14-02-45-45\\\",\\\"ProcessingResources\\\":{\\\"ClusterConfig\\\":{\\\"InstanceCount\\\":2.0,\\\"InstanceType\\\":\\\"ml.c5.2xlarge\\\",\\\"VolumeSizeInGB\\\":30.0}},\\\"StoppingCondition\\\":{\\\"MaxRuntimeInSeconds\\\":7200.0},\\\"AppSpecification\\\":{\\\"ImageUri\\\":\\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\\"ContainerEntrypoint\\\":[\\\"python3\\\",\\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"],\\\"ContainerArguments\\\":[\\\"--train-split-percentage\\\",\\\"0.9\\\",\\\"--validation-split-percentage\\\",\\\"0.05\\\",\\\"--test-split-percentage\\\",\\\"0.05\\\",\\\"--max-seq-length\\\",\\\"64\\\",\\\"--balance-dataset\\\",\\\"True\\\"]},\\\"RoleArn\\\":\\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\\"ExperimentConfig\\\":{},\\\"ProcessingJobArn\\\":\\\"arn:aws:sagemaker:us-east-1:835319576252:processing-job/training-pipeline-2020-09-14-02-45-45\\\",\\\"ProcessingJobStatus\\\":\\\"Completed\\\",\\\"ProcessingEndTime\\\":1.600051987E12,\\\"ProcessingStartTime\\\":1.600051795E12,\\\"LastModifiedTime\\\":1.600051987E12,\\\"CreationTime\\\":1.600051573E12,\\\"Tags\\\":{\\\"AWS_STEP_FUNCTIONS_EXECUTION_ARN\\\":\\\"arn:aws:states:us-east-1:835319576252:execution:training-pipeline-2020-09-14-02-45-45:training-pipeline-2020-09-14-02-46-12\\\",\\\"MANAGED_BY_AWS\\\":\\\"STARTED_BY_STEP_FUNCTIONS\\\"}}\", \"outputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1600051991.513, \"type\": \"TaskStateExited\", \"id\": 7, \"previousEventId\": 6, \"stateExitedEventDetails\": {\"name\": \"Processing Job\", \"output\": \"{\\\"ProcessingInputs\\\":[{\\\"InputName\\\":\\\"raw_input\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/data/\\\",\\\"S3DataType\\\":\\\"S3_PREFIX\\\",\\\"S3InputMode\\\":\\\"FILE\\\",\\\"S3DataDistributionType\\\":\\\"SHARDEDBYS3KEY\\\"}},{\\\"InputName\\\":\\\"code\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/code\\\",\\\"S3DataType\\\":\\\"S3_PREFIX\\\",\\\"S3InputMode\\\":\\\"FILE\\\",\\\"S3DataDistributionType\\\":\\\"FULLYREPLICATED\\\"}}],\\\"ProcessingOutputConfig\\\":{\\\"Outputs\\\":[{\\\"OutputName\\\":\\\"bert-train\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/train\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}},{\\\"OutputName\\\":\\\"bert-validation\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/validation\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}},{\\\"OutputName\\\":\\\"bert-test\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/test\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}}]},\\\"ProcessingJobName\\\":\\\"training-pipeline-2020-09-14-02-45-45\\\",\\\"ProcessingResources\\\":{\\\"ClusterConfig\\\":{\\\"InstanceCount\\\":2.0,\\\"InstanceType\\\":\\\"ml.c5.2xlarge\\\",\\\"VolumeSizeInGB\\\":30.0}},\\\"StoppingCondition\\\":{\\\"MaxRuntimeInSeconds\\\":7200.0},\\\"AppSpecification\\\":{\\\"ImageUri\\\":\\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\\"ContainerEntrypoint\\\":[\\\"python3\\\",\\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"],\\\"ContainerArguments\\\":[\\\"--train-split-percentage\\\",\\\"0.9\\\",\\\"--validation-split-percentage\\\",\\\"0.05\\\",\\\"--test-split-percentage\\\",\\\"0.05\\\",\\\"--max-seq-length\\\",\\\"64\\\",\\\"--balance-dataset\\\",\\\"True\\\"]},\\\"RoleArn\\\":\\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\\"ExperimentConfig\\\":{},\\\"ProcessingJobArn\\\":\\\"arn:aws:sagemaker:us-east-1:835319576252:processing-job/training-pipeline-2020-09-14-02-45-45\\\",\\\"ProcessingJobStatus\\\":\\\"Completed\\\",\\\"ProcessingEndTime\\\":1.600051987E12,\\\"ProcessingStartTime\\\":1.600051795E12,\\\"LastModifiedTime\\\":1.600051987E12,\\\"CreationTime\\\":1.600051573E12,\\\"Tags\\\":{\\\"AWS_STEP_FUNCTIONS_EXECUTION_ARN\\\":\\\"arn:aws:states:us-east-1:835319576252:execution:training-pipeline-2020-09-14-02-45-45:training-pipeline-2020-09-14-02-46-12\\\",\\\"MANAGED_BY_AWS\\\":\\\"STARTED_BY_STEP_FUNCTIONS\\\"}}\", \"outputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1600051991.522, \"type\": \"TaskStateEntered\", \"id\": 8, \"previousEventId\": 7, \"stateEnteredEventDetails\": {\"name\": \"Training\", \"input\": \"{\\\"ProcessingInputs\\\":[{\\\"InputName\\\":\\\"raw_input\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/data/\\\",\\\"S3DataType\\\":\\\"S3_PREFIX\\\",\\\"S3InputMode\\\":\\\"FILE\\\",\\\"S3DataDistributionType\\\":\\\"SHARDEDBYS3KEY\\\"}},{\\\"InputName\\\":\\\"code\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/pipeline_sklearn_processing/1600051223/code/preprocess-scikit-text-to-bert.py\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/code\\\",\\\"S3DataType\\\":\\\"S3_PREFIX\\\",\\\"S3InputMode\\\":\\\"FILE\\\",\\\"S3DataDistributionType\\\":\\\"FULLYREPLICATED\\\"}}],\\\"ProcessingOutputConfig\\\":{\\\"Outputs\\\":[{\\\"OutputName\\\":\\\"bert-train\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/train\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}},{\\\"OutputName\\\":\\\"bert-validation\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/validation\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}},{\\\"OutputName\\\":\\\"bert-test\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/bert/test\\\",\\\"S3UploadMode\\\":\\\"END_OF_JOB\\\"}}]},\\\"ProcessingJobName\\\":\\\"training-pipeline-2020-09-14-02-45-45\\\",\\\"ProcessingResources\\\":{\\\"ClusterConfig\\\":{\\\"InstanceCount\\\":2.0,\\\"InstanceType\\\":\\\"ml.c5.2xlarge\\\",\\\"VolumeSizeInGB\\\":30.0}},\\\"StoppingCondition\\\":{\\\"MaxRuntimeInSeconds\\\":7200.0},\\\"AppSpecification\\\":{\\\"ImageUri\\\":\\\"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\\\",\\\"ContainerEntrypoint\\\":[\\\"python3\\\",\\\"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py\\\"],\\\"ContainerArguments\\\":[\\\"--train-split-percentage\\\",\\\"0.9\\\",\\\"--validation-split-percentage\\\",\\\"0.05\\\",\\\"--test-split-percentage\\\",\\\"0.05\\\",\\\"--max-seq-length\\\",\\\"64\\\",\\\"--balance-dataset\\\",\\\"True\\\"]},\\\"RoleArn\\\":\\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\\"ExperimentConfig\\\":{},\\\"ProcessingJobArn\\\":\\\"arn:aws:sagemaker:us-east-1:835319576252:processing-job/training-pipeline-2020-09-14-02-45-45\\\",\\\"ProcessingJobStatus\\\":\\\"Completed\\\",\\\"ProcessingEndTime\\\":1.600051987E12,\\\"ProcessingStartTime\\\":1.600051795E12,\\\"LastModifiedTime\\\":1.600051987E12,\\\"CreationTime\\\":1.600051573E12,\\\"Tags\\\":{\\\"AWS_STEP_FUNCTIONS_EXECUTION_ARN\\\":\\\"arn:aws:states:us-east-1:835319576252:execution:training-pipeline-2020-09-14-02-45-45:training-pipeline-2020-09-14-02-46-12\\\",\\\"MANAGED_BY_AWS\\\":\\\"STARTED_BY_STEP_FUNCTIONS\\\"}}\", \"inputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1600051991.522, \"type\": \"TaskScheduled\", \"id\": 9, \"previousEventId\": 8, \"taskScheduledEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createTrainingJob.sync\", \"region\": \"us-east-1\", \"parameters\": \"{\\\"HyperParameters\\\":{\\\"epochs\\\":\\\"3\\\",\\\"learning_rate\\\":\\\"1e-05\\\",\\\"epsilon\\\":\\\"1e-08\\\",\\\"train_batch_size\\\":\\\"128\\\",\\\"validation_batch_size\\\":\\\"128\\\",\\\"test_batch_size\\\":\\\"128\\\",\\\"train_steps_per_epoch\\\":\\\"100\\\",\\\"validation_steps\\\":\\\"100\\\",\\\"test_steps\\\":\\\"100\\\",\\\"use_xla\\\":\\\"true\\\",\\\"use_amp\\\":\\\"true\\\",\\\"max_seq_length\\\":\\\"64\\\",\\\"freeze_bert_layer\\\":\\\"false\\\",\\\"enable_sagemaker_debugger\\\":\\\"false\\\",\\\"enable_checkpointing\\\":\\\"false\\\",\\\"enable_tensorboard\\\":\\\"false\\\",\\\"run_validation\\\":\\\"true\\\",\\\"run_test\\\":\\\"true\\\",\\\"run_sample_predictions\\\":\\\"true\\\",\\\"sagemaker_submit_directory\\\":\\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/source/sourcedir.tar.gz\\\\\\\"\\\",\\\"sagemaker_program\\\":\\\"\\\\\\\"tf_bert_reviews.py\\\\\\\"\\\",\\\"sagemaker_enable_cloudwatch_metrics\\\":\\\"false\\\",\\\"sagemaker_container_log_level\\\":\\\"20\\\",\\\"sagemaker_job_name\\\":\\\"\\\\\\\"training-pipeline-2020-09-14-02-45-45/estimator-source\\\\\\\"\\\",\\\"sagemaker_region\\\":\\\"\\\\\\\"us-east-1\\\\\\\"\\\",\\\"model_dir\\\":\\\"\\\\\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/estimator-source/model\\\\\\\"\\\"},\\\"DebugHookConfig\\\":{\\\"S3OutputPath\\\":\\\"s3://sagemaker-us-east-1-835319576252/\\\"},\\\"AlgorithmSpecification\\\":{\\\"TrainingImage\\\":\\\"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-cpu-py3\\\",\\\"TrainingInputMode\\\":\\\"Pipe\\\",\\\"MetricDefinitions\\\":[{\\\"Name\\\":\\\"train:loss\\\",\\\"Regex\\\":\\\"loss: ([0-9\\\\\\\\.]+)\\\"},{\\\"Name\\\":\\\"train:accuracy\\\",\\\"Regex\\\":\\\"accuracy: ([0-9\\\\\\\\.]+)\\\"},{\\\"Name\\\":\\\"validation:loss\\\",\\\"Regex\\\":\\\"val_loss: ([0-9\\\\\\\\.]+)\\\"},{\\\"Name\\\":\\\"validation:accuracy\\\",\\\"Regex\\\":\\\"val_accuracy: ([0-9\\\\\\\\.]+)\\\"}]},\\\"StoppingCondition\\\":{\\\"MaxRuntimeInSeconds\\\":86400},\\\"TrainingJobName\\\":\\\"estimator-training-pipeline-2020-09-14-02-46-12\\\",\\\"OutputDataConfig\\\":{\\\"S3OutputPath\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/models\\\"},\\\"ResourceConfig\\\":{\\\"InstanceCount\\\":1,\\\"InstanceType\\\":\\\"ml.c5.9xlarge\\\",\\\"VolumeSizeInGB\\\":1024},\\\"InputDataConfig\\\":[{\\\"DataSource\\\":{\\\"S3DataSource\\\":{\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\\\",\\\"S3DataDistributionType\\\":\\\"ShardedByS3Key\\\"}},\\\"ChannelName\\\":\\\"train\\\"},{\\\"DataSource\\\":{\\\"S3DataSource\\\":{\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\\\",\\\"S3DataDistributionType\\\":\\\"ShardedByS3Key\\\"}},\\\"ChannelName\\\":\\\"validation\\\"},{\\\"DataSource\\\":{\\\"S3DataSource\\\":{\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3Uri\\\":\\\"s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\\\",\\\"S3DataDistributionType\\\":\\\"ShardedByS3Key\\\"}},\\\"ChannelName\\\":\\\"test\\\"}],\\\"RoleArn\\\":\\\"arn:aws:iam::835319576252:role/service-role/AmazonSageMaker-ExecutionRole-20191006T135881\\\",\\\"Tags\\\":[{\\\"Key\\\":\\\"MANAGED_BY_AWS\\\",\\\"Value\\\":\\\"STARTED_BY_STEP_FUNCTIONS\\\"}]}\"}}, {\"timestamp\": 1600051991.575, \"type\": \"TaskStarted\", \"id\": 10, \"previousEventId\": 9, \"taskStartedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createTrainingJob.sync\"}}, {\"timestamp\": 1600051991.817, \"type\": \"TaskSubmitted\", \"id\": 11, \"previousEventId\": 10, \"taskSubmittedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createTrainingJob.sync\", \"output\": \"{\\\"SdkHttpMetadata\\\":{\\\"AllHttpHeaders\\\":{\\\"x-amzn-RequestId\\\":[\\\"655a9b2f-6951-45f1-a4a6-5a93e4112a00\\\"],\\\"Content-Length\\\":[\\\"122\\\"],\\\"Date\\\":[\\\"Mon, 14 Sep 2020 02:53:11 GMT\\\"],\\\"Content-Type\\\":[\\\"application/x-amz-json-1.1\\\"]},\\\"HttpHeaders\\\":{\\\"Content-Length\\\":\\\"122\\\",\\\"Content-Type\\\":\\\"application/x-amz-json-1.1\\\",\\\"Date\\\":\\\"Mon, 14 Sep 2020 02:53:11 GMT\\\",\\\"x-amzn-RequestId\\\":\\\"655a9b2f-6951-45f1-a4a6-5a93e4112a00\\\"},\\\"HttpStatusCode\\\":200},\\\"SdkResponseMetadata\\\":{\\\"RequestId\\\":\\\"655a9b2f-6951-45f1-a4a6-5a93e4112a00\\\"},\\\"TrainingJobArn\\\":\\\"arn:aws:sagemaker:us-east-1:835319576252:training-job/estimator-training-pipeline-2020-09-14-02-46-12\\\"}\", \"outputDetails\": {\"truncated\": false}}}] };\n",
       "\n",
       "    var graph = new sfn.StateMachineExecutionGraph(definition, events, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait for ^^ Number of Events ^^ to Reach At Least 6_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Job Name: training-pipeline-2020-09-14-02-45-45\n",
      "\n",
      "Processed Data Bert Train S3 URI: s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train\n",
      "Processed Data Bert Validation S3 URI: s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation\n",
      "Processed Data Bert Test S3 URI: s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "processing_job_name = json.loads(events[5]['taskSucceededEventDetails']['output'])['ProcessingJobName']\n",
    "print('Processing Job Name: {}'.format(processing_job_name))\n",
    "\n",
    "print('')\n",
    "\n",
    "processing_job_outputs = json.loads(events[5]['taskSucceededEventDetails']['output'])['ProcessingOutputConfig']['Outputs']\n",
    "\n",
    "for output in processing_job_outputs:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "\n",
    "print('Processed Data Bert Train S3 URI: {}'.format(train_data_s3_uri))\n",
    "print('Processed Data Bert Validation S3 URI: {}'.format(validation_data_s3_uri))\n",
    "print('Processed Data Bert Test S3 URI: {}'.format(test_data_s3_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train/part-algo-1-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord', 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train/part-algo-1-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord', 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-train/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord']\n",
      "['s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation/part-algo-1-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord', 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation/part-algo-1-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord', 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-validation/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord']\n",
      "['s3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test/part-algo-1-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord', 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test/part-algo-1-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord', 's3://sagemaker-us-east-1-835319576252/training-pipeline-2020-09-14-02-45-45/processing/output/bert-test/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "print(S3Downloader.list(train_data_s3_uri))\n",
    "\n",
    "from sagemaker.s3 import S3Downloader\n",
    "print(S3Downloader.list(validation_data_s3_uri))\n",
    "\n",
    "from sagemaker.s3 import S3Downloader\n",
    "print(S3Downloader.list(test_data_s3_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n",
      "Number of events:  11\n"
     ]
    }
   ],
   "source": [
    "events = execution.list_events()\n",
    "\n",
    "while len(events) <= 11:\n",
    "    print('Number of events:  {}'.format(len(events)))\n",
    "    time.sleep(30)\n",
    "    events = execution.list_events()\n",
    "\n",
    "print('Number of events:  {}'.format(len(events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_job_name = json.loads(events[11]['taskSucceededEventDetails']['output'])['TrainingJobName']\n",
    "print('Training Job Name: {}'.format(training_job_name))\n",
    "\n",
    "print('')\n",
    "\n",
    "trained_model_s3_uri = json.loads(events[11]['taskSucceededEventDetails']['output'])['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Trained Model S3 URI: {}'.format(trained_model_s3_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a></b>'.format(region, training_job_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $trained_model_s3_uri ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./model/\n",
    "!tar -xvzf ./model.tar.gz -C ./model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Model Prediction Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir ./model/tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = execution.list_events()\n",
    "\n",
    "while len(events) <= 24:\n",
    "    print('Number of events:  {}'.format(len(events)))\n",
    "    time.sleep(30)\n",
    "    events = execution.list_events()\n",
    "\n",
    "print('Number of events:  {}'.format(len(events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait for ^^ Number of Events ^^ to Reach At Least 19_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step_functions_pipeline_endpoint_name = json.loads(events[24]['taskScheduledEventDetails']['parameters'])['EndpointName']\n",
    "\n",
    "print('Endpoint Name: {}'.format(step_functions_pipeline_endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = execution.list_events()\n",
    "\n",
    "while len(events) <= 27:\n",
    "    print('Number of events:  {}'.format(len(events)))\n",
    "    time.sleep(30)\n",
    "    events = execution.list_events()    \n",
    "\n",
    "print('Number of events:  {}'.format(len(events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait for ^^ Number of Events ^^ to Reach At Least 22_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_functions_pipeline_endpoint_arn = json.loads(events[27]['stateExitedEventDetails']['output'])['EndpointArn']\n",
    "\n",
    "print('Endpoint ARN: {}'.format(step_functions_pipeline_endpoint_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST Endpoint</a></b>'.format(region, step_functions_pipeline_endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebooks(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(step_functions_pipeline_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store step_functions_pipeline_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Remove this once StepFunctions SDK supports SageMaker 2.5.5\n",
    "!pip install sagemaker==2.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
