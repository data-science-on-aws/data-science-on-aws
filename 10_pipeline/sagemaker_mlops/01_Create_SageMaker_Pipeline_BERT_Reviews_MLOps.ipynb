{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker MLOps Project for Pipelines\n",
    "Automatically run pipelines when code changes using Amazon SageMaker Projects\n",
    "\n",
    "Note:  This requires that you have enabled products within SageMaker Studio\n",
    "\n",
    "![](../img/enable-service-catalog-portfolio-for-studio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "sc = boto3.Session().client(service_name=\"servicecatalog\", region_name=region)\n",
    "sts = boto3.Session().client(service_name=\"sts\", region_name=region)\n",
    "iam = boto3.Session().client(service_name=\"iam\", region_name=region)\n",
    "codepipeline = boto3.Session().client(\"codepipeline\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod-j3ufw6hl7utxm\n"
     ]
    }
   ],
   "source": [
    "search_response = sc.search_products(\n",
    "    Filters={\"FullTextSearch\": [\"MLOps template for model building, training, and deployment\"]}\n",
    ")\n",
    "\n",
    "sagemaker_pipeline_product_id = search_response[\"ProductViewSummaries\"][0][\"ProductId\"]\n",
    "print(sagemaker_pipeline_product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response = sc.describe_product(Id=sagemaker_pipeline_product_id)\n",
    "\n",
    "sagemaker_pipeline_product_provisioning_artifact_id = describe_response[\"ProvisioningArtifacts\"][0][\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa-oacphmo7m2bji\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker_pipeline_product_provisioning_artifact_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID p-nynqfqk1cz5v\n",
      "Project ARN arn:aws:sagemaker:us-east-1:231218423789:project/dsoaws-1615231417\n"
     ]
    }
   ],
   "source": [
    "sagemaker_project_name = \"dsoaws-{}\".format(timestamp)\n",
    "\n",
    "create_response = sm.create_project(\n",
    "    ProjectName=sagemaker_project_name,\n",
    "    ProjectDescription=\"dsoaws-{}\".format(timestamp),\n",
    "    ServiceCatalogProvisioningDetails={\n",
    "        \"ProductId\": sagemaker_pipeline_product_id,\n",
    "        \"ProvisioningArtifactId\": sagemaker_pipeline_product_provisioning_artifact_id,\n",
    "    },\n",
    ")\n",
    "\n",
    "sagemaker_project_id = create_response[\"ProjectId\"]\n",
    "sagemaker_project_arn = create_response[\"ProjectArn\"]\n",
    "\n",
    "print(\"Project ID {}\".format(sagemaker_project_id))\n",
    "print(\"Project ARN {}\".format(sagemaker_project_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Project ID and ARN combined: dsoaws-1615231417-p-nynqfqk1cz5v\n"
     ]
    }
   ],
   "source": [
    "sagemaker_project_name_and_id = \"{}-{}\".format(sagemaker_project_name, sagemaker_project_id)\n",
    "\n",
    "print(\"Combined Project ID and ARN combined: {}\".format(sagemaker_project_name_and_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait for the Project to be Created_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Project...\n",
      "Please wait...\n",
      "Project status: CreateInProgress\n",
      "Please wait...\n",
      "Project status: CreateInProgress\n",
      "Please wait...\n",
      "Project status: CreateInProgress\n",
      "Please wait...\n",
      "Project status: CreateInProgress\n",
      "Please wait...\n",
      "Project status: CreateInProgress\n",
      "Please wait...\n",
      "Project status: CreateCompleted\n",
      "Project CreateCompleted\n",
      "{'ProjectArn': 'arn:aws:sagemaker:us-east-1:231218423789:project/dsoaws-1615231417', 'ProjectName': 'dsoaws-1615231417', 'ProjectId': 'p-nynqfqk1cz5v', 'ProjectDescription': 'dsoaws-1615231417', 'ServiceCatalogProvisioningDetails': {'ProductId': 'prod-j3ufw6hl7utxm', 'ProvisioningArtifactId': 'pa-oacphmo7m2bji'}, 'ServiceCatalogProvisionedProductDetails': {'ProvisionedProductId': 'pp-jx3cgosqxbvhc'}, 'ProjectStatus': 'CreateCompleted', 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:231218423789:user-profile/d-scapt1dubpfo/demo', 'UserProfileName': 'demo', 'DomainId': 'd-scapt1dubpfo'}, 'CreationTime': datetime.datetime(2021, 3, 8, 19, 23, 37, 709000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'aefc3586-a454-4779-bd29-c7f2ebe3336a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'aefc3586-a454-4779-bd29-c7f2ebe3336a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '614', 'date': 'Mon, 08 Mar 2021 19:26:37 GMT'}, 'RetryAttempts': 0}}\n",
      "CPU times: user 81.2 ms, sys: 7.43 ms, total: 88.7 ms\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "try:\n",
    "    describe_project_response = sm.describe_project(ProjectName=sagemaker_project_name)\n",
    "    project_status = describe_project_response[\"ProjectStatus\"]\n",
    "    print(\"Creating Project...\")\n",
    "\n",
    "    while project_status in [\"Pending\", \"CreateInProgress\"]:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "        describe_project_response = sm.describe_project(ProjectName=sagemaker_project_name)\n",
    "        project_status = describe_project_response[\"ProjectStatus\"]\n",
    "        print(\"Project status: {}\".format(project_status))\n",
    "\n",
    "    if project_status == \"CreateCompleted\":\n",
    "        print(\"Project {}\".format(project_status))\n",
    "\n",
    "    else:\n",
    "        print(\"Project status: {}\".format(project_status))\n",
    "        raise Exception(\"Project not created.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(describe_project_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait for Project to be Created ^^ Above ^^_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach IAM Policies for FeatureStore \n",
    "This is the role used by Code Build when it starts the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_role_name = \"AmazonSageMakerServiceCatalogProductsUseRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231218423789\n"
     ]
    }
   ],
   "source": [
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "print(account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '585947e3-0018-4b20-87c5-acb920ca6d2a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '585947e3-0018-4b20-87c5-acb920ca6d2a', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 08 Mar 2021 19:26:38 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = iam.attach_role_policy(RoleName=sc_role_name, PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'b89a99b9-7bed-4786-97f0-d87c11810323', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b89a99b9-7bed-4786-97f0-d87c11810323', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 08 Mar 2021 19:26:38 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = iam.attach_role_policy(\n",
    "    RoleName=sc_role_name, PolicyArn=\"arn:aws:iam::aws:policy/AmazonSageMakerFeatureStoreAccess\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '98bca404-7b84-40b0-83bd-f52e2c559bb7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '98bca404-7b84-40b0-83bd-f52e2c559bb7', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 08 Mar 2021 19:26:38 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = iam.attach_role_policy(RoleName=sc_role_name, PolicyArn=\"arn:aws:iam::aws:policy/IAMFullAccess\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop the `Abalone` Sample Pipeline that Ships with SageMaker Pipelines\n",
    "The sample \"abalone\" pipeline starts automatically when we create the project. Â We want to stop this pipeline to release these resources and use them for our own pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_abalone_pipeline_execution_arn = sm.list_pipeline_executions(PipelineName=sagemaker_project_name_and_id)[\"PipelineExecutionSummaries\"][0][\"PipelineExecutionArn\"]\n",
    "\n",
    "# print(sample_abalone_pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.stop_pipeline_execution(PipelineExecutionArn=sample_abalone_pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# try:\n",
    "#     describe_pipeline_execution_response = sm.describe_pipeline_execution(\n",
    "#         PipelineExecutionArn=sample_abalone_pipeline_execution_arn\n",
    "#     )\n",
    "#     pipeline_execution_status = describe_pipeline_execution_response[\"PipelineExecutionStatus\"]\n",
    "\n",
    "#     while pipeline_execution_status not in [\"Stopped\", \"Failed\"]:\n",
    "#         print(\"Please wait...\")\n",
    "#         time.sleep(30)\n",
    "#         describe_pipeline_execution_response = sm.describe_pipeline_execution(\n",
    "#             PipelineExecutionArn=sample_abalone_pipeline_execution_arn\n",
    "#         )\n",
    "#         pipeline_execution_status = describe_pipeline_execution_response[\"PipelineExecutionStatus\"]\n",
    "#         print(\"Pipeline execution status: {}\".format(pipeline_execution_status))\n",
    "\n",
    "#     if pipeline_execution_status in [\"Stopped\", \"Failed\"]:\n",
    "#         print(\"Pipeline execution status {}\".format(pipeline_execution_status))\n",
    "#     else:\n",
    "#         print(\"Pipeline execution status: {}\".format(pipeline_execution_status))\n",
    "#         raise Exception(\"Pipeline execution not deleted.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "\n",
    "# print(describe_pipeline_execution_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.delete_pipeline(PipelineName=sagemaker_project_name_and_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone the MLOps Repositories in AWS CodeCommit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "sm_studio_root_path = \"/root/\"\n",
    "sm_notebooks_root_path = \"/home/ec2-user/SageMaker/\"\n",
    "\n",
    "root_path = sm_notebooks_root_path if os.path.isdir(sm_notebooks_root_path) else sm_studio_root_path\n",
    "\n",
    "print(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://git-codecommit.us-east-1.amazonaws.com/v1/repos/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modelbuild\n"
     ]
    }
   ],
   "source": [
    "code_commit_repo1 = \"https://git-codecommit.{}.amazonaws.com/v1/repos/sagemaker-{}-modelbuild\".format(\n",
    "    region, sagemaker_project_name_and_id\n",
    ")\n",
    "print(code_commit_repo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dsoaws-1615231417-p-nynqfqk1cz5v/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modelbuild\n"
     ]
    }
   ],
   "source": [
    "sagemaker_mlops_build_code = \"{}{}/sagemaker-{}-modelbuild\".format(\n",
    "    root_path, sagemaker_project_name_and_id, sagemaker_project_name_and_id\n",
    ")\n",
    "print(sagemaker_mlops_build_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://git-codecommit.us-east-1.amazonaws.com/v1/repos/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modeldeploy\n"
     ]
    }
   ],
   "source": [
    "code_commit_repo2 = \"https://git-codecommit.{}.amazonaws.com/v1/repos/sagemaker-{}-modeldeploy\".format(\n",
    "    region, sagemaker_project_name_and_id\n",
    ")\n",
    "print(code_commit_repo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dsoaws-1615231417-p-nynqfqk1cz5v/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modeldeploy\n"
     ]
    }
   ],
   "source": [
    "sagemaker_mlops_deploy_code = \"{}{}/sagemaker-{}-modeldeploy\".format(\n",
    "    root_path, sagemaker_project_name_and_id, sagemaker_project_name_and_id\n",
    ")\n",
    "print(sagemaker_mlops_deploy_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper '!aws codecommit credential-helper $@'\n",
    "!git config --global credential.UseHttpPath true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/root/dsoaws-1615231417-p-nynqfqk1cz5v/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modelbuild'...\n",
      "remote: Counting objects: 26, done.\u001b[K\n",
      "Unpacking objects: 100% (26/26), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone $code_commit_repo1 $sagemaker_mlops_build_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/root/dsoaws-1615231417-p-nynqfqk1cz5v/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modeldeploy'...\n",
      "remote: Counting objects: 12, done.\u001b[K\n",
      "Unpacking objects: 100% (12/12), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone $code_commit_repo2 $sagemaker_mlops_deploy_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Sample `Abalone` Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $sagemaker_mlops_build_code/pipelines/abalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Workshop Code Into Local Project Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/workshop/10_pipeline/sagemaker_mlops/sagemaker-project-modelbuild\n"
     ]
    }
   ],
   "source": [
    "workshop_project_build_code = \"{}workshop/10_pipeline/sagemaker_mlops/sagemaker-project-modelbuild\".format(root_path)\n",
    "print(workshop_project_build_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/workshop/10_pipeline/sagemaker_mlops/sagemaker-project-modeldeploy\n"
     ]
    }
   ],
   "source": [
    "workshop_project_deploy_code = \"{}workshop/10_pipeline/sagemaker_mlops/sagemaker-project-modeldeploy\".format(root_path)\n",
    "print(workshop_project_deploy_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R $workshop_project_build_code/* $sagemaker_mlops_build_code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R $workshop_project_deploy_code/* $sagemaker_mlops_deploy_code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit New Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dsoaws-1615231417-p-nynqfqk1cz5v/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modelbuild\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker_mlops_build_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
      "\n",
      "\t\u001b[31mmodified:   README.md\u001b[m\n",
      "\t\u001b[31mmodified:   codebuild-buildspec.yml\u001b[m\n",
      "\t\u001b[31mdeleted:    pipelines/abalone/__init__.py\u001b[m\n",
      "\t\u001b[31mdeleted:    pipelines/abalone/evaluate.py\u001b[m\n",
      "\t\u001b[31mdeleted:    pipelines/abalone/pipeline.py\u001b[m\n",
      "\t\u001b[31mdeleted:    pipelines/abalone/preprocess.py\u001b[m\n",
      "\t\u001b[31mmodified:   pipelines/run_pipeline.py\u001b[m\n",
      "\t\u001b[31mmodified:   setup.py\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\n",
      "\t\u001b[31mpipelines/dsoaws/\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "[main d7b7e8b] Data Science on AWS\n",
      " 14 files changed, 2197 insertions(+), 456 deletions(-)\n",
      " delete mode 100644 pipelines/abalone/evaluate.py\n",
      " delete mode 100644 pipelines/abalone/pipeline.py\n",
      " delete mode 100644 pipelines/abalone/preprocess.py\n",
      " rename pipelines/{abalone => dsoaws}/__init__.py (100%)\n",
      " create mode 100644 pipelines/dsoaws/evaluate_model_metrics.py\n",
      " create mode 100644 pipelines/dsoaws/inference.py\n",
      " create mode 100644 pipelines/dsoaws/pipeline.py\n",
      " create mode 100644 pipelines/dsoaws/preprocess-scikit-text-to-bert-feature-store.py\n",
      " create mode 100644 pipelines/dsoaws/test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n",
      " create mode 100644 pipelines/dsoaws/tf_bert_reviews.py\n",
      "Enumerating objects: 21, done.\n",
      "Counting objects: 100% (21/21), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects: 100% (15/15), done.\n",
      "Writing objects: 100% (15/15), 18.15 MiB | 15.02 MiB/s, done.\n",
      "Total 15 (delta 5), reused 0 (delta 0)\n",
      "To https://git-codecommit.us-east-1.amazonaws.com/v1/repos/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modelbuild\n",
      "   cca9f7e..d7b7e8b  main -> main\n"
     ]
    }
   ],
   "source": [
    "!cd $sagemaker_mlops_build_code; git status; git add --all .; git commit -m \"Data Science on AWS\"; git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
      "\n",
      "\t\u001b[31mmodified:   README.md\u001b[m\n",
      "\t\u001b[31mmodified:   prod-config.json\u001b[m\n",
      "\t\u001b[31mmodified:   staging-config.json\u001b[m\n",
      "\t\u001b[31mmodified:   test/test.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "[main e0c4217] Data Science on AWS\n",
      " 4 files changed, 4 insertions(+), 6 deletions(-)\n",
      "Enumerating objects: 13, done.\n",
      "Counting objects: 100% (13/13), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects: 100% (7/7), done.\n",
      "Writing objects: 100% (7/7), 617 bytes | 88.00 KiB/s, done.\n",
      "Total 7 (delta 5), reused 0 (delta 0)\n",
      "To https://git-codecommit.us-east-1.amazonaws.com/v1/repos/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modeldeploy\n",
      "   ef5ed27..e0c4217  main -> main\n"
     ]
    }
   ],
   "source": [
    "!cd $sagemaker_mlops_deploy_code; git status; git add --all .; git commit -m \"Data Science on AWS\"; git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'sagemaker_mlops_build_code' (str)\n",
      "Stored 'sagemaker_mlops_deploy_code' (str)\n",
      "Stored 'sagemaker_project_name' (str)\n",
      "Stored 'sagemaker_project_id' (str)\n",
      "Stored 'sagemaker_project_name_and_id' (str)\n",
      "Stored 'sagemaker_project_arn' (str)\n",
      "Stored 'sagemaker_pipeline_product_id' (str)\n",
      "Stored 'sagemaker_pipeline_product_provisioning_artifact_id' (str)\n"
     ]
    }
   ],
   "source": [
    "%store sagemaker_mlops_build_code\n",
    "%store sagemaker_mlops_deploy_code\n",
    "%store sagemaker_project_name\n",
    "%store sagemaker_project_id\n",
    "%store sagemaker_project_name_and_id\n",
    "%store sagemaker_project_arn\n",
    "%store sagemaker_pipeline_product_id\n",
    "%store sagemaker_pipeline_product_provisioning_artifact_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 100\n",
      "drwxr-xr-x 3 root root  6144 Mar  8 19:29 .\n",
      "drwxr-xr-x 3 root root  6144 Mar  8 19:29 ..\n",
      "-rw-r--r-- 1 root root     0 Mar  8 19:29 __init__.py\n",
      "-rw-r--r-- 1 root root  8039 Mar  8 19:29 evaluate_model_metrics.py\n",
      "-rw-r--r-- 1 root root  3501 Mar  8 19:29 inference.py\n",
      "-rw-r--r-- 1 root root 16552 Mar  8 19:29 pipeline.py\n",
      "-rw-r--r-- 1 root root 22207 Mar  8 19:29 preprocess-scikit-text-to-bert-feature-store.py\n",
      "drwxr-xr-x 2 root root  6144 Mar  8 19:29 test_data\n",
      "-rw-r--r-- 1 root root 25080 Mar  8 19:29 tf_bert_reviews.py\n"
     ]
    }
   ],
   "source": [
    "!ls -al $sagemaker_mlops_build_code/pipelines/dsoaws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33mExample workflow pipeline script for BERT pipeline.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m                                                 . -RegisterModel\u001b[39;49;00m\n",
      "\u001b[33m                                                .\u001b[39;49;00m\n",
      "\u001b[33m    Process-> Train -> (Evaluate -> Condition) .\u001b[39;49;00m\n",
      "\u001b[33m                                                .\u001b[39;49;00m\n",
      "\u001b[33m                                                 . -(stop)\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mImplements a get_pipeline(**kwargs) method.\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mbotocore\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexceptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClientError\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mestimator\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Estimator\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minputs\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingInput\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SKLearnProcessor\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TensorFlow\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_metrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    MetricsSource,\n",
      "    ModelMetrics,\n",
      ")\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    ProcessingInput,\n",
      "    ProcessingOutput,\n",
      "    ScriptProcessor,\n",
      ")\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ParameterInteger, ParameterString, ParameterFloat\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpipeline\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msteps\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ProcessingStep, TrainingStep, CreateModelStep\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_metrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MetricsSource, ModelMetrics\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mconditions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ConditionGreaterThanOrEqualTo\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcondition_step\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    ConditionStep,\n",
      "    JsonGet,\n",
      ")\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mproperties\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PropertyFile\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mstep_collections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m RegisterModel\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Model\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minputs\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m CreateModelInput\n",
      "\n",
      "\n",
      "sess = sagemaker.Session()\n",
      "bucket = sess.default_bucket()\n",
      "timestamp = \u001b[36mint\u001b[39;49;00m(time.time())\n",
      "\n",
      "BASE_DIR = os.path.dirname(os.path.realpath(\u001b[31m__file__\u001b[39;49;00m))\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBASE_DIR: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(BASE_DIR))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_pipeline\u001b[39;49;00m(region, role, default_bucket, pipeline_name, model_package_group_name, base_job_prefix):\n",
      "    \u001b[33m\"\"\"Gets a SageMaker ML Pipeline instance working with BERT.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m        region: AWS region to create and run the pipeline.\u001b[39;49;00m\n",
      "\u001b[33m        role: IAM role to create and run steps and pipeline.\u001b[39;49;00m\n",
      "\u001b[33m        default_bucket: the bucket to use for storing the artifacts\u001b[39;49;00m\n",
      "\u001b[33m        pipeline_name:  name of this pipeline\u001b[39;49;00m\n",
      "\u001b[33m        model_package_group_name:  model package group\u001b[39;49;00m\n",
      "\u001b[33m        base_job_prefix:  prefic of the job name\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\n",
      "\u001b[33m        an instance of a pipeline\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    sm = boto3.Session().client(service_name=\u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "    input_data = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mInputDataUrl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/amazon-reviews-pds/tsv/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(bucket),\n",
      "    )\n",
      "\n",
      "    processing_instance_count = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessingInstanceCount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    processing_instance_type = ParameterString(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessingInstanceType\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mml.c5.2xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    max_seq_length = ParameterInteger(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mMaxSeqLength\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[34m64\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    balance_dataset = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mBalanceDataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    train_split_percentage = ParameterFloat(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrainSplitPercentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[34m0.90\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    validation_split_percentage = ParameterFloat(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mValidationSplitPercentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    test_split_percentage = ParameterFloat(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTestSplitPercentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    feature_store_offline_prefix = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureStoreOfflinePrefix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mreviews-feature-store-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(timestamp),\n",
      "    )\n",
      "\n",
      "    feature_group_name = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mreviews-feature-group-\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(timestamp)\n",
      "    )\n",
      "\n",
      "    train_instance_type = ParameterString(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrainInstanceType\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mml.c5.9xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    train_instance_count = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrainInstanceCount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m# PROCESSING STEP\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    processor = SKLearnProcessor(\n",
      "        framework_version=\u001b[33m\"\u001b[39;49;00m\u001b[33m0.23-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        role=role,\n",
      "        instance_type=processing_instance_type,\n",
      "        instance_count=processing_instance_count,\n",
      "        env={\u001b[33m\"\u001b[39;49;00m\u001b[33mAWS_DEFAULT_REGION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: region},\n",
      "        max_runtime_in_seconds=\u001b[34m7200\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    processing_inputs = [\n",
      "        ProcessingInput(\n",
      "            input_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mraw-input-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            source=input_data,\n",
      "            destination=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            s3_data_distribution_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mShardedByS3Key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        )\n",
      "    ]\n",
      "\n",
      "    processing_outputs = [\n",
      "        ProcessingOutput(\n",
      "            output_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            s3_upload_mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mEndOfJob\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            source=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        ),\n",
      "        ProcessingOutput(\n",
      "            output_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            s3_upload_mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mEndOfJob\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            source=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        ),\n",
      "        ProcessingOutput(\n",
      "            output_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            s3_upload_mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mEndOfJob\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            source=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        ),\n",
      "    ]\n",
      "\n",
      "    \u001b[37m# TODO:  Figure out why the Parameter's are not resolving properly to their native type when user here.\u001b[39;49;00m\n",
      "    \u001b[37m#        We shouldn't be using `default_value`\u001b[39;49;00m\n",
      "    processing_step = ProcessingStep(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        processor=processor,\n",
      "        inputs=processing_inputs,\n",
      "        outputs=processing_outputs,\n",
      "        job_arguments=[\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(train_split_percentage.default_value),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(validation_split_percentage.default_value),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(test_split_percentage.default_value),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(max_seq_length.default_value),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(balance_dataset.default_value),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(feature_store_offline_prefix.default_value),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-group-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(feature_group_name.default_value),\n",
      "        ],\n",
      "        code=os.path.join(BASE_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpreprocess-scikit-text-to-bert-feature-store.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "    )\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m# TRAINING STEP\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    epochs = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mEpochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    learning_rate = ParameterFloat(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mLearningRate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m0.00001\u001b[39;49;00m)\n",
      "\n",
      "    epsilon = ParameterFloat(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mEpsilon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m0.00000001\u001b[39;49;00m)\n",
      "\n",
      "    train_batch_size = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrainBatchSize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m128\u001b[39;49;00m)\n",
      "\n",
      "    validation_batch_size = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mValidationBatchSize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m128\u001b[39;49;00m)\n",
      "\n",
      "    test_batch_size = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTestBatchSize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m128\u001b[39;49;00m)\n",
      "\n",
      "    train_steps_per_epoch = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrainStepsPerEpoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m50\u001b[39;49;00m)\n",
      "\n",
      "    validation_steps = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mValidationSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m50\u001b[39;49;00m)\n",
      "\n",
      "    test_steps = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTestSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m50\u001b[39;49;00m)\n",
      "\n",
      "    train_volume_size = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrainVolumeSize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m1024\u001b[39;49;00m)\n",
      "\n",
      "    use_xla = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mUseXLA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    use_amp = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mUseAMP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    freeze_bert_layer = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mFreezeBERTLayer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mFalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    enable_sagemaker_debugger = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mEnableSageMakerDebugger\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mFalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    enable_checkpointing = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mEnableCheckpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mFalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    enable_tensorboard = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mEnableTensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mFalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    input_mode = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mInputMode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mFile\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    run_validation = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mRunValidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    run_test = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mRunTest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mFalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    run_sample_predictions = ParameterString(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mRunSamplePredictions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mFalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    metrics_definitions = [\n",
      "        {\u001b[33m\"\u001b[39;49;00m\u001b[33mName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain:loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mRegex\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mloss: ([0-9\u001b[39;49;00m\u001b[33m\\\\\u001b[39;49;00m\u001b[33m.]+)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\n",
      "        {\u001b[33m\"\u001b[39;49;00m\u001b[33mName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain:accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mRegex\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy: ([0-9\u001b[39;49;00m\u001b[33m\\\\\u001b[39;49;00m\u001b[33m.]+)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\n",
      "        {\u001b[33m\"\u001b[39;49;00m\u001b[33mName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation:loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mRegex\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mval_loss: ([0-9\u001b[39;49;00m\u001b[33m\\\\\u001b[39;49;00m\u001b[33m.]+)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\n",
      "        {\u001b[33m\"\u001b[39;49;00m\u001b[33mName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation:accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mRegex\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mval_accuracy: ([0-9\u001b[39;49;00m\u001b[33m\\\\\u001b[39;49;00m\u001b[33m.]+)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\n",
      "    ]\n",
      "\n",
      "    train_src = os.path.join(BASE_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33msrc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model_path = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{default_bucket}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{base_job_prefix}\u001b[39;49;00m\u001b[33m/output/model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "    estimator = TensorFlow(\n",
      "        entry_point=\u001b[33m\"\u001b[39;49;00m\u001b[33mtf_bert_reviews.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        source_dir=BASE_DIR,\n",
      "        role=role,\n",
      "        output_path=model_path,\n",
      "        instance_count=train_instance_count,\n",
      "        instance_type=train_instance_type,\n",
      "        volume_size=train_volume_size,\n",
      "        py_version=\u001b[33m\"\u001b[39;49;00m\u001b[33mpy37\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        framework_version=\u001b[33m\"\u001b[39;49;00m\u001b[33m2.3.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        hyperparameters={\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: epochs,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: learning_rate,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mepsilon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: epsilon,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: train_batch_size,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: validation_batch_size,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: test_batch_size,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: train_steps_per_epoch,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: validation_steps,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: test_steps,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33muse_xla\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_xla,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33muse_amp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mmax_seq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: max_seq_length,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mfreeze_bert_layer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: freeze_bert_layer,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33menable_sagemaker_debugger\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: enable_sagemaker_debugger,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33menable_checkpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: enable_checkpointing,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33menable_tensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: enable_tensorboard,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mrun_validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: run_validation,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mrun_test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: run_test,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mrun_sample_predictions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: run_sample_predictions,\n",
      "        },\n",
      "        input_mode=input_mode,\n",
      "        metric_definitions=metrics_definitions,\n",
      "        \u001b[37m#        max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    training_step = TrainingStep(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        estimator=estimator,\n",
      "        inputs={\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: TrainingInput(\n",
      "                s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].S3Output.S3Uri,\n",
      "                content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            ),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: TrainingInput(\n",
      "                s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].S3Output.S3Uri,\n",
      "                content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            ),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: TrainingInput(\n",
      "                s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].S3Output.S3Uri,\n",
      "                content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            ),\n",
      "        },\n",
      "    )\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m# EVALUATION STEP\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    evaluation_processor = SKLearnProcessor(\n",
      "        framework_version=\u001b[33m\"\u001b[39;49;00m\u001b[33m0.23-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        role=role,\n",
      "        instance_type=processing_instance_type,\n",
      "        instance_count=processing_instance_count,\n",
      "        env={\u001b[33m\"\u001b[39;49;00m\u001b[33mAWS_DEFAULT_REGION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: region},\n",
      "        max_runtime_in_seconds=\u001b[34m7200\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    evaluation_report = PropertyFile(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mEvaluationReport\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, output_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, path=\u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    evaluation_step = ProcessingStep(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mEvaluateModel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        processor=evaluation_processor,\n",
      "        code=os.path.join(BASE_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mevaluate_model_metrics.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        inputs=[\n",
      "            ProcessingInput(\n",
      "                source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
      "                destination=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            ),\n",
      "            ProcessingInput(\n",
      "                source=processing_step.properties.ProcessingInputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mraw-input-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].S3Input.S3Uri,\n",
      "                destination=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            ),\n",
      "        ],\n",
      "        outputs=[\n",
      "            ProcessingOutput(\n",
      "                output_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, s3_upload_mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mEndOfJob\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, source=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/metrics/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            ),\n",
      "        ],\n",
      "        job_arguments=[\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            \u001b[36mstr\u001b[39;49;00m(max_seq_length.default_value),\n",
      "        ],\n",
      "        property_files=[evaluation_report],  \u001b[37m# these cause deserialization issues\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    model_metrics = ModelMetrics(\n",
      "        model_statistics=MetricsSource(\n",
      "            s3_uri=\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "                evaluation_step.arguments[\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessingOutputConfig\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mOutputs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mS3Output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mS3Uri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "            ),\n",
      "            content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        )\n",
      "    )\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m## REGISTER TRAINED MODEL STEP\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    model_approval_status = ParameterString(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mModelApprovalStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mPendingManualApproval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    deploy_instance_type = ParameterString(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mDeployInstanceType\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[33m\"\u001b[39;49;00m\u001b[33mml.m4.xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    deploy_instance_count = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mDeployInstanceCount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    inference_image_uri = sagemaker.image_uris.retrieve(\n",
      "        framework=\u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        region=region,\n",
      "        version=\u001b[33m\"\u001b[39;49;00m\u001b[33m2.3.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        py_version=\u001b[33m\"\u001b[39;49;00m\u001b[33mpy37\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        instance_type=deploy_instance_type,\n",
      "        image_scope=\u001b[33m\"\u001b[39;49;00m\u001b[33minference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    \u001b[36mprint\u001b[39;49;00m(inference_image_uri)\n",
      "\n",
      "    register_step = RegisterModel(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mRegisterModel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        estimator=estimator,\n",
      "        image_uri=inference_image_uri,  \u001b[37m# we have to specify, by default it's using training image\u001b[39;49;00m\n",
      "        model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
      "        content_types=[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        response_types=[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        inference_instances=[\n",
      "            deploy_instance_type\n",
      "        ],  \u001b[37m# The JSON spec must be within these instance types or we will see \"Instance Type Not Allowed\" Exception\u001b[39;49;00m\n",
      "        transform_instances=[deploy_instance_type],\n",
      "        model_package_group_name=model_package_group_name,\n",
      "        approval_status=model_approval_status,\n",
      "    )\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m## CREATE MODEL FOR DEPLOYMENT STEP\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    model = Model(\n",
      "        image_uri=inference_image_uri,\n",
      "        model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
      "        sagemaker_session=sess,\n",
      "        role=role,\n",
      "    )\n",
      "\n",
      "    create_inputs = CreateModelInput(\n",
      "        instance_type=deploy_instance_type,\n",
      "    )\n",
      "\n",
      "    create_step = CreateModelStep(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mCreateModel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        model=model,\n",
      "        inputs=create_inputs,\n",
      "    )\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m## CONDITION STEP:  EVALUATE THE MODEL\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    min_accuracy_value = ParameterFloat(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mMinAccuracyValue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[34m0.01\u001b[39;49;00m)\n",
      "\n",
      "    minimum_accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
      "        left=JsonGet(\n",
      "            step=evaluation_step,\n",
      "            property_file=evaluation_report,\n",
      "            json_path=\u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics.accuracy.value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        ),\n",
      "        right=min_accuracy_value,  \u001b[37m# accuracy\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    minimum_accuracy_condition_step = ConditionStep(\n",
      "        name=\u001b[33m\"\u001b[39;49;00m\u001b[33mAccuracyCondition\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        conditions=[minimum_accuracy_condition],\n",
      "        if_steps=[register_step, create_step],  \u001b[37m# success, continue with model registration\u001b[39;49;00m\n",
      "        else_steps=[],  \u001b[37m# fail, end the pipeline\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m## CREATE PIPELINE\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    pipeline = Pipeline(\n",
      "        name=pipeline_name,\n",
      "        parameters=[\n",
      "            input_data,\n",
      "            processing_instance_count,\n",
      "            processing_instance_type,\n",
      "            max_seq_length,\n",
      "            balance_dataset,\n",
      "            train_split_percentage,\n",
      "            validation_split_percentage,\n",
      "            test_split_percentage,\n",
      "            feature_store_offline_prefix,\n",
      "            feature_group_name,\n",
      "            train_instance_type,\n",
      "            train_instance_count,\n",
      "            epochs,\n",
      "            learning_rate,\n",
      "            epsilon,\n",
      "            train_batch_size,\n",
      "            validation_batch_size,\n",
      "            test_batch_size,\n",
      "            train_steps_per_epoch,\n",
      "            validation_steps,\n",
      "            test_steps,\n",
      "            train_volume_size,\n",
      "            use_xla,\n",
      "            use_amp,\n",
      "            freeze_bert_layer,\n",
      "            enable_sagemaker_debugger,\n",
      "            enable_checkpointing,\n",
      "            enable_tensorboard,\n",
      "            input_mode,\n",
      "            run_validation,\n",
      "            run_test,\n",
      "            run_sample_predictions,\n",
      "            min_accuracy_value,\n",
      "            model_approval_status,\n",
      "            deploy_instance_type,\n",
      "            deploy_instance_count,\n",
      "        ],\n",
      "        steps=[processing_step, training_step, evaluation_step, minimum_accuracy_condition_step],\n",
      "        sagemaker_session=sess,\n",
      "    )\n",
      "\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "    \u001b[37m## RETURN PIPELINE\u001b[39;49;00m\n",
      "    \u001b[37m#########################\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m pipeline\n"
     ]
    }
   ],
   "source": [
    "!pygmentize $sagemaker_mlops_build_code/pipelines/dsoaws/pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for Pipeline Execution to Start\n",
    "Now that we have committed code, our pipeline will start.  Let's wait for the pipeline to start.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Check Why Pipeline Doesn't Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n",
      "Listing executions for our pipeline...\n",
      "Please wait...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceNotFound\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceNotFound\u001b[0m: An error occurred (ResourceNotFound) when calling the ListPipelineExecutions operation: Pipeline 'arn:aws:sagemaker:us-east-1:231218423789:pipeline/dsoaws-1615231417-p-nynqfqk1cz5v' does not exist.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(\"Listing executions for our pipeline...\")\n",
    "        list_executions_response = sm.list_pipeline_executions(PipelineName=sagemaker_project_name_and_id)[\n",
    "            \"PipelineExecutionSummaries\"\n",
    "        ]\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(list_executions_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_pipeline_name = \"sagemaker-{}-modelbuild\".format(sagemaker_project_name_and_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Check <a target=\"blank\" href=\"https://console.aws.amazon.com/codesuite/codepipeline/pipelines/sagemaker-dsoaws-1615231417-p-nynqfqk1cz5v-modelbuild/view?region=us-east-1\">ModelBuild</a> Pipeline</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Check <a target=\"blank\" href=\"https://console.aws.amazon.com/codesuite/codepipeline/pipelines/{}/view?region={}\">ModelBuild</a> Pipeline</b>'.format(\n",
    "            build_pipeline_name, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait For Pipeline Execution To Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceNotFound",
     "evalue": "An error occurred (ResourceNotFound) when calling the ListPipelineExecutions operation: Pipeline 'arn:aws:sagemaker:us-east-1:231218423789:pipeline/dsoaws-1615231417-p-nynqfqk1cz5v' does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceNotFound\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceNotFound\u001b[0m: An error occurred (ResourceNotFound) when calling the ListPipelineExecutions operation: Pipeline 'arn:aws:sagemaker:us-east-1:231218423789:pipeline/dsoaws-1615231417-p-nynqfqk1cz5v' does not exist."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=sagemaker_project_name_and_id)[\n",
    "    \"PipelineExecutionSummaries\"\n",
    "]\n",
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status == \"Executing\":\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=sagemaker_project_name_and_id)[\n",
    "            \"PipelineExecutionSummaries\"\n",
    "        ]\n",
    "        pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "    #        print('Executions for our pipeline...')\n",
    "    #        print(pipeline_execution_status)\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(executions_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait for the Pipeline Execution ^^ Above ^^ to Complete_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Pipeline Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'executions_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5ee50520a6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_execution_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutions_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PipelineExecutionStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_execution_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'executions_response' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_arn = executions_response[0][\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)\n",
    "\n",
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approve the Registered Model for Staging\n",
    "The pipeline that was executed created a Model Package version within the specified Model Package Group. Of particular note, the registration of the model/creation of the Model Package was done so with approval status as `PendingManualApproval`.\n",
    "\n",
    "Notes:  \n",
    "* You can do this within SageMaker Studio, as well.  However, we are approving programmatically here in this example.\n",
    "* This approval is only for Staging.  For Production, you must go through the CodePipeline (deep link is below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(\"Executions for our pipeline...\")\n",
    "        list_model_packages_response = sm.list_model_packages(ModelPackageGroupName=sagemaker_project_name_and_id)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(list_model_packages_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)\n",
    "\n",
    "model_package_arn = list_model_packages_response[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]\n",
    "print(model_package_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update_response = sm.update_model_package(\n",
    "    ModelPackageArn=model_package_arn,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    ")\n",
    "\n",
    "print(model_package_update_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)\n",
    "\n",
    "model_name = sm.list_models()[\"Models\"][0][\"ModelName\"]\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/models/{}\">Model</a></b>'.format(\n",
    "            region, model_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_pipeline_name = \"sagemaker-{}-modeldeploy\".format(sagemaker_project_name_and_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Check <a target=\"blank\" href=\"https://console.aws.amazon.com/codesuite/codepipeline/pipelines/{}/view?region={}\">ModelDeploy</a> Pipeline</b>'.format(\n",
    "            deploy_pipeline_name, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_endpoint_name = \"{}-staging\".format(sagemaker_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker Staging REST Endpoint</a></b>'.format(\n",
    "            region, staging_endpoint_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the Staging Endpoint is Deployed_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "        print(\"Waiting for staging endpoint to be in `InService`...\")\n",
    "        waiter.wait(EndpointName=staging_endpoint_name)\n",
    "        break\n",
    "    except:\n",
    "        print(\"Waiting for staging endpoint to be in `Creating`...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"Staging endpoint deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the ^^ Staging Endpoint ^^ is Deployed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Artifact Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run A Sample Prediction in Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.serializers import JSONLinesSerializer\n",
    "from sagemaker.deserializers import JSONLinesDeserializer\n",
    "\n",
    "predictor = TensorFlowPredictor(\n",
    "    endpoint_name=staging_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    model_name=\"saved_model\",\n",
    "    model_version=0,\n",
    "    content_type=\"application/jsonlines\",\n",
    "    accept_type=\"application/jsonlines\",\n",
    "    serializer=JSONLinesSerializer(),\n",
    "    deserializer=JSONLinesDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\"features\": [\"This is great!\"]}, {\"features\": [\"This is bad.\"]}]\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)\n",
    "\n",
    "for predicted_class in predicted_classes:\n",
    "    print(\"Predicted star_rating: {}\".format(predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/codesuite/codepipeline/pipelines/sagemaker-{}-modeldeploy/view?region={}\"> Deploy to Production </a> Pipeline</b> '.format(\n",
    "            sagemaker_project_name_and_id, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_name = \"DeployStaging\"\n",
    "action_name = \"ApproveDeployment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)\n",
    "\n",
    "stage_states = codepipeline.get_pipeline_state(name=deploy_pipeline_name)[\"stageStates\"]\n",
    "\n",
    "for stage_state in stage_states:\n",
    "\n",
    "    if stage_state[\"stageName\"] == stage_name:\n",
    "        for action_state in stage_state[\"actionStates\"]:\n",
    "            if action_state[\"actionName\"] == action_name:\n",
    "                token = action_state[\"latestExecution\"][\"token\"]\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = codepipeline.put_approval_result(\n",
    "    pipelineName=deploy_pipeline_name,\n",
    "    stageName=stage_name,\n",
    "    actionName=action_name,\n",
    "    result={\"summary\": \"Approve from Staging to Production\", \"status\": \"Approved\"},\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Production Endpoint Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)\n",
    "\n",
    "production_endpoint_name = \"{}-prod\".format(sagemaker_project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker Production REST Endpoint</a></b>'.format(\n",
    "            region, production_endpoint_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the Production Endpoint is Deployed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "        print(\"Waiting for production endpoint to be in `InService`...\")\n",
    "        waiter.wait(EndpointName=production_endpoint_name)\n",
    "        break\n",
    "    except:\n",
    "        print(\"Waiting for production endpoint to be in `Creating`...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"Production endpoint deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the ^^ Production Endpoint ^^ is Deployed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.serializers import JSONLinesSerializer\n",
    "from sagemaker.deserializers import JSONLinesDeserializer\n",
    "\n",
    "predictor = TensorFlowPredictor(\n",
    "    endpoint_name=production_endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    model_name=\"saved_model\",\n",
    "    model_version=0,\n",
    "    content_type=\"application/jsonlines\",\n",
    "    accept_type=\"application/jsonlines\",\n",
    "    serializer=JSONLinesSerializer(),\n",
    "    deserializer=JSONLinesDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\"features\": [\"This is great!\"]}, {\"features\": [\"This is bad.\"]}]\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)\n",
    "\n",
    "for predicted_class in predicted_classes:\n",
    "    print(\"Predicted star_rating: {}\".format(predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Artifact Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
