{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "source ~/.bash_profile\n",
    "\n",
    "#### Check environment variables are set with valid values\n",
    "echo $S3_BUCKET\n",
    "echo $AWS_CLUSTER_NAME\n",
    "echo $STACK_NAME\n",
    "echo $INSTANCE_ROLE_NAME\n",
    "echo $INSTANCE_PROFILE_ARN\n",
    "\n",
    "#### Download the `kfctl` CLI tool\n",
    "curl --location https://github.com/kubeflow/kfctl/releases/download/v1.0-rc.1/kfctl_v1.0-rc.1-0-g963c787_linux.tar.gz | tar xz\n",
    "\n",
    "sudo mv kfctl /usr/local/bin\n",
    "\n",
    "#### Get the latest Kubeflow configuration file\n",
    "export CONFIG_URI='https://raw.githubusercontent.com/kubeflow/manifests/v0.7-branch/kfdef/kfctl_aws.0.7.1.yaml'\n",
    "echo \"export CONFIG_URI=${CONFIG_URI}\" | tee -a ~/.bash_profile\n",
    "\n",
    "#### Set Kubeflow environment variables \n",
    "export KF_NAME=${AWS_CLUSTER_NAME}\n",
    "echo \"export KF_NAME=${KF_NAME}\" | tee -a ~/.bash_profile\n",
    "\n",
    "cd ~/SageMaker/kubeflow/notebooks/part-3-kubernetes\n",
    "\n",
    "export KF_DIR=$PWD/${KF_NAME}\n",
    "echo \"export KF_DIR=${KF_DIR}\" | tee -a ~/.bash_profile\n",
    "\n",
    "#### Customize the configuration files\n",
    "# We'll edit the configuration with the right names for the cluster and node groups before deploying Kubeflow.\n",
    "\n",
    "mkdir -p ${KF_DIR}\n",
    "cd ${KF_DIR}\n",
    "\n",
    "curl -O ${CONFIG_URI}\n",
    "\n",
    "export CONFIG_FILE=${KF_DIR}/kfctl_aws.0.7.1.yaml\n",
    "echo \"export CONFIG_FILE=${CONFIG_FILE}\" | tee -a ~/.bash_profile\n",
    "\n",
    "sed -i.bak -e \"s@eksctl-kubeflow-aws-nodegroup-ng-a2-NodeInstanceRole-xxxxxxx@$INSTANCE_ROLE_NAME@\" ${CONFIG_FILE}\n",
    "sed -i.bak -e 's/kubeflow-aws/'\"$AWS_CLUSTER_NAME\"'/' ${CONFIG_FILE}\n",
    "sed -i.bak -e \"s@us-west-2@$AWS_REGION@\" ${CONFIG_FILE}\n",
    "\n",
    "#### Generate the Kubeflow installation files\n",
    "cd ${KF_DIR}\n",
    "\n",
    "rm -rf kustomize\n",
    "rm -rf .cache\n",
    "\n",
    "kfctl build -V -f ${CONFIG_FILE}\n",
    "\n",
    "#### Deploy Kubeflow\n",
    "cd ${KF_DIR}\n",
    "\n",
    "kfctl apply -V -f ${CONFIG_FILE}\n",
    "\n",
    "#### Wait for resource to become available\n",
    "\n",
    "#### Monitor changes by running kubectl get all namespaces command.\n",
    "kubectl -n kubeflow get all\n",
    "\n",
    "#### Delete the usage reporting beacon\n",
    "kubectl delete all -l app=spartakus --namespace=kubeflow\n",
    "\n",
    "#### Change to `kubeflow` namespace\n",
    "kubectl config set-context --current --namespace=kubeflow\n",
    "\n",
    "#### Navigate to the Kubeflow Dashboard - THIS WILL TAKE A FEW MINUTES\n",
    "# Note:  DNS is eventually consistent and will take a few minutes to propagate.  Please be patient if you see a 404 in your browser.  It will take a few minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Re-run the following until it returns a valid hostname. \n",
    "\n",
    "_Note:  This may take 5-10 minutes._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The connection to the server localhost:8080 was refused - did you specify the right host or port?\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo $(kubectl get ingress -n istio-system -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')\n",
    "\n",
    "### EXPECTED OUTPUT - THIS WILL TAKE A FEW MINUTES!! ###\n",
    "# <some-long-subdomain-name>.<aws-region>.elb.amazonaws.com \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Navigate to the link above\n",
    "\n",
    "_Note:  This will take a few minutes to propogate through DNS._\n",
    "\n",
    "Click on `Start Setup`.\n",
    "\n",
    "Set the namespace to `eksworkshop`.\n",
    "\n",
    "Note: If you accidentally use the default `anonymous` namespace, you will be OK.  Please continue.\n",
    "\n",
    "Click `Finish` to view the dashboard.\n",
    "\n",
    "![Kubeflow Dashboard Start](img/kubeflow-dashboard-start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kubeflow Dashboard](img/kubeflow-dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT CONTINUE UNTIL YOU SEE THE DASHBOARD SCREEN ABOVE !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup AWS Credentials in EKS cluster\n",
    "\n",
    "AWS credentials are required to save a model in S3. These credentials are stored in the EKS cluster as a Kubernetes secret.\n",
    "\n",
    "Create an IAM user `s3user`, attach the S3 access policy, and retrieve temporary credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "source ~/.bash_profile\n",
    "\n",
    "aws iam create-user --user-name s3user\n",
    "aws iam attach-user-policy --user-name s3user --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n",
    "aws iam attach-user-policy --user-name s3user --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n",
    "aws iam attach-user-policy --user-name s3user --policy-arn arn:aws:iam::aws:policy/AmazonElasticMapReduceFullAccess\n",
    "aws iam attach-user-policy --user-name s3user --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\n",
    "aws iam create-access-key --user-name s3user > /tmp/create_output.json\n",
    "\n",
    "#### Next, save the new user's credentials into environment variables:\n",
    "\n",
    "export AWS_ACCESS_KEY_ID_VALUE=$(jq -j .AccessKey.AccessKeyId /tmp/create_output.json | base64)\n",
    "echo \"export AWS_ACCESS_KEY_ID_VALUE=${AWS_ACCESS_KEY_ID_VALUE}\" | tee -a ~/.bash_profile\n",
    "\n",
    "export AWS_SECRET_ACCESS_KEY_VALUE=$(jq -j .AccessKey.SecretAccessKey /tmp/create_output.json | base64)\n",
    "echo \"export AWS_SECRET_ACCESS_KEY_VALUE=${AWS_SECRET_ACCESS_KEY_VALUE}\" | tee -a ~/.bash_profile\n",
    "\n",
    "#### Apply to EKS cluster.\n",
    "# Note:  If you accidentally used `anonymous` for the namespace, please substitute `--namespace anonymous` below instead of `--namespace eksworkshop`.\n",
    "\n",
    "cat <<EOF | kubectl apply --namespace eksworkshop -f -\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: aws-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID_VALUE\n",
    "  AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY_VALUE\n",
    "EOF\n",
    "\n",
    "#### Add the secret to the `kubeflow` namespace, as well.  This is needed until KF Pipelines support namespaces.\n",
    "cat <<EOF | kubectl apply --namespace kubeflow -f -\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: aws-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID_VALUE\n",
    "  AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY_VALUE\n",
    "EOF\n",
    "\n",
    "\n",
    "#### More credentials used by Kubeflow to access SageMaker\n",
    "TRUST=\"{ \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Effect\\\": \\\"Allow\\\", \\\"Principal\\\": { \\\"Service\\\": \\\"sagemaker.amazonaws.com\\\" }, \\\"Action\\\": \\\"sts:AssumeRole\\\" } ] }\"\n",
    "aws iam create-role --role-name eksworkshop-sagemaker-kfp-role --assume-role-policy-document \"$TRUST\"\n",
    "aws iam attach-role-policy --role-name eksworkshop-sagemaker-kfp-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n",
    "aws iam attach-role-policy --role-name eksworkshop-sagemaker-kfp-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n",
    "aws iam attach-role-policy --role-name eksworkshop-sagemaker-kfp-role --policy-arn arn:aws:iam::aws:policy/AmazonElasticMapReduceFullAccess\n",
    "\n",
    "export SAGEMAKER_ROLE_ARN=$(aws iam get-role --role-name eksworkshop-sagemaker-kfp-role --output text --query 'Role.Arn')\n",
    "echo \"export SAGEMAKER_ROLE_ARN=${SAGEMAKER_ROLE_ARN}\" | tee -a ~/.bash_profile\n",
    "\n",
    "cat <<EoF > sagemaker-invoke.json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:InvokeEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "EoF\n",
    "\n",
    "aws iam put-role-policy --role-name eksworkshop-sagemaker-kfp-role --policy-name sagemaker-invoke-for-worker --policy-document file://sagemaker-invoke.json\n",
    "aws iam put-role-policy --role-name ${INSTANCE_ROLE_NAME} --policy-name sagemaker-invoke-for-worker --policy-document file://sagemaker-invoke.json\n",
    "\n",
    "echo \"Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
